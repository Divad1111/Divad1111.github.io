{"meta":{"title":"张卫的博客","subtitle":"Done is better than perfect","description":null,"author":"张卫","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2020-07-18T07:18:33.000Z","updated":"2025-04-26T11:06:24.134Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"学习之地"},{"title":"分类","date":"2019-11-24T10:04:14.000Z","updated":"2025-04-26T11:06:24.134Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-11-24T06:58:16.000Z","updated":"2025-04-26T11:06:24.233Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"性能优化-GPU","slug":"Unity/Optimization/性能优化-GPU","date":"2025-07-21T12:14:20.000Z","updated":"2025-08-07T14:59:08.585Z","comments":true,"path":"2025/07/21/Unity/Optimization/性能优化-GPU/","link":"","permalink":"http://yoursite.com/2025/07/21/Unity/Optimization/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-GPU/","excerpt":"GPU 硬件架构与渲染管线概述 GPU 是并行计算密集型硬件，在移动SoC中负责加速图形渲染。其硬件模块包括： 算术逻辑单元（ALU）或统一着色器单元（执行顶点/片元着色器） 光栅化单元（将图元转换为像素/片元） 纹理采样单元（纹理映射与滤波） 像素合成单元（ROP/Output Merger），将像素结果写入帧缓冲，进行混合、深度测试等操作 Z/Stencil测试单元，执行早期深度剔除、模板测试等以减少不必要的计算 缓存层次结构（如顶点缓存、纹理缓存、片元缓存、片元缓存(Tile Cache)、L1/L2 Cache等，用于减少主内存访问延迟） 指令调度器/任务调度器, 管理线程分派与波前（wavefront/thread group）调度","text":"GPU 硬件架构与渲染管线概述 GPU 是并行计算密集型硬件，在移动SoC中负责加速图形渲染。其硬件模块包括： 算术逻辑单元（ALU）或统一着色器单元（执行顶点/片元着色器） 光栅化单元（将图元转换为像素/片元） 纹理采样单元（纹理映射与滤波） 像素合成单元（ROP/Output Merger），将像素结果写入帧缓冲，进行混合、深度测试等操作 Z/Stencil测试单元，执行早期深度剔除、模板测试等以减少不必要的计算 缓存层次结构（如顶点缓存、纹理缓存、片元缓存、片元缓存(Tile Cache)、L1/L2 Cache等，用于减少主内存访问延迟） 指令调度器/任务调度器, 管理线程分派与波前（wavefront/thread group）调度 这些模块在渲染管线各阶段协同工作：顶点处理模块执行模型变换、视锥剔除和光栅化设置；裁剪/剔除阶段剔除不可见图元；光栅化单元将可见三角形转换为片元；片元着色器在统一着色器中执行逐像素计算；最后深度测试、混合和写回操作将最终颜色写入帧缓冲。例如，GeForce6架构的GPU在栅格化阶段使用专门的Z-cull模块快速丢弃被更近几何遮挡的像素；在片元处理阶段，纹理单元从内存中加载并过滤纹理数据，再由片元着色器执行并隐藏纹理访问延迟；最终，片元在Z测试和混合单元处理后写回帧缓冲。现代GPU支持统一着色器架构，即顶点、片元（和可选的几何、计算等）共用同一组ALU流水线，可动态分配着色器工作，以提高利用率。缓存方面，GPU通常具有专用的顶点/几何缓存和纹理缓存，以及可供片元处理访问的小容量帧缓冲缓存，辅以更大容量的L2缓存，尽量减少对主内存的访问。 在渲染管线各阶段中，各硬件模块配合如下：输入顶点首先经由顶点着色器（在统一着色器中）计算变换和属性，然后通过裁剪和图元组装模块形成三角形。可见的三角形传递给光栅化单元，该单元使用硬件加速的扫描转换算法计算覆盖的片元坐标，并生成深度测试数据。每个生成的片元会经由片元着色器（片段着色器）执行逐像素光照、纹理采样等计算，其中纹理单元负责高效访问和滤波纹理数据。GPU调度单元则负责管理大量并行线程（或称“线程束”/“波前”），动态分配就绪线程到ALU单元执行。在多线程环境下，GPU采用SIMD方式并行处理一组片元（通常称为“像素四元组”或更宽），同时可遮挡剔除（深度测试）并行进行，以避免过度计算。最终的深度和颜色测试、模板测试、混合等操作在固定功能单元（ROP）中完成，结果写入帧缓冲或渲染目标。需要注意的是，在移动SoC中，GPU往往与CPU共享统一内存，片元缓存和深度缓存多使用片上高速存储，以降低高带宽DDR访问次数。 GPU 架构类型：IMR、TBR、TBDR 不同GPU供应商采用不同渲染架构来平衡性能与功耗。**即刻模式渲染（IMR, Immediate-Mode Rendering）**传统地按照API提交顺序“流水线”地处理三角形：顶点着色→裁剪→光栅化→逐个片元着色，整个帧缓冲作为活跃工作集，着色过程中对帧缓冲进行随机访问。IMR的优势是实现简单，对流水线无特殊需求，但缺点是片元着色会频繁跳跃访问帧缓冲，工作集往往是全屏大小（高分辨率时工作集可达数十MB），需要大量外部带宽，且着色操作可能对位置无序的像素进行冗余计算。典型代表为传统桌面GPU或早期移动GPU，例如高通Adreno系列长期以来基本采用近似IMR模型（或称“简化瓦片渲染”），依赖复杂的帧缓冲缓存和早期深度测试减少工作量。有资料指出Adreno在必要时可退化到真IMR模式。 瓦片基渲染（TBR, Tile-Based Rendering）将帧缓冲划分为若干小瓦片（如16×16像素），渲染分两阶段完成：首先在几何阶段对所有图元执行顶点变换并进行分桶（Binning），将每个图元的列表按照会影响的瓦片进行记录；然后按照每个瓦片顺序进行光栅化，每次仅处理一个瓦片内的图元，并将瓦片颜色/深度数据加载到片上高速缓存中，处理完毕后再写回内存。这样做的优势在于：瓦片工作集很小，色深Stencil可全部驻留在片上快速存储，减少了对外部内存的随机访问。由于只需将最终瓦片数据一次性写出，TBR大幅降低了内存带宽开销（尤其是在移动设备有限带宽下优势明显）。缺点是TBR需要额外的几何分桶硬件，以及整个渲染必须等待几何阶段全部完成后才执行片元阶段（增加了延迟和硬件复杂度），同时分桶缓冲区需要占用片上或片外内存，若几何非常复杂时可能形成瓶颈。典型的TBR架构GPU包括ARM Mali系列和Imagination PowerVR系列（虽PowerVR进一步延伸为TBDR），它们通常使用16×16或更小的瓦片。 延迟式瓦片渲染（TBDR, Tile-Based Deferred Rendering）是PowerVR等架构引入的瓦片渲染变体，其核心思想是尽可能推迟昂贵的纹理采样与片元着色，先执行完全的可见性判断（HSR）再对可见像素进行着色。TBDR同样对场景分割瓦片，首先收集并划分几何，随后在每瓦片内利用深度信息确定可见表面，最后只对可见像素运行片元着色（推迟过度绘制），且可对透明对象额外处理。PowerVR的TBDR架构在每个瓦片内常被比作类似光线投射的过程，以确定最近表面。这种架构的优势是原理上消除了不可见像素的处理，降低了过绘（overdraw）带来的多余计算，非常适合片上片外带宽受限的移动平台；同时，每个瓦片数据可完整驻留片上内存中，一次输出。劣势是实现复杂，需要跟踪每个像素的最前表面ID，并在遇半透明时回写已着色像素，还需要较大片上内存（PowerVR最早代每像素只跟踪深度及ID）。典型代表是Apple的GPU（采用TBDR方案）和Imagination PowerVR（Series 1/2）。对比来看，IMR适用于对延迟敏感、流水线推理简单的场景；TBR适合几何复杂度中等且需节省带宽的场合；TBDR在带宽极其紧张或过绘消除要求高时最优。例如Apple在WWDC提到其移动GPU为TBDR，并强调通过HSR彻底降低了过绘；ARM文档也指出瓦片渲染显著降低内存带宽需求。各家厂商在此基础上发展出不同混合模式：有资料称Adreno采用灵活的瓦片大小和模式（“FlexRender”），可以在一些场景下退化为IMR；Mali也在新一代架构（Valhall）中改进线程和调度，但核心仍为TBR。 硬件层面优化技术 现代GPU通过多种硬件技术减少冗余工作和带宽消耗： 隐藏表面移除（Hidden Surface Removal, HSR）：在任何片元着色前，通过深度信息剔除被遮挡的像素。Apple GPU在TBDR阶段使用可渲染瓦片的片上深度缓存，在运行片元着色前先计算每像素的前端可见三角形，仅对可见像素最终着色，理论上消除所有不透明像素的过绘。Mali新一代GPU也引入了**片元预处理（Fragment Pre-pass）**机制，硬件自动执行一次隐式的Z预通道（类似软件的Z pre-pass）来筛选可见像素，再进行实际着色。HSR能够显著降低过绘次数和片元着色次数（Apple称即使提交顺序不按深度，也能保持像素级的最前表面），但需付出额外前处理复杂度和资源。当存在半透明时，GPU可能回退到正常着色流程，对透明对象执行“flush”操作。应用层面可配合HSR：按渲染顺序绘制不透明物体优先，再绘制可丢弃片元（alpha-test）物体，最后绘制透明物体，以最大化HSR效率。 深度剔除（Z-culling / Early-Z）：在片元进入片元着色器前先做深度测试，快速丢弃远端像素。NVIDIA等桌面GPU中常见专门的Z-cull硬件，可在像素进入计算单元前根据深度值屏蔽掉不可见片元。移动GPU也普遍支持早期深度测试，例如在多级深度缓存（Hierarchical Z）中快速判断一整块像素是否全被遮挡，只对剩余像素执行细粒度深度测试，减少后续流水线负担。ARM Mali的新机制则是将这种思想延伸到更主动的预通道，通过硬件第一次剔除绝大多数不可见片元，从而避免了传统的“深度测试＋丢弃”流程中的多余片元计算。 瓦片缓存（Tile Cache / Tile Memory）：瓦片渲染架构在片元阶段使用的本地片上RAM。每个瓦片通常可以同时存储该区域所有像素的颜色、深度和模板数据。例如，据开源驱动透露，老款iOS设备每像素可在片上存储128位数据（颜色+深度+模板），新设备可达512位。类似地，ARM Mali一块16×16瓦片需要4KB本地RAM来保存深度和颜色数据。这些片上缓存使得全帧写回带宽大幅降低；清除操作可以直接在片内完成，无需读写全局内存。另外，瓦片缓存天然适合多重采样抗锯齿（MSAA）实现：GPU仅在本地存储多重采样像素，分辨率降低操作（resolve）在写出前于片上完成，大幅节省带宽。 着色器指令重排序：一些新GPU架构（如NVIDIA Ada）支持运行时动态重排序着色器片段，以提高ALU利用率。移动GPU更多依赖编译器在编译阶段进行指令调度，以减少流水线气泡。例如ARM新一代Valhall架构取消了固定发射组，将硬件进行更多出序调度，从而提高指令执行并行度。同时，应用开发上应尽量避免使片元着色器中包含阻塞指令（如动态分支、大量纹理Fetch），以便硬件更有效利用多线程并行。 内存带宽优化：移动GPU常用各种压缩技术减少内存访问。ARM提出的**帧缓冲压缩（AFBC）是一种块压缩方案，对渲染输出纹理进行无损压缩，显著降低读写带宽。AFBC在GPU写回帧缓冲时压缩数据，随后显示或后续读取时解压缩，能在写出和读回两端节省带宽。此外，TBDR架构引入可编程混合（Programmable Blending）和无内存渲染目标（Memoryless Render Targets）**等特性：前者允许在片上直接读取当前瓦片的像素数据，合并多个渲染通道为一步，减少跨通道读写；后者让开发者声明某些中间附件为“无内存”模式，只在片上保留数据，不分配主存，从而节省资源。这些技术共同作用，有效压缩和省略了不必要的内存传输。 典型移动GPU平台分析 高通 Snapdragon (Adreno GPU)：Adreno系列传统上采用近似即时模式渲染，依赖较大片上缓存（称为GMEM）和强大的硬件裁剪来减少带宽。最新研究表明，Adreno GPU实际上实现了可配置的瓦片渲染：例如Adreno 540的GMEM约1MB，可以支持128×128的大瓦片；并且通过“FlexRender”可动态调整瓦片大小，在需要时退化为真正的IMR模式。调度上，Adreno以SIMD多线程方式运行，每条指令横跨多个片元并发执行（类似“warp”）。硬件支持早期深度测试和粗粒度的HSR，但不像TBDR那样完全推迟着色。驱动方面，Qualcomm提供Vulkan/OpenGL ES驱动优化指导，建议减少绘制调用数量、避免无用状态切换，并充分利用实例化和批处理等特性。整体而言，Adreno对几何带宽敏感，建议提前进行丢弃（例如使用剔除技术）、合理组织顶点索引和内存访问，以配合其混合的渲染模式。 Apple M 系列 (Apple GPU)：Apple自主设计的GPU为TBDR架构，与其统一内存架构（UMA）紧密耦合。GPU没有独立显存，CPU/GPU共享片上内存，但GPU拥有专用的高速瓦片内存。根据开源分析，M1 GPU可同时运行24个线程组（threadgroup），每组最多1024个线程，总共可并发执行24576个线程；每个线程组有约208KB寄存器文件。硬件采用标量ALU和向量I/O，支持16位浮点和高效的指令发射。Apple GPU完全采用TBDR流程：先将几何着色和分桶完成到片上缓冲，然后对每瓦片执行HSR，再仅对最终可见像素运行片元着色。加载/存储操作（Load/Store Actions）由开发者通过Metal API明确指定，以控制瓦片内存的使用。Apple GPU在可见表面确定后才着色，默认取消传统的顶点属性读硬件，所有状态由着色器取代，使硬件更为简洁。由于UMA特性，CPU与GPU访问同一内存，减少了额外复制和一致性操作（Metal在驱动层负责资源驻留管理）。综合来看，Apple GPU利用TBDR和大量线程并行，在移动平台上实现了优秀的带宽使用效率和高吞吐。 ARM Mali (Immortalis/Mali 系列)：早期Mali（Utgard/Midgard）架构以片上16×16瓦片为渲染单元，采用TBR流程。现代Mali（Bifrost/Valhall）继续使用TBR，但在线程调度上已进化：早期Mali每个片元流水线很窄（如4宽SIMD），新Valhall已经增加到16宽warp。Mali GPU拥有可配置核心数量，每核含统一着色引擎、多级纹理单元和深度缓存。驱动通过ARM提供的资源（如OpenCL/Vulkan指导）优化几何带宽使用，尽量将可丢弃几何在分桶阶段剔除。Mali也支持早期深度测试和多重采样抗锯齿。在缓存一致性方面，Mali与CPU共享内存需手动同步（如用API内存屏障）；ARM平台通常通过驱动保证CPU/GPU可以安全协作。异构计算方面，Mali及其平台经常与big.LITTLE CPU配合，使用OpenCL/Vulkan或厂商中间件实现CPU-GPU协同任务（如基于GPU的后处理、神经网络加速等）。 总体对比：Adreno偏向灵活的“半瓦片”IMR模式以降低复杂度，强调几何带宽；Apple GPU完全采用TBDR以最大化过绘剔除和带宽节约；Mali则典型地以TBR折中，依赖分桶高效利用片上缓存。在调度上，三者均为统一着色器多线程执行，但Warp/ThreadGroup宽度和调度策略不同（Adreno和Mali类似于NVIDIA的warp，Apple则采用Metal定义的线程组概念）。缓存一致性方面，Apple UMA架构优势明显；Adreno/Mali依赖系统内存，通信延迟稍大。异构计算上，三者都支持通用计算API，但Apple更倾向Metal+Metal Performance Shaders生态，Snapdragon/Mali则依赖Vulkan/OpenCL等跨平台API。 GPU 驱动与图形API协作 在图形API层面，驱动程序负责将CPU提交的渲染命令高效地转换为GPU执行单元的工作负载，并优化资源和状态使用。对于Vulkan，开发者通过命令缓冲(Command Buffer)明确记录绘制和计算指令，驱动在提交时逐条下发给GPU。Vulkan强调多线程并行命令录制和显存管理：建议将命令缓冲构建并行化、避免过度分散的小提交、重用管线对象和缓冲池。驱动会对图形管线状态做缓存（Pipeline Cache）和编译优化，减少绑定流水线（vkCmdBindPipeline）的次数；同时利用渲染通道(Render Pass)和亚通道(Subpass)机制，帮助GPU按瓦片更高效地利用片上内存。在数据传输方面，Vulkan驱动管理主机侧和显存侧的资源；通过内存屏障和同步原语（比如vkQueueSubmit和vkPipelineBarrier）协调CPU/GPU访问。当GPU执行绘制或计算任务时，驱动确保所需资源已经驻留，并且在必要时执行异步数据上传。高级优化包括使用描述符集(Descriptor Set)来绑定大量纹理和缓冲，使用推送常量和动态UAV减少驱动开销，以及在渲染路径中尽可能减少状态切换（例如一次绑定多个帧缓冲附件，一并绘制）等。 对于Metal（Apple专有API），驱动利用硬件的TBDR特性，开发者明确指定每个渲染目标的加载/存储操作（Load/Store Action），以控制瓦片内存的使用和清空。例如，Metal要求在每个渲染通道开始时声明是否需要加载之前的颜色/深度数据或清除它们，这直接映射到Apple GPU的载入/存储操作。Metal还提供了资源堆(Heap)和Argument Buffer等功能，允许更灵活地管理纹理和缓冲，并通过预编译的**图形管线状态对象（PSO）**减少运行时开销。由于Apple硬件和Metal深度协同优化，Metal驱动会尽量复用片上资源，对纹理数据和帧缓冲实现自动压缩。Metal中的多线程渲染（如MTLCommandQueue和MTLCommandBuffer）可以在CPU多核上并行填充命令，而GPU异步消费。值得一提的是，Metal API设计充分考虑了TBDR特点，比如推荐先绘制不透明再透明、使用Memoryless模式、尽量减少跨帧读回等（均可极大提升TBDR效率）。 在执行图形任务时，驱动和API还负责GPU资源调度：比如智能地调度提交队列、在多个硬件上下文（graphics/compute）间分配执行资源，以及应用后端合并（batching）和前端剔除技术来减少CPU与GPU之间的同步等待。高级技术如异步计算队列允许图形和计算任务并行执行，从而提高硬件利用率（需要驱动管理好依赖）。无论是Vulkan还是Metal，优化原则包括：并行化命令生成、多用批量提交、避免不必要的管线切换和状态绑定、使用内存屏障确保一致性，以及充分利用硬件压缩与内存连续性等。例如NVIDIA建议尽可能减少vkCmdBindPipeline调用，因为每次绑定代价高昂；对于Metal，则建议利用可编程混合和Memoryless模式合并多通道操作、减小带宽开销。 游戏开发者与渲染器设计优化建议 减少过绘：尽量按不透明→可剪裁→透明顺序绘制对象，以利用HSR/Early-Z尽早丢弃被遮挡像素。对于不可见面做剔除、深度预通道等策略可提高效率。尽可能避免在片元着色器中使用会修改深度的操作（如discard或在深度反馈后再绘制），因这些操作会让早期深度测试失效。 优化填充率（Fill Rate）：利用多边形细分和LOD（层次细节）减少三角形数量，避免在低分辨率下绘制极多小三角形。开启MSAA时可利用TBR架构优势，仅在片上缓存中进行采样融合，无须写出多份帧缓冲。 纹理和内存带宽优化：采用压缩纹理（如ASTC、RGBA8等），使用GPU支持的帧缓冲压缩（AFBC）格式存储渲染目标，减少读写流量。对于中间中间纹理，可考虑Memoryless存储模式（不分配片外内存）。尽量在片上执行可能的计算，减少来回交换数据次数。 管线状态管理：批量提交绘制调用、复用管线和资源。使用现代API的管线缓存（Vulkan pipeline cache、Metal PSO）预编译着色器，避免运行时重新编译。将共享状态（如渲染目标格式、混合模式）相同的绘制合并到同一管线调用中，减少vkCmdBindPipeline、vkCmdBindDescriptorSets等指令次数。使用动态偏移量或推送常量来替代频繁的缓冲绑定切换。 命令缓冲录制并行化：在Vulkan中，将命令缓冲的录制任务分散到多线程，以降低CPU提交瓶颈。合理规划每帧命令结构，避免太多小提交导致驱动负担。尽量重用命令缓冲和内存池，以减少分配/销毁开销。 利用硬件特性：针对TBDR GPU，应明确使用API提供的加载/存储选项（load/store actions），只加载必要的缓冲数据（无需清除时直接clear），以充分利用片上缓存效率。对于支持可编程混合的GPU，可合并多通道后处理pass为一次绘制，以减少中间帧缓冲写读。同时，根据GPU架构特性调整工作负载：例如在Adreno上尽量避免Geometry Shader或其他可能破坏分桶的特性；在Mali上注意几何带宽，尽量使用Instance Rendering减少顶点负载。 Shader优化技巧 避免使用高计算成本的数学函数 方法：减少使用如 pow、exp、log、sqrt、sin、cos 等复杂数学函数。可以用近似算法或预先查表的方式替代。 原理：这些函数通常没有直接的硬件支持，需要分解成多条基础计算指令，增加运算负担。 减少 if 语句和 discard 的使用 方法：使用像 step()、lerp()、saturate() 这样的数学函数来代替 if 语句，并尽量避免用 discard。 原理：GPU 使用并行的 SIMD 模型，if 会导致线程不同步，影响性能；discard 会让像素被跳过 Early-Z 流程，降低渲染效率。 控制纹理采样次数 方法：合并重复使用的通道、共享计算结果，或提前在顶点阶段进行处理。 原理：纹理采样是显存读取操作，成本较高，频繁采样会增加延迟。 使用较低精度的数据类型 方法：如果变量不需要高精度（如颜色、UV 坐标、法线等），可以用 half、fixed 或 lowp 来替代 float。 原理：低精度类型可以减少内存占用，使 GPU 能执行更多线程，提高效率。 把计算尽可能放到顶点着色器中 方法：可预测的计算先在 vertex shader 中处理，再通过 varying 传给 fragment shader。 原理：顶点着色器每个顶点执行一次，而片元着色器每个像素执行一次，把计算前移能显著降低总体负担。 移除无用的 varying 变量和多渲染目标（MRT） 方法：检查并删除未使用的输出变量和渲染目标，避免资源浪费。 原理：这些多余的数据会增加显存传输和写入成本，对性能不利。 合理使用 Early-Z 和 ZWrite 方法：优先绘制不透明物体并开启深度写入，可以利用 GPU 的 Early-Z 优化。 原理：Early-Z 可以在执行片元着色器前丢弃被遮挡的像素，节省计算。 在一张纹理中打包多个属性 方法：将 AO（环境光遮蔽）、光泽度、粗糙度、高光强度等属性分别存储在 RGBA 四个通道中。 原理：打包可以减少纹理数量，从而减少采样次数和显存使用。 压缩小数据并使用 bit mask 操作 方法：将多个布尔值或小范围整数压缩成一个字节或使用位操作管理。 原理：压缩后的数据能有效减少内存和带宽占用，常用于延迟渲染等场景。 减少像素重绘（Overdraw） 方法：通过 Z-Prepass（预先写入深度）优先绘制前景物体，避免重复绘制被遮挡部分。 原理：如果一个像素被多次绘制，会让 fragment shader 重复执行，浪费性能。","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"优化","slug":"优化","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"UE5启动流程与结构解析","slug":"UE5/UE5启动流程与结构解析","date":"2025-05-19T12:04:05.000Z","updated":"2025-08-09T13:53:01.077Z","comments":true,"path":"2025/05/19/UE5/UE5启动流程与结构解析/","link":"","permalink":"http://yoursite.com/2025/05/19/UE5/UE5%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/","excerpt":"UE5启动流程与结构解析 一、引擎启动流程详解 1.1 启动入口与平台特化 在 Windows 平台上，Unreal Engine 5 的执行始于标准 GUI 程序入口 WinMain。该入口函数首先调用平台特化的 LaunchWindowsStartup（位于 LaunchWindows.cpp），随后进入通用的跨平台启动函数 GuardedMain（定义于 Launch.cpp）。其调用链条如下： 1WinMain → LaunchWindowsStartup → GuardedMainWrapper → GuardedMain → EnginePreInit → FEngineLoop::PreInit → FEngineLoop::PreInitPreStartupScreen","text":"UE5启动流程与结构解析 一、引擎启动流程详解 1.1 启动入口与平台特化 在 Windows 平台上，Unreal Engine 5 的执行始于标准 GUI 程序入口 WinMain。该入口函数首先调用平台特化的 LaunchWindowsStartup（位于 LaunchWindows.cpp），随后进入通用的跨平台启动函数 GuardedMain（定义于 Launch.cpp）。其调用链条如下： 1WinMain → LaunchWindowsStartup → GuardedMainWrapper → GuardedMain → EnginePreInit → FEngineLoop::PreInit → FEngineLoop::PreInitPreStartupScreen 1.2 GuardedMain 函数结构 GuardedMain 函数作为整个引擎生命周期的主控流程，其主要职责包括平台抽象层初始化、引擎子系统注册、核心模块加载、主循环驱动以及资源清理。简化伪代码如下： 123456789101112131415GuardedMain(const TCHAR* CmdLine)&#123; // 初始化阶段 EnginePreInit(); EngineInit(); // 主执行循环 while (!IsEngineExitRequested()) &#123; EngineTick(); &#125; // 引擎析构与清理 EngineExit();&#125; 1.3 初始化子系统分层解析 命令行与环境配置：通过 FCommandLine 解析参数，初始化全局环境变量如 GIsEditor、GIsClient。 平台与IO子系统：包括 FPlatformFileManager 构建虚拟文件系统层，配置加载器（GConfig）以及多日志通道日志器（GLog）。 模块加载与引擎实例化：FEngineLoop::PreInit 会加载核心运行时模块（如 Core、CoreUObject、Engine），随后 FEngineLoop::Init 生成 UEngine 或 UEditorEngine 实例。 世界初始化：通过 UEngine::Start() 加载默认世界并生成 UWorld 实例，注册 GameMode、PlayerController 等运行时关键对象。 1.4 模式差异化执行路径 编辑器模式：实例化 UEditorEngine，加载 Slate 编辑器模块和各种工具集。 独立运行模式：使用 UGameEngine，仅保留运行时必要模块，避免加载额外编辑器负担。 1.5 原生窗口创建与生命周期管理 Unreal Engine 在 Windows 平台下使用 Win32 原生窗口进行渲染上下文承载和消息收集，其生命周期管理如下： 创建时机：在 FWindowsPlatformApplicationMisc::InitializeWindow()（位于 WindowsApplication.cpp）被调用时创建原生窗口。该函数在 FEngineLoop::Init() 完成渲染器初始化（RHI）后执行。具体过程包括： 准备窗口类（WNDCLASSEX）并注册到系统。 调用 CreateWindowEx 或 CreateWindowW 创建窗口句柄（HWND）。 更新机制：主循环内调用 FWindowsApplication::PumpMessages() 轮询 Win32 消息队列，通过 PeekMessage/GetMessage 接收消息；再在 TranslateMessage 与 DispatchMessage 后，Win32 调用窗口过程 WndProc，进一步路由到 FWindowsApplication::ProcessMessage()。 交换链与渲染上下文：与原生窗口关联的 RHI 交换链（DX11/DX12/Vulkan SwapChain）在窗口创建后初始化，并在每帧通过 SwapBuffers 或 Present 交换前后缓冲区。 销毁时机：当引擎接收到退出请求后，FWindowsApplication::DestroyWindows() 会依次遍历所有 HWND，调用 DestroyWindow 释放窗口资源；随后在 GuardedMain 的退出阶段调用 UnregisterClass 注销窗口类。 1.6 原生窗口与渲染管线关联 为了将 Win32 窗口与 GPU 渲染流程绑定，Unreal Engine 在 RHI 初始化阶段执行以下关键步骤： 创建渲染设备与命令队列：在 PlatformCreateDynamicRHI()（如 D3D12RHI.cpp 中）调用 D3D12CreateDevice 创建 ID3D12Device，并初始化图形和显示命令队列（ID3D12CommandQueue）。 窗口与交换链关联：随后调用 IDXGIFactory::CreateSwapChainForHwnd（封装在 D3D12RHI::CreateSwapChain()）并传入之前创建的 HWND 与命令队列，生成 IDXGISwapChain3 对象。该交换链会管理前后缓冲区并负责与窗口表面同步。 渲染目标视图（RTV）绑定：在交换链创建后，RHI 会为每个缓冲区调用 CreateRenderTargetView（RTV），并在每帧 RHIPresent 时使用 OMSetRenderTargets 将 RTV 绑定到管线输出合并阶段。 帧呈现：每次 RHIPresent 调用时，通过 IDXGISwapChain3::Present 将渲染完成的后缓冲区呈现到原生窗口。 这样，游戏逻辑和 Slate 渲染均可通过 RHI 层透明地提交命令到与 HWND 绑定的交换链，实现最终画面输出。 二、模块化架构与加载机制 2.1 模块生命周期管理器 FModuleManager FModuleManager 为模块化架构的核心调度器，负责模块的动态解析、生命周期控制与实例缓存。其提供以下能力： 加载：LoadModule(), LoadModuleWithFailureReason() 卸载：UnloadModule() 访问：GetModule(), IsModuleLoaded() 模块以 IMPLEMENT_MODULE 宏形式注册，启动时触发 StartupModule()，终止时调用 ShutdownModule()。 2.2 模块元描述与阶段控制 模块的描述由 FModuleDescriptor 承载，定义于 .uproject 或 .uplugin 配置文件中，其关键字段如下： Name：模块唯一标识 Type：模块分类（Runtime、Editor、Developer 等） LoadingPhase：加载时机（PreDefault、Default、PostEngineInit 等） 引擎启动阶段通过 LoadModulesForPhase() 自动分阶段解析和装配模块。 2.3 静态与动态模块机制对比 静态模块：构建时链接进可执行文件，通过 StaticallyLinkedModuleInitializers 注册初始化函数。 动态模块（DLL）：运行时使用 FPlatformProcess::GetDllHandle() 加载，调用 GetDllExport() 提取 InitializeModule() 等符号以完成动态注册。 模块卸载遵循逆序清理原则，保障依赖顺序的一致性与资源完整回收。 2.4 模块加载与调用流程 在引擎启动阶段，以及运行时需要动态引入或卸载功能时，Unreal Engine 依赖 FModuleManager 结合项目和插件描述完成模块的发现、加载、初始化及调用。 2.4.1 模块发现与注册 启动时扫描：引擎启动时，FProjectManager 和 FPluginManager 分别读取 .uproject、.uplugin 中的 FModuleDescriptor 列表，并将所有声明的模块按 LoadingPhase 分组。 静态注册：对于单片（Monolithic）构建模式，所有模块在编译时通过 IMPLEMENT_MODULE 宏将初始化委托注册到 StaticallyLinkedModuleInitializers 映射中；插件和项目模块也以同样方式嵌入可执行文件或主 DLL。 2.4.2 动态加载流程 调用 LoadModulesForPhase(Phase) 时，FModuleManager 枚举本阶段所有 FModuleDescriptor。 对于每个模块名，FModuleManager::LoadModuleWithFailureReason： 若为静态模块，直接从 StaticallyLinkedModuleInitializers 调用委托，返回 IModuleInterface 实例。 若为动态模块，使用 FPlatformProcess::GetDllHandle 在预定义路径（Engine、Project、Plugin 二进制目录）加载对应 DLL；再通过 GetDllExport 查找符号 \"InitializeModule\"，执行返回的新模块实例。 将生成的 IModuleInterface 指针保存在 ModuleNameToInfo 映射，调用 StartupModule() 完成模块自身初始化逻辑。 广播 ModulesChangedEvent 通知其他子系统，如 UObject 加载器，注册由模块提供的类或服务。 2.4.3 引擎对模块的调用 接口查询：运行时代码可通过 FModuleManager::Get().GetModuleChecked(ModuleName) 获得已加载模块的接口引用。 服务注入：模块通常在 StartupModule 中向全局子系统注册服务（如渲染模块注册渲染工厂，网络模块注册网络驱动），引擎通过静态或虚函数调用这些接口完成对应功能。 生命周期管理：当模块完成其职责或需要热重载时，调用 UnloadModule(ModuleName) 会按逆序调用 ShutdownModule()，并释放 DLL 句柄。 2.4.4 热重载支持 编辑器模式和某些运行时插件支持 Hot Reload：在源码或插件代码修改后，调用 LiveCoding 或 RecompileInEditor 可以卸载旧模块并重命名加载新 DLL，FModuleManager 确保在重载前调用所有模块的 ShutdownModule，再重新执行加载和 StartupModule 过程。 三、引擎主循环机制剖析 主循环逻辑位于 FEngineLoop::Tick()，该函数被 GuardedMain 持续调用，驱动引擎完成一帧游戏更新。其核心逻辑结构如下： 12345678while (!IsEngineExitRequested())&#123; // 处理窗口消息 FPlatformApplicationMisc::PumpMessages(true); // 主游戏引擎tick (world, game objects, etc.) GEngine-&gt;Tick(FApp::GetDeltaTime(), bIdleMode);&#125; 3.1 各子阶段功能拆解 世界 Tick：依序更新每个 UWorld 实例，包括其内部所有 AActor 和组件。 子系统更新：物理仿真、动画控制器、AI 系统、音频引擎、GC 系统等依照特定顺序被调度。 在编辑器模式下，还需驱动多视口、多编辑器对象 Tick，进一步增加主循环复杂度。 四、核心运行时对象结构 4.1 UWorld：多子系统集成容器 UWorld 是游戏世界的运行时表示，其核心职责包含： 管理 ULevel 和动态加载的 Streaming Levels 管理所有 AActor 实例的生命周期与调度 提供时间流控制（World Time、DeltaTime、Pause 等） 持有所有关键子系统的引用，如 UPhysicsScene、UNavigationSystemV1、UAIController 等 关联 AGameModeBase、APlayerController、AGameState 等游戏规则对象 其结构设计支持多个世界并行存在。 4.2 AActor：游戏对象原语单元 AActor 是所有可交互、可放置、可网络同步实体的基类。其设计支持以下关键能力： 空间坐标系定义（Transform） Tick 生命周期方法：BeginPlay() → Tick() → EndPlay() 支持组件组合系统，通过 UActorComponent 实现功能模块化 支持网络属性同步与远程函数调用（RPC）机制 支持蓝图与 C++ 混合开发与扩展 常见子类如 APawn、ACharacter、AStaticMeshActor 等均继承自 AActor。 4.3 组件化与解耦设计 UActorComponent 是功能原子单元，可被多个 Actor 复用 SceneComponent 派生类（如 Mesh、Camera、Light）具备空间信息 支持运行时动态添加、编辑器中组合，可提高模块内聚性与解耦能力 组件体系提升了可扩展性、降低逻辑冗余并促进代码复用，是 UE 面向数据驱动设计的重要体现。 五、外部输入处理机制（Windows 平台） 5.1 平台消息获取与分发 在 Windows 平台上，引擎通过 Win32 API 接口获取原生消息队列事件（如 WM_MOUSEMOVE, WM_LBUTTONDOWN, WM_KEYDOWN, WM_INPUT 等）。FWindowsApplication::PumpMessages() 会在主循环中调用 TranslateMessage 与 DispatchMessage，并在 WndProc 中将消息转发给 FWindowsApplication::ProcessMessage()。 123456MSG Msg;while (PeekMessage(&amp;Msg, NULL, 0, 0, PM_REMOVE))&#123; TranslateMessage(&amp;Msg); DispatchMessage(&amp;Msg);&#125; 5.2 消息处理与转换 ProcessMessage(HWND Window, uint32 Message, WPARAM wParam, LPARAM lParam) 会根据消息类型调用对应的处理函数，例如： 鼠标事件：ProcessMouseButtonDown/Up, ProcessMouseMove, ProcessMouseWheel。 键盘事件：ProcessKeyDown/Up, ProcessKeyChar。 原始输入（Raw Input）：WM_INPUT 对触摸、手柄等应用多平台统一处理。 每个处理函数会构建对应的 Slate 事件对象（FPointerEvent, FKeyEvent）并调用 MessageHandler-&gt;OnMouseButtonDown() 或 OnKeyDown() 等接口。 5.3 Slate 层事件分发 FSlateApplication 作为统一 UI 框架入口，通过 FSlateApplication::ProcessDeferredEvents() 聚合并投递从平台层上报的事件。事件处理流： 平台层产生的原始事件通过 FGenericApplicationMessageHandler 回调注册到 Slate。 FSlateApplication::PumpMessages() 中调用 ProcessMessageQueue()，将消息排入 Slate 内部队列。 在 FSlateApplication::Tick() 阶段，遍历事件队列，调用 RoutePointerEvent 或 RouteKeyEvent，将事件分发给焦点窗口和对应 Widget。 5.4 引擎输入子系统 Slate 处理完 UI 输入后，会根据配置将输入路由至引擎输入子系统（UPlayerInput），生成 FInputKey, FInputAxis 或 FInputTouch 数据。主要流程： FSlateApplication 调用 FInputProcessorSlate::ProcessKeyDownEvent 或对应方法，将键盘/鼠标事件转为 UPlayerInput 的调用。 UPlayerInput 根据项目 DefaultInput.ini 中的映射，将物理键或控制器按钮映射为游戏内抽象的输入动作（动作）与轴（Axis）。 APlayerController::InputKey 或 InputAxis 接收这些事件，进一步调用绑定到 Actor 的 UInputComponent 中的委托。 5.5 控制器与触摸支持 控制器：Windows 使用 XInput（WindowsApplication.cpp 中的 FWindowsControllerInterface）轮询手柄状态，产生 FControllerState 并转为 FInputKey。 触摸屏：触摸事件通过 Win32 的触摸输入 API（WM_TOUCH），并在 ProcessMessage 中解析为 FPointerEvent，最终传递给 Slate。 5.6 整体事件流示意 12345678910111213Win32 Message Queue ↓ (TranslateMessage&#x2F;DispatchMessage)FWindowsApplication::ProcessMessage ↓ (构建 FPointerEvent&#x2F;FKeyEvent)FGenericApplicationMessageHandler → FSlateApplication ↓ (Slate 内部队列)FSlateApplication::Tick() ↓ (RoutePointerEvent&#x2F;RouteKeyEvent)焦点 Widget + Active Window ↓ (UI 处理完成)FInputProcessorSlate → UPlayerInput ↓ (映射 动作&#x2F;Axis)APlayerController → UInputComponent → Actor 六、主要UE5模块概览 在整体架构中，UE5 由若干基础模块和功能模块组成，每个模块在引擎启动和运行时通过 FModuleManager 驱动加载，并在引擎生命周期内被相应子系统调用。以下为关键模块及其设计结构、核心职责和驱动方式： 6.1 核心基础模块 Core：提供跨平台底层功能，包括内存管理、字符串与容器模板、文件系统接口。以静态方式链接，最先被加载，由 FEngineLoop::PreInit 驱动。 CoreUObject：UObject 系统与反射框架实现，管理对象生命周期、序列化与垃圾回收。通过静态注册委托加载，StartupModule 中初始化反射元数据。 6.2 引擎功能模块 Engine：GameFramework 核心，管理世界（UWorld）、GameMode、Actor 生命周期和场景更新。依赖 CoreUObject，在 PreDefault 阶段加载，StartupModule 注册世界管理器。 RenderCore：封装渲染流水线基础接口，如命令缓冲、资源管理。作为渲染子系统前置模块，在 Default 阶段加载，渲染线程启动时被绑定。 RHI：渲染硬件抽象层，提供对 DirectX、Vulkan、Metal 等后端的统一接口。动态模块，根据平台在 Default 阶段载入，渲染初始化流程中调用 CreateRHI。 6.3 UI 与输入模块 SlateCore：Defines 基本 UI 树结构、事件处理和布局算法。作为静态模块在 PreDefault 阶段加载，由 FSlateApplication 实例驱动。 Slate：UI 渲染与绘制实现，依赖 SlateCore 和 RHI，在 PostEngineInit 阶段加载，StartupModule 中注册渲染器。 UMG (UMGEditor)：基于 Slate 的可视化 UI 编辑与运行时模块，Editor 版在 Editor 阶段加载，Runtime 版在 Default 阶段加载，由 WidgetReflector 驱动。 InputCore：定义键位与轴映射基础数据结构。静态加载，UPlayerInput 在世界 Tick 前自动初始化并回调映射。 6.4 网络与游戏系统模块 Networking：核心网络协议与封包实现（Sockets、LowLevelNet）。动态模块，Default 阶段加载，在 NetDriver 初始化时被调用。 OnlineSubsystem：平台在线服务接口（如 Steam、Epic Online Services）。插件形态，运行时根据配置载入，对应子系统在 WorldInit 时注册服务。 GameplayAbility：提供技能（Ability）与效果（Effect）系统框架。模块在 PostDefault 阶段加载，StartupModule 中注册 UAbilitySystemComponent 工厂，GameMode 或 Actor 在构造时创建组件实例。 6.5 工具与扩展模块 Editor：编辑器核心功能集合，在 Editor 阶段加载，为编辑器注入菜单、工具窗口与自定义命令。 BlueprintGraph：蓝图可视化脚本支持，Editor 模式下加载，由蓝图编译器和可视化编辑器驱动。 LiveCoding：支持热重载的模块，运行时监听文件变化，在插件重载流程中被 FModuleManager 调用 ShutdownModule 和 StartupModule。 七、UE5 多线程架构 UE5 在运行时启动时，会创建多个专职线程来处理不同的子系统，以保证性能与资源利用。以下是主要线程的创建、更新与销毁位置及职责说明： 7.1 主要线程列表 游戏主线程 (Game Thread) 渲染线程 (Render Thread) RHI 线程 (RHI Thread) 任务图线程 (Task Graph Threads) 异步加载线程 (Async Loading Thread) 额外子系统线程（物理、音频等） 7.2 游戏主线程 创建时机：在 GuardedMain 内启动后，即进入 FEngineLoop::Init 后恢复到主线程上下文。 更新：每帧由 FEngineLoop::Tick() 驱动，处理游戏逻辑、UWorld Tick、Actor Tick 等。 销毁：当 IsEngineExitRequested() 为真退出主循环后，主线程在 GuardedMain 中执行 EngineExit 清理并终止进程。 7.3 渲染线程 创建时机：在 RHI 初始化阶段（如 D3D12DynamicRHI::Init()）调用 FRHICommandContext::InitializeResources() 时通过 FRunnableThread::Create 启动。 更新：在每帧渲染提交阶段，由 FRenderCommandFence 和 FRHICommandList 在渲染线程上下文中提交绘制命令。 销毁：在 D3DRHI::Shutdown() 或通用 RHIExit() 中调用 FRunnableThread::Kill 并释放线程对象。 7.4 RHI 线程 创建时机：与渲染线程类似，部分平台（如 Vulkan）在 CreateRHI 后为异步命令提交启动独立 RHI 线程。 更新：负责管理底层驱动命令队列、Fence 同步与交换链 Present 调用。 销毁：在 RHI 退出流程中依次停止并销毁。 7.5 任务图线程 创建时机：在 FTaskGraphInterface::Startup() 中，通过 FTaskGraphInterface::Get().Startup() 启动一组后台线程。 更新：按需执行 FGraphEvent 调度的任务节点，如资源加载、AI 逻辑、物理仿真子任务。 销毁：在引擎退出阶段 FTaskGraphInterface::Shutdown() 中回收所有任务线程。 7.6 异步加载线程 创建时机：在引擎初始化阶段 FAsyncLoadingThread::Init() 中，通过 FRunnableThread::Create 启动用于包/资源加载。 更新：不断读取 FAsyncLoadingThread 的请求队列，在后台加载资产并在完成时通知主线程。 销毁：在 FAsyncLoadingThread::Shutdown() 中停止线程并清理队列。 7.7 其他子系统线程 物理线程：如 FPhysScene::InitPhysScene() 可创建用于并行物理仿真的线程。 音频线程：在 FAudioDevice::Init() 中创建，用于音频混合与解码。 八、反射系统 Unreal Engine 的反射系统（Reflection System，又称 Property System）是 C++ 语言在运行时缺乏本地支持的情况下，为实现运行时类型查询、序列化、垃圾回收、网络复制及蓝图交互而设计的通用机制 。该系统由 Unreal Header Tool（UHT）在编译时生成元数据，并在运行时通过静态注册与 FArchive 等组件提供完整的元信息访问。 8. 1 核心元类：UObject 与 UClass UObject 是所有受反射支持对象的基类，定义于Engine/Source/Runtime/CoreUObject/Public/UObject/Object.h。它承载了 GetClass()、Serialize()、垃圾回收标记等核心方法。 UClass 是 UObject 类的元类，描述一个具体 UObject 派生类型的属性、函数列表和构造器。每个反射类在运行时都对应一个唯一的 UClass 实例，并保存于全局类注册表中。 8. 2 注解与 UHT 生成 UCLASS()：标记一个类使其参与反射，生成对应的 UClass 元数据。 USTRUCT()：标记一个 struct 参与反射，生成 UStruct 元数据。 UPROPERTY(...)：标记成员变量参与属性反射，生成 FProperty 元数据。 UFUNCTION(...)：标记成员函数参与方法反射，生成 UFunction 元数据。 所有宏定义与 UHT 针对这些标记生成的头文件，位于 .generated.h，并在编译时被包含于源文件末尾 8. 3 反射元数据对象 UStruct / UClass：分别保存结构体和类的字段（PropertyLink 链表）、父类指针、元数据（MetaData）等。 FProperty：所有属性的基类，子类如 FIntProperty、FStructProperty、FArrayProperty 等实现具体序列化与访问接口，定义于 Property.cpp。 UFunction：保存函数签名、参数列表与可调用指针，用于在蓝图或网络复制时动态调用。 UEnum：保存枚举类型信息，实现编辑器下枚举面板数据填充。 8. 4 反射注册机制 编译时生成 UHT 根据注解宏解析 C++ 源码，生成 .generated.h，其中包含： 123static void StaticRegisterNativesUMyClass();UClass* Z_Construct_UClass_UMyClass();template&lt;&gt; MYMODULE_API UClass* StaticClass&lt;UMyClass&gt;(); 这些函数最终注册到 Z_CompiledInDeferFile 数组中，延迟于运行时统一调用。 静态初始化 在模块加载（IMPLEMENT_MODULE）时，FModuleManager 调用 StartupModule，触发 Z_CompiledInDeferFile 中的 FRegisterCompiledInInfo，自动注册所有 UClass、UStruct、UEnum 等到全局注册表。 运行时访问 UMyClass::StaticClass() 返回对应的 UClass*，可用于动态创建实例或做类型判断。 MyObject-&gt;GetClass() 返回实例的 UClass*，支持 IsA()、Cast&lt;&gt;() 等运行时安全转换。 8.5 运行时类型信息 StaticClass / GetClass 12UClass* AMyActorClass = AMyActorClass::StaticClass();UClass* RuntimeClass = MyActorInstance-&gt;GetClass(); 前者通过模板实现，后者从 UObject 基类获取实例类型指针，二者均依赖全局注册表。 类型查询与转换 UObject::IsA(UClass*) 和 Cast(Object) 在底层调用 GetClass()-&gt;IsChildOf(DesiredClass)，实现安全的继承链检查与指针转换 动态创建对象 UE5中动态创建对象主要分为两类场景： 创建普通UObject派生类（非Actor对象） 使用 NewObject() 模板函数，直接通过UClass信息创建对象。 代码示例： 123// 假设存在一个反射类 UMyObject : public UObjectUClass* MyClass = UMyObject::StaticClass(); // 获取UClassUMyObject* MyObj = NewObject&lt;UMyObject&gt;(GetTransientPackage(), MyClass); 创建Actor派生类（需存在于游戏场景中） Actor必须通过UWorld::SpawnActor()方法生成，且需要指定位置和旋转信息。 代码示例： 12345// 假设存在一个反射类 AMyActor : public AActorUClass* MyActorClass = AMyActor::StaticClass(); // 获取UClassFVector SpawnLocation = FVector(100.0f, 100.0f, 100.0f);FRotator SpawnRotation = FRotator(0.0f, 0.0f, 0.0f);AMyActor* MyActor = GetWorld()-&gt;SpawnActor&lt;AMyActor&gt;(MyActorClass, SpawnLocation, SpawnRotation); - 关键点： - 必须通过UWorld上下文调用（通常在Actor或Component中使用GetWorld()）。 - Actor会自动注册到游戏场景中，并受引擎生命周期管理。 动态获取UClass的三种方法 若需通过字符串类名动态获取UClass，需结合反射系统： 使用 FindClass() 函数 12FString ClassName = TEXT(\"MyProject.MyObject\");UClass* TargetClass = FindObject&lt;UClass&gt;(ANY_PACKAGE, *ClassName); 限制：类必须已在内存中加载（如被蓝图引用或代码显式加载） 使用 FSoftClassPath（推荐） 12FSoftClassPath ClassPath(TEXT(\"/Game/Blueprints/MyActor.MyActor_C\")); // 蓝图类路径UClass* TargetClass = ClassPath.TryLoadClass&lt;UObject&gt;(); 优势：支持异步加载和热重载，适用于蓝图类。 路径格式：/Game/Path/To/Asset.AssetName_C（蓝图类需加_C后缀）。 通过静态类名直接获取 1UClass* TargetClass = LoadClass&lt;UObject&gt;(nullptr, TEXT(\"/Script/MyProject.MyObject\")); 适用场景：已知类的完整名称（C++原生类的路径格式为/Script/ProjectName.ClassName）。 九、串行化系统 9.1 什么是串行化？ 串行化（Serialization）是将对象数据转换为字节流（用于存储或传输）的过程，反串行化（Deserialization）是将字节流还原为对象的过程。在 UE5 中，串行化是构建以下系统的基石： 关卡和资源的加载与保存（.uasset、.umap） 对象的网络复制（Replication） 蓝图与编辑器属性持久化 SaveGame 系统 GC 跟踪对象引用 9.2 核心组件概览 模块 作用 核心类/结构 Archive 系统 底层读写抽象 FArchive（抽象基类） Property System 属性元数据反射与逐成员序列化 FProperty 及其子类 Linker 系统 资源级别的加载/保存管理器 FLinkerLoad, FLinkerSave Package 系统 uasset/umap 资源的封装与版本控制 UPackage, FPackageFileSummary Object Serializer 对象级别的序列化逻辑 UObject::Serialize()、FStructuredArchive 9.3 底层核心类详解 FArchive - 串行化的抽象基类 定义于：Runtime/Core/Public/Serialization/Archive.h 123456789class CORE_API FArchive&#123;public: virtual FArchive&amp; operator&lt;&lt;(class UObject*&amp; Value); virtual FArchive&amp; operator&lt;&lt;(class FName&amp; Value); virtual FArchive&amp; operator&lt;&lt;(int32&amp; Value); virtual FArchive&amp; operator&lt;&lt;(FString&amp; Value); ...&#125;; 特点： - 所有读写行为都通过重载 &lt;&lt; 操作符完成 - 可被继承形成不同上下文的读写器，如： - FMemoryReader / FMemoryWriter：对内存块操作 - FArchiveFileReader / FArchiveFileWriter：对文件操作 - FStructuredArchive：支持结构化序列化（分组、字段名等） FProperty - 元属性序列化 每个 UCLASS / USTRUCT 中声明了 UPROPERTY 的变量，会对应一个 FProperty 对象，自动遍历并串行化。 核心接口： 1virtual void SerializeItem(FArchive&amp; Ar, void* Value, void const* Defaults) const; 派生类示例： FIntProperty：int 类型字段 FStructProperty：嵌套结构体字段 FArrayProperty：TArray 类型字段 123FProperty* Property = ...;void* DataPtr = ...;Property-&gt;SerializeItem(Ar, DataPtr); UObject::Serialize() - 对象级别的自定义序列化 定义于 Object.cpp： 12345virtual void UObject::Serialize(FArchive&amp; Ar)&#123; // 引擎默认会遍历属性链表并调用 SerializeItem Super::Serialize(Ar);&#125; 每个 UCLASS 都可以覆写此函数，实现自定义的写入逻辑（但仍应调用 Super::Serialize 保证基础属性被序列化）。 FLinkerLoad / FLinkerSave - 资源串行化入口 FLinkerLoad：用于从 .uasset/.umap 文件中加载对象 FLinkerSave：用于保存对象到磁盘文件 关键函数： 12void FLinkerLoad::SerializeExport(UObject* Object);void FLinkerSave::SavePackage(); 这些类通常与 UPackage 关联，管理该包中所有对象的加载、名字查找、资源依赖。 9.4 结构化序列化格式：FStructuredArchive UE5 引入了新的结构化序列化 API，替代传统的线性 FArchive，提供更强的稳定性与版本支持。 核心结构： 123FStructuredArchive Archive(UnderlyingArchive);FStructuredArchive::FSlot Slot = Archive.Open();Slot.EnterRecord()-&gt;EnterField(TEXT(\"Health\")) &lt;&lt; MyHealth; 优势： 支持字段名 支持嵌套记录 更好的人类可读性（对 JSON/YAML 友好） 支持版本差异（Field Skipping） 9.5 序列化文件格式：.uasset 与 FPackageFileSummary 所有资源（蓝图、纹理、关卡等）最终都被存储为 .uasset 或 .umap 文件。其头部格式由以下结构描述： 1234567891011121314struct FPackageFileSummary&#123; int32 Tag; int32 LegacyFileVersion; FGuid CustomVersionContainer; int32 NameCount; FPackageIndex ExportCount; ...&#125;;这些字段由 FLinkerLoad 加载，决定后续如何解析对象及其依赖。定义位置：Runtime/CoreUObject/Public/UObject/PackageFileSummary.h 9.6 实际序列化流程（加载流程图） 123456LoadPackage() └──&gt; Create FLinkerLoad └──&gt; Read FPackageFileSummary └──&gt; Load Name Map, Export Map, Import Map └──&gt; Call UObject::Serialize for each export └──&gt; FProperty::SerializeItem() 逐字段读取 9.7 SaveGame 系统 SaveGame 系统 是一套用于将游戏中的状态（如玩家属性、关卡信息、物品等）序列化为磁盘文件，并在需要时恢复（反序列化）这些状态的机制。它提供了一个 高层封装的方式来保存和加载游戏数据，通常用于存档、存盘、断点续玩等场景 使用示例： 1234567891011121314USTRUCT(BlueprintType)struct FPlayerSaveData&#123; GENERATED_BODY() UPROPERTY() int32 Level; UPROPERTY() float Health; UPROPERTY() FString PlayerName;&#125;; 这段结构体在 SaveGame 中保存时，会通过 FStructProperty 自动进行字段遍历与序列化。 9.8 版本控制：FCustomVersion UE 支持多版本资源兼容，通过 FArchive::CustomVer() 查询： 12345int32 Version = Ar.CustomVer(FMyPluginVersion::GUID);if (Version &lt; SOME_VERSION)&#123; // 使用旧的反序列化方式&#125; 9.10 字节序 什么是字节序（Endian）？ Little Endian（小端）：低位字节在前（低地址） Big Endian（大端）：高位字节在前（低地址） UE 的主机平台（如 Windows 和 Linux）一般使用 小端 存储，因此 .uasset 文件默认也使用小端格式 核心处理类：FArchive UE 中字节序的读写是通过 FArchive 抽象类处理的。派生类中会根据平台和目标字节序做转换。 关键成员变量： 12bool FArchive::ForceByteSwapping;bool FArchive::IsPersistent; // 读写的是磁盘文件 核心逻辑： 每个派生类在读写整数等多字节数据时，都会调用如下代码来决定是否做字节翻转： 123456789101112131415161718192021222324252627void FArchive::ByteSwap(void* V, int32 Length)&#123; uint8* Ptr = (uint8*)V; int32 Top = Length - 1; int32 Bottom = 0; while (Bottom &lt; Top) &#123; Swap(Ptr[Top--], Ptr[Bottom++]); &#125;&#125;FArchive&amp; FArchive::SerializeByteOrderSwapped(void* V, int32 Length)&#123; if (IsLoading()) &#123; Serialize(V, Length); // Read. ByteSwap(V, Length); // Swap. &#125; else // Writing &#123; ByteSwap(V, Length); // Swap V. Serialize(V, Length); // Write V. ByteSwap(V, Length); // Swap V back to its original byte order to prevent caller from observing V swapped. &#125; return *this;&#125; 保存时的字节序 默认行为： UE 保存 .uasset 文件时通常 不做字节翻转（即保存为主机平台字节序，小端）。 例如 Windows 保存时使用小端格式，且不会设置 ForceByteSwapping 为 true。 可选配置： UE 支持强制以大端格式保存资源（用于跨平台），但一般只在构建某些平台的资源包时启用，比如： 12FArchive&amp; Ar;Ar.SetByteSwapping(true); // 强制切换字节序 加载时的字节序检测 文件头中的魔数（Magic Number） UE 使用 FPackageFileSummary::Tag 字段中的魔数来判断文件是否需要字节翻转。 12#define PACKAGE_FILE_TAG 0x9E2A83C1#define PACKAGE_FILE_TAG_SWAPPED 0xC1832A9E 当加载 .uasset 时，UE 首先读入前 4 字节作为 Tag，然后判断是否为正常魔数或反转魔数, 这个魔数存在于文件最前面，即 FPackageFileSummary 的开头部分。 十、总结 通过以上模块划分与驱动说明，可以看到 UE5 通过 FModuleManager 实现高度模块化架构，各模块在不同加载阶段注册初始化，并在引擎生命周期中由相应子系统调用，保证了功能隔离与灵活扩展。 通过上述分析，我们梳理了 UE5 在 Windows 平台下从操作系统原生消息到游戏逻辑回调的完整输入处理管道。","categories":[{"name":"UE5","slug":"UE5","permalink":"http://yoursite.com/categories/UE5/"}],"tags":[{"name":"UE5","slug":"UE5","permalink":"http://yoursite.com/tags/UE5/"}]},{"title":"在Hexo中使用Mathjax渲染数学公式","slug":"工具/Hexo/在Hexo中渲染MathJax数学公式","date":"2025-04-26T11:06:24.132Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2025/04/26/工具/Hexo/在Hexo中渲染MathJax数学公式/","link":"","permalink":"http://yoursite.com/2025/04/26/%E5%B7%A5%E5%85%B7/Hexo/%E5%9C%A8Hexo%E4%B8%AD%E6%B8%B2%E6%9F%93MathJax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/","excerpt":"前言 Mathjax是一种支持在所有浏览器上显示数学公式的Javascript引擎，它支持绝大部分的Tex/LaTex语法，并且有三种输出HTML/CSS, SVG(矢量图)和MathML（Firefox支持的数公式标记语言）。本文主要介绍如何在将Mathjax整合进入Hexo中，在整合之前先介绍一下Tex/LaTex。","text":"前言 Mathjax是一种支持在所有浏览器上显示数学公式的Javascript引擎，它支持绝大部分的Tex/LaTex语法，并且有三种输出HTML/CSS, SVG(矢量图)和MathML（Firefox支持的数公式标记语言）。本文主要介绍如何在将Mathjax整合进入Hexo中，在整合之前先介绍一下Tex/LaTex。 Tex/LaTex介绍 TeX是由著名的计算机科学家Donald E. Knuth（高德纳）发明的宏语言排版系统，由于Tex是一种程序式的排版系统，对于一般用户来说很难上手，此时LaTex就应运而生了。 LaTeX（LATEX，音译“拉泰赫”）是一种基于ΤΕΧ的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也可以充分发挥由TeX所提供的强大功能，能在几天，甚至几小时内生成很多具有书籍质量的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。这个系统同样适用于生成从简单的信件到完整书籍的所有其他种类的文档。由于本文的目的是为了在网页上显示数学公式，对于LaTeX的其他文档的排版不做介绍。先来看看LaTex实例，如下： 1234$a=\\begin&#123;bmatrix&#125;1&amp;2&amp;3\\end&#123;bmatrix&#125;$ %行向量$a=\\begin&#123;bmatrix&#125;1\\\\2\\\\3\\end&#123;bmatrix&#125;$ %列向量$a\\cdot b=\\sum_&#123;i=1&#125;^&#123;n&#125;a_ib_i$ %累加和 对应的渲染效果，如下： a=\\(\\begin{bmatrix}1&amp;2&amp;3\\end{bmatrix}\\) a=\\(\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\) \\(a\\cdot b=\\sum_{i=1}^{n}a_ib_i\\) Hexo配置Mathjax 在Hexo配置Mathjax只需要5个步骤： 步骤1：卸载默认的渲染引擎hexo-renderer-marked 1npm uninstall hexo-renderer-marked --save 步骤2：安装新的渲染引擎hexo-renderer-pandoc 1npm install hexo-renderer-pandoc --save 步骤3：安装新渲染引擎依赖的第三方工具pandoc 直接进入pandoc的官方网站下载，地址:https://pandoc.org/installing.html 每个系统的安装方式不一样，根据自己的系统进行安装。 步骤4：修改Hexo的主题配置 本人使用的是next主题，在./themes/next/_config.yml文件中修改，如下： 步骤5：在需要支持Mathjax文章的头部添加mathjax标记,如下图： LaTex常用数学表达式语法 直接参见：超详细 LaTex数学公式 参考文献 [1] 网页上显示数学公式目前哪种方案最好？ [2] LaTeX排版系统 [3] LaTeX快速入门：一文浅谈TeX排版语法 [4] Latex基础语法 [5] Latex官网 [6] 超详细 LaTex数学公式","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Hexo","slug":"工具/Hexo","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"Mathjax","slug":"Mathjax","permalink":"http://yoursite.com/tags/Mathjax/"}]},{"title":"国债基础知识","slug":"理财/国债基础知识","date":"2025-02-21T12:03:28.000Z","updated":"2025-05-27T00:45:09.239Z","comments":true,"path":"2025/02/21/理财/国债基础知识/","link":"","permalink":"http://yoursite.com/2025/02/21/%E7%90%86%E8%B4%A2/%E5%9B%BD%E5%80%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"什么是国债 国债（Government Bonds）是指国家发行的债券，由政府作为借款人向社会筹集资金，并承诺在未来某个时间偿还本金并支付利息。由于国债由国家信用担保，因此一般被认为是最安全的债券。 国债的基本特点 发行主体：政府 由财政部或中央银行代表政府发行。 在中国，国债由财政部发行，在银行间市场和交易所市场流通。 风险极低，接近“无风险” 由于政府有税收和货币政策支持，违约风险极低。 通常被视为无风险利率（Risk-Free Rate）的基准。 收益相对较低 安全性高，收益率低于企业债、高收益债等。 但对于长期稳健投资者来说，国债是重要资产。 用途：政府融资、调控市场 主要用于填补财政赤字、基础设施建设、调节市场流动性。","text":"什么是国债 国债（Government Bonds）是指国家发行的债券，由政府作为借款人向社会筹集资金，并承诺在未来某个时间偿还本金并支付利息。由于国债由国家信用担保，因此一般被认为是最安全的债券。 国债的基本特点 发行主体：政府 由财政部或中央银行代表政府发行。 在中国，国债由财政部发行，在银行间市场和交易所市场流通。 风险极低，接近“无风险” 由于政府有税收和货币政策支持，违约风险极低。 通常被视为无风险利率（Risk-Free Rate）的基准。 收益相对较低 安全性高，收益率低于企业债、高收益债等。 但对于长期稳健投资者来说，国债是重要资产。 用途：政府融资、调控市场 主要用于填补财政赤字、基础设施建设、调节市场流动性。 国债的分类 按期限划分 短期国债（1年以内）：如3个月、6个月、1年期国债。 中期国债（1-10年）：如3年、5年、7年期国债。 长期国债（10年以上）：如10年、20年、30年期国债。 按付息方式划分 贴现国债：无定期利息，到期一次性支付本金和利息（如3个月、6个月短期国债）。 附息国债：定期支付利息，到期还本（如10年期国债）。 浮动利率国债：利率随市场变化（较少见）。 按交易方式划分 记账式国债：电子化交易，可在二级市场买卖（银行间市场、交易所）。 储蓄国债：个人投资者购买，不能在市场流通，但可提前兑付。 凭证式国债：类似定期存款，不能交易，利率固定。 国债的影响 影响市场利率 国债收益率（如10年期国债利率）是市场利率的重要风向标，影响贷款利率、企业债收益率等。 国债价格下跌 → 国债收益率上升 → 市场利率上升（资金成本变贵）。 国债价格上涨 → 国债收益率下降 → 市场利率下降（资金成本变便宜）。 影响股市 国债收益率上升（市场利率上升） → 资金流向债市，股市承压。 国债收益率下降（市场利率下降） → 资金流向股市，股市受益。 影响央行货币政策 央行通过买卖国债调整市场流动性，如“公开市场操作”（OMO）。 央行买入国债（释放资金）→ 降低利率，刺激经济。 央行卖出国债（回收资金）→ 提高利率，抑制通胀。 国债的收益率和价格 国债的收益率（YTM，Yield to Maturity）和价格是反向关系的，即： 国债价格上升 → 国债收益率下降 国债价格下降 → 国债收益率上升 这种关系的核心逻辑是：债券收益率是市场利率的一种反映，而债券价格由市场供需决定。 国债收益率（YTM）是什么？ 国债收益率是指投资者在当前市场价格下持有债券到期所能获得的年化收益率。主要有以下几种： 票面收益率（Coupon Rate） 计算方式： \\[ 票面收益率 = \\frac {票面利息} {票面价值} \\] 例如，一张面值1000元、年息3%的国债，每年付息30元，其票面收益率是3%。 到期收益率（YTM, Yield to Maturity）（市场最常用） 计算方式：综合考虑债券当前市场价格、票面利率、剩余期限和本金偿还。 公式： \\[P=\\sum{\\frac{C}{(1+YTM)^t}} + \\frac{F}{(1+YTM)^T}\\] ​ 其中： P：当前债券市场价格 C：每年的票面利息（Coupon） F：债券到期时的面值（一般是100元或1000元） YTM：到期收益率 T：剩余期限 t: 第 t 年 YTM可以近似理解为：当前市场上买入该债券后，持有到期的年化收益率。 为什么国债收益率和价格反向变动？ 核心逻辑：市场利率变动 → 债券吸引力变化 → 价格调整 （1）市场利率上升 → 国债收益率上升 → 国债价格下跌 - 假设市场利率从3%上升到4%，新发行的国债利息更高，老国债（票面利率3%）就不划算了，导致价格下跌。 - 投资者不愿意花1000元买利率3%的国债，只有当它的市场价格低于1000元时，整体收益率才会提升到接近4%。 （2）市场利率下降 → 国债收益率下降 → 国债价格上涨 假设市场利率从3%下降到2%，老国债（票面利率3%）比新发债券（票面利率2%）更值钱，因此市场愿意溢价购买，导致债券价格上涨。 3. 计算案例 假设你持有一张面值1000元、年利息50元（5%）、剩余期限5年的国债： （1）市场利率上升到6%时： 投资者希望获得6%的回报率，因此他们愿意支付的债券价格P需要调整： ​ \\[P=\\frac{50}{(1.06)^1} + \\frac{50}{(1.06)^2} + \\frac{50}{(1.06)^3} + \\frac{50}{(1.06)^4} + \\frac{1050}{(1.06)^5} \\] 计算后，债券价格大约 跌到950元左右（低于面值1000元）。 （2）市场利率下降到4%时： 投资者对4%的回报率感兴趣，因此债券价格会上涨： \\[P=\\frac{50}{(1.04)^1} + \\frac{50}{(1.04)^2} + \\frac{50}{(1.04)^3} + \\frac{50}{(1.04)^4} + \\frac{1050}{(1.04)^5} \\] ​ 计算后，债券价格大约 涨到1050元左右（高于面值1000元）。 国债价格波动的原因 价格波动原因 债券价格的波动主要受到市场利率、通胀预期、政策调控、市场供需等因素的影响。 市场利率变动（最关键因素） 核心逻辑：债券价格与市场利率呈反向关系 当市场利率上升（如央行加息），新发行的债券收益率变高，旧债券的固定利息相对不划算，市场价格下跌。 当市场利率下降（如央行降息），旧债券的固定利息相对更有吸引力，市场价格上升。 举例： 你手里有一张年息3%的国债，市场利率突然涨到4%，大家更愿意买新的4%国债，你的3%国债就不值钱了，价格下降。 反之，如果市场利率降到2%，你的3%国债就更值钱了，价格上升。 通货膨胀预期 通胀上升 → 购买力下降 → 债券的固定利息变得不值钱 → 债券价格下跌。 通胀下降 → 购买力增强 → 债券的固定收益更具吸引力 → 债券价格上涨。 举例： 如果市场预期未来通胀率会上升（如CPI数据飙升），投资者会抛售长期债券，导致债券价格下跌。 货币政策与财政政策 央行政策会影响市场利率，从而影响债券价格 央行加息 → 市场利率上升 → 债券价格下跌（尤其是长期债）。 央行降息 → 市场利率下降 → 债券价格上涨。 央行缩表（减少市场流动性） → 资金紧张，债券抛售，价格下跌。 央行放水（增加市场流动性） → 资金宽松，债券价格上涨。 财政政策 政府加大债券发行量（如扩张性财政政策） → 市场上债券供给增加，价格可能下降。 政府减少债券供给 → 价格可能上涨。 信用风险（主要影响信用债，不影响国债） 对于国债而言，信用风险很低（政府不会轻易违约），但对于企业债、高收益债，信用评级下降可能导致债券价格下跌。 举例： 某公司债券原本信用评级AAA，突然爆雷被降级到BBB，市场担心违约，债券价格暴跌。 市场供需关系 债券供给增加（政府或企业大量发债） → 价格下降。 市场需求旺盛（避险情绪高涨，大量买入债券） → 价格上涨。 举例： 经济衰退时，股市下跌，避险资金流入债券市场，债券价格上涨。 经济过热时，资金流向股票、房地产，债券需求下降，价格下跌。 总结 因素 对债券价格的影响 市场利率上升 债券价格下跌 市场利率下降 债券价格上涨 通胀预期上升 债券价格下跌 通胀预期下降 债券价格上涨 央行加息/缩表 债券价格下跌 央行降息/放水 债券价格上涨 信用风险上升 债券价格下跌（对国债影响小） 市场避险情绪增强 债券价格上涨 债券供给增加 债券价格下跌 债券需求增加 债券价格上涨 什么是市场利率 市场利率是一个广义概念，指的是金融市场上资金借贷的利率水平。以下是几个常见的市场利率指标，它们对债券价格有不同程度的影响： 政策利率（央行基准利率） MLF（中期借贷便利，Medium-term Lending Facility） 由中国人民银行（央行）提供给商业银行的中期资金借贷工具，期限一般为6个月或1年。 MLF利率变动会影响LPR（贷款市场报价利率），进而影响市场利率和债券价格。 MLF利率上调 → 市场利率上升 → 债券价格下跌 MLF利率下调 → 市场利率下降 → 债券价格上涨 公开市场操作（OMO）利率 央行通过正/逆回购向市场投放或回收流动性。 OMO利率上调 → 资金变贵，市场利率上升，债券价格下跌。 存款准备金率（RRR） 影响商业银行可贷资金量，间接影响市场利率。 LPR（贷款市场报价利率） 由MLF决定，影响企业和个人贷款利率，进而影响市场利率。 银行间市场利率 DR007（银行间7天回购利率） 代表银行间市场7天期质押式回购利率，反映短期资金价格。 DR007上升 → 资金紧张，市场利率上升，债券价格下跌。 SHIBOR（上海银行间同业拆借利率） 银行间短期借款的利率，类似于国际上的LIBOR。 债券市场收益率（YTM，Yield to Maturity） 10年期国债收益率 是市场利率的重要参考指标，影响长期债券价格。 10年期国债收益率上升 → 长期债券价格下跌。 10年期国债收益率下降 → 长期债券价格上涨。 短期国债收益率（1年、3年） 主要影响短期债券价格。 美债收益率（对国内市场的影响） 美联储加息 → 美债收益率上升 → 资金流出新兴市场 → 中国市场利率上升 → 债券价格下跌。 美联储降息 → 美债收益率下降 → 资金流入中国市场 → 债券价格上涨。 通胀时期为何减持长债、买入短债？ 久期（Duration）及其作用 什么是久期（Duration）？ 久期是衡量债券价格对利率变化敏感程度的指标，表示债券价格对市场利率变动的弹性。 简单来说： 久期越长，债券价格对利率变化越敏感，价格波动更大。 久期越短，债券价格对利率变化不太敏感，价格波动更小。 影响久期的因素 （1）债券期限（Maturity） 期限长 → 久期大（对利率敏感）。 期限短 → 久期小（对利率不敏感）。 （2）票面利率（Coupon Rate） 票面利率高 → 久期小（因投资者能更快回本）。 票面利率低 → 久期大（因主要现金流集中在远期）。 （3）市场利率（YTM） 市场利率上升 → 久期下降（贴现现金流的影响）。 市场利率下降 → 久期上升（未来现金流折现值更高）。 为什么通胀时期减持长债、买入短债 利率上升预期 通胀通常促使央行加息以抑制物价上涨。利率上升导致债券价格下跌，尤其长期债券因久期长而跌幅更大。例如： 长期债券：若10年期国债收益率从3%升至4%，久期8年，价格下跌约8%（8年×1%）。 短期债券：1年期国债收益率同样上升1%，久期接近1年，价格仅下跌约1%。 再投资风险 短期债券到期快，投资者可迅速将本金再投资于更高利率的新债券，锁定更高收益。而长期债券锁定低利率的时间更长，机会成本更高。 实际收益率下降 通胀侵蚀固定收益债券的实际购买力。例如，票面利率3%的债券，若通胀达5%，实际收益为-2%。短期债券因到期快，能更快调整至新利率环境，减少实际亏损。","categories":[{"name":"理财","slug":"理财","permalink":"http://yoursite.com/categories/%E7%90%86%E8%B4%A2/"}],"tags":[{"name":"理财","slug":"理财","permalink":"http://yoursite.com/tags/%E7%90%86%E8%B4%A2/"}]},{"title":"HUD实现","slug":"Unity/UI/HUD实现","date":"2025-01-17T08:59:14.000Z","updated":"2025-05-27T00:45:09.239Z","comments":true,"path":"2025/01/17/Unity/UI/HUD实现/","link":"","permalink":"http://yoursite.com/2025/01/17/Unity/UI/HUD%E5%AE%9E%E7%8E%B0/","excerpt":"设计需求 解决大量(1000个)HUD创建，更新的性能问题 创建1000个的开销控制在3ms 更新1000个的开销控制在2ms 有多种HUD的排序策略 通过x,y,z排序 通过指定顺序 通过函数排序 支持动静分离 支持三种控件，Image, Text和Slider 需要支持动态图集（可选） 文字支持描边和投影（可选） 支持布局器（可选）","text":"设计需求 解决大量(1000个)HUD创建，更新的性能问题 创建1000个的开销控制在3ms 更新1000个的开销控制在2ms 有多种HUD的排序策略 通过x,y,z排序 通过指定顺序 通过函数排序 支持动静分离 支持三种控件，Image, Text和Slider 需要支持动态图集（可选） 文字支持描边和投影（可选） 支持布局器（可选） UGUI分析 UGUI的核心类主要包括: Canvas相关 Canvas: UI渲染的根节点,负责管理UI的渲染顺序、渲染模式等 CanvasRenderer: CanvasRenderer的具体作用是将UI元素转换为可渲染的网格数据，并将其提交到渲染管线中。Unity底层会对这些Quad进行排序和合并，以提高渲染效率，而不是每个UI元素单独渲染。 CanvasUpdateRegistry: 管理UI元素的重建和更新 CanvasScaler: 用于控制UI的缩放 基础组件 Graphic: 所有可视UI元素的基类,提供基础的渲染功能 MaskableGraphic: 支持遮罩功能的Graphic基类 Image: 图片渲染组件 RawImage: 原始图片渲染组件 Text: 文本渲染组件 交互组件 Selectable: 所有可交互UI元素的基类 Button: 按钮组件 Toggle: 开关组件 Slider: 滑动条组件 Scrollbar: 滚动条组件 ScrollRect: 滚动视图组件 布局系统 LayoutGroup: 布局组基类 HorizontalLayoutGroup: 水平布局组件 VerticalLayoutGroup: 垂直布局组件 GridLayoutGroup: 网格布局组件 LayoutElement: 布局元素属性定义 核心工作流程 Canvas更新流程: 注册需要重建的组件到CanvasUpdateRegistry 执行PerformUpdate处理重建请求 调用Layout、Graphic重建流程 UI元素渲染流程: Graphic组件标记重建 CanvasUpdateRegistry收集重建请求 执行重建生成Mesh数据 CanvasRenderer将Mesh提交到渲染管线 布局计算流程: 自顶向下遍历布局树收集LayoutElement信息 计算每个元素所需尺寸 自底向上分配空间确定最终布局 具体流程可以参考UGUI源码 HUD设计与实现","categories":[],"tags":[]},{"title":"理财基础知识","slug":"理财/理财基础知识","date":"2024-11-07T15:03:28.000Z","updated":"2025-05-27T00:45:09.239Z","comments":true,"path":"2024/11/07/理财/理财基础知识/","link":"","permalink":"http://yoursite.com/2024/11/07/%E7%90%86%E8%B4%A2/%E7%90%86%E8%B4%A2%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","excerpt":"本文将通《世界金融史》了解金融的发展，金融是如何驱动世界运转，以及相关的理财产品。 世界金融简史 世界金融的发展经历了多个重要阶段，每个阶段都标志着金融体系的重大变革和进步： 古代金融的萌芽（公元前3000年 - 公元5世纪） 标志：货币的出现和早期借贷行为。 发展：最早的金融活动可以追溯到古代美索不达米亚和埃及，当时人们使用谷物、牲畜等作为交换媒介。随着金属货币的出现（如中国的铜钱、希腊的银币），金融活动逐渐规范化。古代罗马和希腊还出现了早期的银行和借贷行为。 中世纪金融的兴起（5世纪 - 15世纪） 标志：汇票和早期银行的诞生。 发展：中世纪欧洲的贸易繁荣催生了金融工具的创新，如汇票（Bill of Exchange）的出现，解决了长途贸易中的支付问题。意大利的佛罗伦萨、威尼斯等城市成为金融中心，诞生了最早的银行（如美第奇银行）。 近代金融体系的形成（16世纪 - 18世纪） 标志：股票市场和中央银行的建立。 发展：随着大航海时代的到来，全球贸易扩张，金融需求激增。1602年，荷兰东印度公司成立了世界上第一家股票交易所——阿姆斯特丹证券交易所。1694年，英格兰银行成立，标志着现代中央银行的诞生。 工业革命与金融全球化（19世纪 - 20世纪初） 标志：工业资本与金融资本的结合。 发展：工业革命推动了生产力的飞跃，金融资本成为工业化的重要支撑。伦敦成为全球金融中心，金本位制被广泛采用，国际资本流动加速。同时，投资银行和保险公司等新型金融机构兴起。","text":"本文将通《世界金融史》了解金融的发展，金融是如何驱动世界运转，以及相关的理财产品。 世界金融简史 世界金融的发展经历了多个重要阶段，每个阶段都标志着金融体系的重大变革和进步： 古代金融的萌芽（公元前3000年 - 公元5世纪） 标志：货币的出现和早期借贷行为。 发展：最早的金融活动可以追溯到古代美索不达米亚和埃及，当时人们使用谷物、牲畜等作为交换媒介。随着金属货币的出现（如中国的铜钱、希腊的银币），金融活动逐渐规范化。古代罗马和希腊还出现了早期的银行和借贷行为。 中世纪金融的兴起（5世纪 - 15世纪） 标志：汇票和早期银行的诞生。 发展：中世纪欧洲的贸易繁荣催生了金融工具的创新，如汇票（Bill of Exchange）的出现，解决了长途贸易中的支付问题。意大利的佛罗伦萨、威尼斯等城市成为金融中心，诞生了最早的银行（如美第奇银行）。 近代金融体系的形成（16世纪 - 18世纪） 标志：股票市场和中央银行的建立。 发展：随着大航海时代的到来，全球贸易扩张，金融需求激增。1602年，荷兰东印度公司成立了世界上第一家股票交易所——阿姆斯特丹证券交易所。1694年，英格兰银行成立，标志着现代中央银行的诞生。 工业革命与金融全球化（19世纪 - 20世纪初） 标志：工业资本与金融资本的结合。 发展：工业革命推动了生产力的飞跃，金融资本成为工业化的重要支撑。伦敦成为全球金融中心，金本位制被广泛采用，国际资本流动加速。同时，投资银行和保险公司等新型金融机构兴起。 现代金融体系的建立（20世纪） 标志：布雷顿森林体系与金融创新。 发展：二战后，布雷顿森林体系确立了美元与黄金挂钩的国际货币体系。20世纪70年代，布雷顿森林体系解体，浮动汇率制成为主流。金融创新层出不穷，如衍生品市场、共同基金和对冲基金的兴起。 当代金融的数字化与全球化（21世纪至今） 标志：金融科技与数字货币的崛起。 发展：互联网和信息技术的发展催生了金融科技（FinTech），移动支付、区块链技术和加密货币（如比特币）改变了传统金融格局。全球化进一步深化，金融市场高度互联，但也带来了新的风险和挑战。 每个阶段的金融发展都反映了当时社会经济需求的变化，推动了人类文明的进步。 社会是如何稳定运转的 从社会科学的角度看，社会是一个多层次、复杂而动态的系统，其基本构成、运转机制和金融的作用可以从以下几个方面进行详细说明： 社会的构成 个体与群体 个体：社会的最基本单位是个体，每个人都有自己的需求、行为和价值观。个体之间的互动构成了社会的基本活力。 群体与家庭：个体以家庭为起点，逐渐扩展到社区、学校、工作单位等群体，这些群体之间既存在内部的协同，也形成了社会互动的网络。 制度与组织 政治、经济、法律与文化制度：社会由一系列正式与非正式制度构成。政治制度、法律体系、经济体制、教育和文化规范等，为社会成员的行为设定了规则和约束。 各类组织：包括政府机构、企业、非政府组织、学术团体等，这些组织通过分工合作共同实现社会整体目标。 文化与价值观 共享的信念与规范：文化和价值观为社会提供共同的精神纽带和行为指南。例如，诚信、合作、公共责任等观念有助于构建社会信任和稳定性。 非正式规范：除正式法律外，习俗、道德、社会契约等非正式规范也在调节人们行为、缓解冲突中起着关键作用。 社会的运转机制 制度保障与法治 法律与规章：法律、规章和政策为社会设定了明确的行为界限和权利义务，确保所有成员在公平正义的框架内活动。 顶层设计：从国家层面，科学的顶层设计通过宏观规划、战略部署和制度建设（例如自1978年以来我国金融改革开放的顶层设计）为整个社会提供长期稳定的运行支撑​。 政府与市场的协调 市场机制：市场通过价格信号、供需平衡和竞争机制高效配置资源，激发经济活力。 政府调控：政府则通过宏观调控、公共服务和社会保障，弥补市场失灵，维护社会公平和稳定。两者相辅相成，共同推动社会的持续进步。 社会信任与文化认同 信任机制：社会运行还依赖于人们之间的信任与契约精神。稳定的信用体系和文化认同减少了交易成本，提高了社会合作效率。 伦理与道德规范：共享的道德标准和文化价值观使得社会成员在面对冲突时能够达成共识，共同应对不确定性和风险。 信息与技术的作用 信息流通：现代社会的信息传播和交流技术确保各级组织与个体能及时获取信息，做出合理决策。 科技进步：技术创新不仅推动了经济发展，同时也优化了社会管理和公共服务，提高了整体运转效率。 金融在社会中的作用 资源配置与经济效率 资本流动：金融体系作为连接储蓄与投资的纽带，通过银行、资本市场、保险等渠道实现资金的有效配置，使得社会资源能够流向最需要的地方，推动经济增长。 提高效率：通过金融工具和市场机制，降低了交易成本，增强了资源配置的灵活性和效率。 风险分散与管理 风险转移：金融市场提供多种工具（如保险、衍生品等）帮助社会成员分散和转移风险，从而降低突发事件对整体经济的冲击。 风险管理机制：良好的金融监管和风险控制体系为防范系统性风险提供保障，维护了经济和社会的整体稳定。 推动创新与经济转型 支持企业创新：金融体系为新兴企业和技术创新提供必要的融资支持，推动产业升级和技术进步。 促进结构调整：通过绿色金融、普惠金融等模式，金融不仅支持传统产业转型升级，还推动社会向低碳、可持续发展转型。 社会公平与可持续发展 普惠金融：金融服务普惠化使更多人群，尤其是中小企业和低收入群体能够获得信贷和其他金融支持，促进财富再分配。 ESG理念：随着环境、社会和治理（ESG）理念的普及，金融投资也开始注重社会责任和可持续性发展，这有助于实现经济效益与社会效益的双重提升​。 金融文化与社会信任 信用体系建设：金融不仅是技术和工具，更是一种文化现象。信用、信任和契约精神构成了金融文化的核心，这种文化对于稳定金融市场、降低交易风险、提升社会整体信任度具有重要意义。 从社会科学的角度看，社会是由个体、群体、制度和文化等多重因素构成的复杂系统，其稳定运转依赖于法律与制度保障、政府与市场的有效协调、以及共享的信任和文化认同。而金融作为社会经济体系的重要组成部分，不仅通过有效配置资源、分散风险和激励创新推动经济发展，还通过普惠金融和ESG理念促进社会公平与可持续发展。总体而言，金融在维护社会稳定、推动经济转型和构建长远社会信任方面扮演着至关重要的角色。 常用的理财产品 常见的理财产品分类与说明 理财产品种类繁多，根据风险等级、投资期限和底层资产的不同，可以分为以下几类： 1. 银行存款类 原理：将资金存入银行，获得固定利息收益。 风险等级：低风险。 底层资产：银行信用。 常见产品： 活期存款：随时存取，利率较低。 定期存款：固定期限，利率较高。 大额存单：起存金额较高，利率高于普通定期存款。 2. 货币基金 原理：投资于短期货币市场工具（如国债、银行存单、商业票据等），流动性强。 风险等级：低风险。 底层资产：短期债券、银行存款等。 常见产品：余额宝、零钱通等。 3. 债券类产品 原理：投资于政府、企业发行的债券，获得固定利息收益。 风险等级：中低风险。 底层资产：国债、企业债、地方债等。 常见产品： 国债：由国家发行，风险极低。 企业债：由企业发行，风险略高于国债。 可转债：兼具债券和股票特性，风险中等。 4. 股票类产品 原理：投资于上市公司股票，通过股价上涨和分红获利。 风险等级：中高风险。 底层资产：上市公司股权。 常见产品： 股票：直接投资于个股。 股票型基金：由基金经理管理，分散投资于多只股票。 指数基金：跟踪特定股票指数（如沪深300、标普500）。 5. 基金类产品 原理：由专业基金经理管理，投资于多种资产（股票、债券、货币等）。 风险等级：根据投资标的不同，风险从低到高不等。 底层资产：股票、债券、货币市场工具等。 常见产品： 货币基金：低风险，投资于短期货币工具。 债券基金：中低风险，投资于债券。 混合基金：中风险，同时投资于股票和债券。 股票基金：中高风险，主要投资于股票。 6. 保险类产品 原理：通过缴纳保费，获得保障或投资收益。 风险等级：低至中风险。 底层资产：保险公司的投资组合（如债券、股票、房地产等）。 常见产品： 分红险：提供保障的同时，分享保险公司盈利。 万能险：兼具保障和投资功能，收益与市场挂钩。 投连险：投资于股票、基金等，收益波动较大。 7. 信托类产品 原理：将资金委托给信托公司，由信托公司进行投资管理。 风险等级：中高风险。 底层资产：房地产、基础设施、股权等。 常见产品： 房地产信托：投资于房地产项目。 基础设施信托：投资于基础设施建设。 股权信托：投资于企业股权。 8. 私募基金 原理：面向高净值人群，投资于非公开市场（如未上市企业股权、房地产等）。 风险等级：高风险。 底层资产：未上市企业股权、房地产、艺术品等。 常见产品： 私募股权基金：投资于未上市企业。 私募证券基金：投资于股票、债券等公开市场产品。 房地产私募基金：投资于房地产项目。 9. 期货与期权 原理：通过买卖标准化合约，进行套期保值或投机。 风险等级：高风险。 底层资产：商品（如原油、黄金）、金融资产（如股票指数、外汇）。 常见产品： 商品期货：如原油期货、黄金期货。 金融期货：如股指期货、外汇期货。 期权：如股票期权、商品期权。 10. 贵金属与大宗商品 原理：投资于黄金、白银、原油等实物或衍生品。 风险等级：中高风险。 底层资产：贵金属、大宗商品。 常见产品： 黄金ETF：跟踪黄金价格的交易所交易基金。 原油期货：投资于原油价格波动。 11. 房地产投资 原理：通过购买房产或房地产基金，获得租金收益或资产增值。 风险等级：中高风险。 底层资产：房地产。 常见产品： 住宅房产：用于自住或出租。 商业地产：如写字楼、商铺。 房地产信托基金（REITs）：投资于房地产项目的基金。 12. 数字货币 原理：投资于比特币、以太坊等加密货币，通过价格波动获利。 风险等级：极高风险。 底层资产：区块链技术支持的加密货币。 常见产品：比特币、以太坊、莱特币等。 13. 结构性理财产品 原理：将固定收益产品与衍生品结合，收益与特定标的（如股票、指数）挂钩。 风险等级：中高风险。 底层资产：债券、衍生品（如期权）。 常见产品：银行发行的结构性存款、结构性理财。 风险等级与投资建议 低风险产品：适合保守型投资者，如银行存款、货币基金、国债。 中低风险产品：适合稳健型投资者，如债券基金、分红险。 中高风险产品：适合平衡型投资者，如股票基金、混合基金、信托。 高风险产品：适合激进型投资者，如私募基金、期货、数字货币。 投资底层资产总结 固定收益类：银行存款、债券、货币基金。 权益类：股票、股票基金、私募股权。 商品类：黄金、原油、大宗商品。 另类投资：房地产、艺术品、数字货币。 参考 《世界金融史》","categories":[{"name":"理财","slug":"理财","permalink":"http://yoursite.com/categories/%E7%90%86%E8%B4%A2/"}],"tags":[{"name":"理财","slug":"理财","permalink":"http://yoursite.com/tags/%E7%90%86%E8%B4%A2/"}]},{"title":"性能优化-CPU","slug":"Unity/Optimization/性能优化-CPU","date":"2024-10-26T04:09:28.000Z","updated":"2025-07-21T14:04:53.737Z","comments":true,"path":"2024/10/26/Unity/Optimization/性能优化-CPU/","link":"","permalink":"http://yoursite.com/2024/10/26/Unity/Optimization/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-CPU/","excerpt":"闪存优化 闪存用于存储游戏数据和资源，包括游戏数据、纹理、音频等。闪存的读写速度比内存慢得多，因此在游戏中，我们需要尽可能地减少闪存的读写次数，以提高游戏的性能。 闪存结构和文件操作流程 在性能优化-基础 中，介绍了相关的基础知识，为了文章的完整性，简单回顾一下。","text":"闪存优化 闪存用于存储游戏数据和资源，包括游戏数据、纹理、音频等。闪存的读写速度比内存慢得多，因此在游戏中，我们需要尽可能地减少闪存的读写次数，以提高游戏的性能。 闪存结构和文件操作流程 在性能优化-基础 中，介绍了相关的基础知识，为了文章的完整性，简单回顾一下。 闪存结构 SOC系统中的闪存一般采用NAND Flash或NOR Flash，作为非易失性存储器，用于存储操作系统、应用程序、资源文件等。 闪存通过总线（如SPI、eMMC、UFS等）与SOC主控芯片连接。 文件读写流程 文件读取： CPU发起文件读取请求，操作系统通过文件系统（如FAT、EXT4等）定位文件在闪存中的物理地址。 文件系统驱动将读取命令通过总线发送到闪存控制器。 闪存控制器根据地址从闪存芯片中读取数据，经过总线传输到SOC的内存（RAM）中。 CPU从内存中获取数据进行处理。 文件写入： CPU将需要写入的数据放入内存缓冲区。 操作系统通过文件系统分配闪存空间，并生成写入命令。 写入命令和数据通过总线传递给闪存控制器。 闪存控制器将数据写入指定的闪存地址。 写入完成后，文件系统更新元数据，保证数据一致性。 应用层API： fopen, fread, fwrite, fclose等函数封装了文件操作的底层细节，但最终都是调用到操作系统API。 也可以使用内存映射的方式，将文件映射到内存中，直接操作内存，避免了文件直接调用API读取的过程, 映射后可以直接操作指针的方式读取和写入内存。 内存映射（Memory Mapping）机制及其优势 内存映射是指将磁盘文件直接映射到虚拟内存空间，从而支持按需访问，避免整体加载： 传统 I/O 模式：调用 Read() 一次性将整个文件加载至内存；内存占用高。 映射模式：调用 OS 提供的 mmap() 或 MapViewOfFile()，按页访问数据；只有实际访问的数据才加载。 优点包括： 降低内存消耗，提升启动速度 支持文件级懒加载和异步加载 适合大型资源（如场景/AssetBundle）流式访问 Unity 的 AssetBundle 加载系统内部即使用此策略，在移动平台和主机平台中应用广泛。 序列化（Serialization） 序列化的概念 Unity 的序列化（Serialization）体系分为编辑器写盘阶段和运行时读盘阶段，它同时支持 二进制格式（Binary SerializedFile）和 文本（YAML）格式。编辑器在构建场景、AssetBundle 或玩家（Player）时，将所有 UnityEngine.Object 派生的对象及其字段按照 “Type Tree + 对象数据” 的方式写入磁盘；运行时则根据磁盘上的 Type Tree 快速定位并重构内存中的 C++ 对象，并通过隐藏指针 m_CachedPtr 将之挂载到对应的 C# 托管对象上。二进制格式在读写时采用专门的 C++ 引擎代码和内存拷贝技术，支持内存映射 (.resS/.resource) 与多线程解压，是极高效的；而文本（YAML）格式则使用文本解析器和反射，仅在编辑器中针对小规模场景或开启 “Force Text” 时使用 序列化数据结构 Unity 采用自定义的 TypeTree 类型树机制进行资源的序列化与反序列化。其核心结构如下： 1. SerializedFileHeader 用于描述 .assets 文件的头部信息。 12345678class SerializedFileHeader &#123; uint metadataSize; uint fileSize; uint version; uint dataOffset; byte endianness; byte[] reserved;&#125; metadataSize：元数据（TypeTree、ObjectInfo等）大小。 fileSize：整个资源文件的总大小（包括元数据和二进制数据）。 version：Unity版本序列化的版本，不是引擎版本（如 0x0D 表示某个格式版本）。 dataOffset：实际资源（如贴图、音频等二进制数据）在文件中的偏移位置。 endianness：字节序标记（0 = 小端，1 = 大端）。 reserved：保留字段（通常为16字节，填0）。 2. SerializedType 描述一个类型信息，包括该类的ID、TypeTree结构等。 12345678class SerializedType &#123; int classID; bool isStrippedType; short scriptTypeIndex; TypeTree typeTree; string scriptIDHash; string typeHash;&#125; classID：Unity内部定义的类ID（如 1=GameObject, 28=Texture2D）。 isStrippedType：是否为裁剪类型（strip=True 表示此类型已裁剪，仅保留元信息）。 scriptTypeIndex：脚本索引（用于 MonoBehavior / ScriptableObject 指向具体脚本）。 typeTree：该类型的字段结构树。 scriptIDHash：用于 MonoScript 类型的 GUID 哈希值。 typeHash：用于校验类型树是否匹配。 3. TypeTree 表示一个复杂类型的字段结构信息。 1234class TypeTree &#123; List&lt;TypeTreeNode&gt; nodes; string className;&#125; nodes：字段树结构的节点数组（深度优先遍历展开）。 className：类型的名称（如 MonoBehaviour, GameObject）。 4. TypeTreeNode 字段节点的定义，描述一个字段的名称、类型及其在内存中的表现。 12345678910class TypeTreeNode &#123; string type; string name; int byteSize; int index; int metaFlag; int version; int depth; bool isArray;&#125; type：字段的类型名称（如 int, float, Vector3, string）。 name：字段名称。 byteSize：该字段在内存中的大小。 index：字段在TypeTree中的顺序索引。 metaFlag：元标志位（控制序列化行为，例如是否为Align16等）。 version：该字段引入的版本（可用于版本判断）。 depth：字段在嵌套结构中的深度（用于还原结构树）。 isArray：是否是数组类型字段。 5. ObjectInfo 表示 .assets 文件中的某个对象的基本信息。 12345678class ObjectInfo &#123; long byteStart; int byteSize; int typeID; ushort classID; short scriptTypeIndex; ulong pathID;&#125; byteStart：该对象数据在文件中的偏移。 byteSize：数据块的大小。 typeID：序列化类型在类型表中的索引。 classID：类型的ClassID。 scriptTypeIndex：用于脚本类的索引。 pathID：唯一标识该对象的ID（可用于引用）。 6. LocalSerializedObjectIdentifier 描述某个对象在当前 .assets 文件中的引用信息。 123class LocalSerializedObjectIdentifier &#123; long localFileID;&#125; localFileID：引用对象的 pathID。 7. FileIdentifier 用于跨文件引用对象时的文件信息。 12345class FileIdentifier &#123; string assetPath; GUID assetGUID; long localFileID;&#125; assetPath：引用文件的路径（在Editor中可显示）。 assetGUID：引用资源的GUID。 localFileID：在目标文件中的对象标识符（pathID）。 这些结构一起组成了 Unity 序列化格式的核心结构。 8. SerializedFile文件的结构 下图展示了SerializedFile文件的结构示意图。该结构遵循 Unity 的序列化文件格式，一般用于 .assets、.sharedAssets、.bundle 中的资源文件： 123456789101112131415161718192021222324252627282930313233+-------------------------+| SerializedFileHeader |+-------------------------+| Metadata Section || || +---------------------+ || | SerializedType[] | || | - classID | || | - typeTree | || | - scriptTypeIndex | || +---------------------+ || || +---------------------+ || | ObjectInfo[] | || | - pathID | || | - byteStart | || | - byteSize | || +---------------------+ || || +---------------------+ || | ScriptTypes (opt) | || +---------------------+ || || +---------------------+ || | ExternalReferences | || | FileIdentifier[] | || | LocalSerialized... | || +---------------------+ |+-------------------------+| Data Section (binary) || Raw serialized objects || aligned &amp; typed |+-------------------------+ SerializedFileHeader：标记整个文件的版本、大小、数据偏移和字节序。 Metadata Section：所有类型信息、对象信息、引用信息等元数据。 SerializedType[]：包含该文件中所有用到的类型的结构（包括TypeTree）。 ObjectInfo[]：文件中所有对象的地址、大小、类型等信息。 ScriptTypes：如果有MonoBehavior等脚本资源，会引用脚本GUID等。 FileIdentifier[]：跨文件引用（external object）用的外部资源列表。 Data Section：资源本体，存放贴图、Mesh、声音等对象的实际数据。 可根据 ObjectInfo.byteStart 和 ObjectInfo.byteSize 从 Data Section 中读取具体对象内容，并通过 TypeTree 解析其结构。 编辑器 YAML 模式与打包二进制格式的转换关系 编辑器开发阶段：可启用 Force Text 模式，使资源以 YAML 格式存储，便于版本控制与协作审阅。 打包阶段（BuildPlayer / AssetBundle）：所有 YAML 文本会被自动转化为二进制序列化形式，以提高加载性能和压缩比。 最终发布版本中的所有资源都以 Unity 自定义的二进制格式存储，并不再保留 YAML 表达。 AssetBundle文件结构 AssetBundle 是一个各种资源序列化后的集合，包括脚本、纹理、模型、音频等资源。 .unity3d 或 .bundle 等资源打包文件时，会使用 BundleFile 的结构。它遵循 Unity 的 UnityFS 或 UnityRaw 等打包格式。以下是其中关键结构体 Header、StorageBlock 和 Node 的字段详细说明及其作用。 1. Header Bundle 文件的开头部分，描述整个文件的基本结构和偏移信息。 12345678910class BundleFileHeader &#123; string signature; // 格式标识，如 \"UnityFS\" 或 \"UnityRaw\" uint version; // 文件格式版本号（如 6, 7, 8） string unityVersion; // 构建该Bundle的Unity版本（如 2020.3.0f1） string unityRevision; // 精确版本号和修订信息 ulong size; // 整个bundle文件的大小 uint compressedBlocksInfoSize; // BlocksInfo的压缩大小 uint uncompressedBlocksInfoSize; // BlocksInfo的解压后大小 uint flags; // 标志位，控制是否压缩、是否嵌入BlocksInfo等&#125; 字段说明： signature：表示打包格式，常见为 UnityFS，决定了后续字段的解析方式。 version：格式版本，不同版本字段解释有细微差异。 unityVersion / unityRevision：提供构建信息，用于兼容性判断。 size：整个 bundle 文件长度，用于验证文件完整性。 compressedBlocksInfoSize / uncompressedBlocksInfoSize：紧随其后的 BlocksInfo 数据块大小（压缩前后）。 flags： 位 0：BlocksInfo 是否嵌入到 header 后部（否则在文件尾部）。 位 1：BlocksInfo 是否经过压缩（如 LZ4, LZMA）。 2. StorageBlock 描述 bundle 文件中每一个数据块的压缩与存储方式。 12345class StorageBlock &#123; uint uncompressedSize; // 原始大小 uint compressedSize; // 压缩后大小 ushort flags; // 压缩方式标记（如 None, LZMA, LZ4）&#125; 字段说明： uncompressedSize：该数据块在解压后的字节数。 compressedSize：实际在 bundle 文件中存储的压缩数据大小。 flags：标志当前数据块使用的压缩算法： 0x00：无压缩（None） 0x01：LZMA 0x02：LZ4 0x03：LZ4HC 这些数据块会依序排列，拼接出完整的资源数据流。 3. Node 描述 bundle 中的虚拟文件结构（例如一个 .assets 文件或其他资源）。 123456class Node &#123; long offset; // 在数据块拼接后的偏移（解压后流中的偏移） long size; // 文件大小 uint flags; // 文件标志（通常未使用） string path; // 资源名称或虚拟路径&#125; 字段说明： offset：该虚拟文件在整个资源流（解压后的数据拼接体）中的偏移位置。 size：该文件的长度。 flags：通常恒为0，可忽略。 path：在 bundle 中定义的资源文件名称。例如：CAB-xxxxxx，或 sharedassets0.assets。 4. Bundle文件结构图 123456789101112131415161718192021222324252627282930313233+------------------------------+| BundleFileHeader || - signature || - version || - unityVersion || - unityRevision || - size || - compressedBlocksInfoSize|| - uncompressedBlocksInfoSize|| - flags |+------------------------------+ ↓+------------------------------+| Compressed BlocksInfo | ← 可选：有些版本在文件尾部+------------------------------+ ↓ 解压后+------------------------------+| StorageBlock[] | ← 描述接下来的压缩数据块结构+------------------------------+| Node[] | ← 描述解压数据中的虚拟文件信息+------------------------------+ ↓+------------------------------+| Compressed Data Blocks | ← 实际资源数据（按 StorageBlock 分段）+------------------------------+ ↓ 解压拼接后+------------------------------+| [Node 1 Content] |+------------------------------+| [Node 2 Content] |+------------------------------+| ... |+------------------------------+ Addressables Unity 的 Addressables 系统是 Unity 提供的一套高级资源管理与异步加载框架，它基于 AssetBundle 构建，但提供了更灵活的资源打包、定位与生命周期管理机制。以下从组件结构、实现机制和与传统 AssetBundle 的差异三个方面进行详细说明。 Addressables 的主要组件 Addressable Asset Settings (地址配置) 位于 Assets/AddressableAssetsData。 包含全局配置（如构建平台、资源定位方式、Profiles 设置等）。 AddressableAssetGroup Addressable 的资源分组单位。 每组可以设置独立的打包策略、加载路径、构建方式。 常见的 Group 类型： Static Content：固定内容，随包一同发布。 Remote Content：远程 CDN 加载资源。 AddressableAssetEntry 每个被标记为 addressable 的资源，对应一个 Entry。 包含资源路径、地址名、Label 等属性。 Profile Settings 支持设置多个环境（开发、测试、发布）下的变量。 如：RemoteLoadPath = http://cdn.mycompany.com/[BuildTarget] Content Catalog 构建时生成的 JSON 文件，描述所有资源的位置与依赖信息。 加载 Addressable 时，先加载 Catalog。 ResourceLocator Catalog 解析后生成的结构，负责将地址映射为资源路径。 Addressables.LoadAssetAsync() 内部依赖它完成地址到路径的解析。 ResourceManager Addressables 底层的加载调度中心。 管理加载任务、依赖树、引用计数、缓存。 IResourceProvider 抽象加载提供器接口。 可自定义如从网络、本地磁盘、WebGL 缓存中加载资源。 Addressables 与 AssetBundle 的区别 资源定位机制 AssetBundle：通过资源路径或 Bundle 名称定位资源，依赖关系由开发者显式管理。 Addressables：通过地址系统与 Catalog 文件进行统一索引，通过 IResourceLocator + 哈希索引管理资源映射，解耦了资源名与加载方式。 加载调度系统 AssetBundle：需手动管理 AssetBundle 加载、依赖、释放、缓存。 Addressables：封装在 ResourceManager 与 AsyncOperationHandle 中，自动管理引用计数、自动卸载、自动释放依赖。 Catalog + RuntimeData Addressables 构建生成的 catalog_xxx.json 是核心索引文件，记录资源地址、哈希、依赖、提供器类型。 RuntimeData 是构建期生成的内部数据结构，包括本地清单、远程路径映射、默认Provider绑定信息。 提供器架构（Provider） Addressables 使用 IResourceProvider 接口（如：BundledAssetProvider、TextDataProvider、AtlasSpriteProvider 等）支持加载多种资源类型。 每种资源类型可配置独立 Provider，也可扩展自定义 Provider 支持版本控制、加密等。 构建流程 Addressables 构建流程为：分析分组 -&gt; 计算依赖 -&gt; 构建 AssetBundle -&gt; 生成 Catalog 和链接关系 -&gt; 可选生成 BuildLayout 文件（用于调试和分析） 相比之下，AssetBundle 仅构建资源和依赖 Bundle，没有结构化 Catalog 文件。 远程与热更新机制 Addressables 提供 CheckForCatalogUpdates、DownloadDependenciesAsync 等接口直接进行差异比对与远程下载。 核心机制是基于 Catalog 哈希比对与本地缓存标记，而传统 AssetBundle 需开发者手动构建下载/更新逻辑。 常用 API 1234567891011121314151617181920212223242526272829// 通过地址加载资源Addressables.LoadAssetAsync&lt;GameObject&gt;(\"PlayerPrefab\");// 加载并实例化Addressables.InstantiateAsync(\"Enemy\");// 卸载资源Addressables.Release(handle);// 获取多个带标签的资源Addressables.LoadAssetsAsync&lt;GameObject&gt;(\"enemy\", callback);// 同步释放实例对象Addressables.ReleaseInstance(instance);// 通过标签加载多个资源（异步）Addressables.LoadAssetsAsync&lt;GameObject&gt;(new List&lt;object&gt; &#123; \"environment\", \"npc\" &#125;, obj =&gt; &#123; Debug.Log(\"加载完成：\" + obj.name);&#125;);// 通过资源名称加载（资源名称即为 key 或地址）string address = \"MyUI/StartButton\";Addressables.LoadAssetAsync&lt;GameObject&gt;(address).Completed += handle =&gt; &#123; if (handle.Status == AsyncOperationStatus.Succeeded) &#123; GameObject go = handle.Result; GameObject.Instantiate(go); &#125;&#125;; 构建与热更新使用流程示例 构建 Addressables 内容（在编辑器中） 12// 编辑器中构建 AddressablesAddressableAssetSettings.BuildPlayerContent(); 检查 Catalog 更新与远程热更新资源 123456789101112// 检查是否有 Catalog 更新var checkHandle = Addressables.CheckForCatalogUpdates();checkHandle.Completed += handle =&gt; &#123; if (handle.Result.Count &gt; 0) &#123; // 下载更新后的 Catalog var updateHandle = Addressables.UpdateCatalogs(handle.Result); updateHandle.Completed += catalogHandle =&gt; &#123; // 可在此下载资源依赖 Addressables.DownloadDependenciesAsync(\"MyRemoteLabel\"); &#125;; &#125;&#125;; 工程中加载远程资源的完整流程 1234567891011121314151617181920212223IEnumerator LoadRemoteAsset(string key)&#123; // 预下载依赖资源 var download = Addressables.DownloadDependenciesAsync(key); yield return download; // 异步加载资源 var handle = Addressables.LoadAssetAsync&lt;GameObject&gt;(key); yield return handle; if (handle.Status == AsyncOperationStatus.Succeeded) &#123; GameObject obj = handle.Result; // 实例化 GameObject instance = GameObject.Instantiate(obj); // 使用完毕后销毁实例 Addressables.ReleaseInstance(instance); &#125; // 卸载依赖（可选） Addressables.Release(handle);&#125; 加载本地资源 1234567891011// 本地路径加载方式相同，Addressables 自动判断是否为本地 Bundlevar handle = Addressables.LoadAssetAsync&lt;GameObject&gt;(\"LocalAssetKey\");handle.Completed += h =&gt; &#123; if (h.Status == AsyncOperationStatus.Succeeded) &#123; var go = GameObject.Instantiate(h.Result); // 使用完成后可销毁并释放资源 Addressables.ReleaseInstance(go); Addressables.Release(h); &#125;&#125;; 常用资源 核心资源类型划分与内存模型 Unity 支持的资源种类涵盖从图形渲染、音频播放到逻辑控制的方方面面，其常见分类及用途如下： Texture2D / Texture3D / Cubemap：图像纹理资源，用于 2D/3D 贴图、天光、环境映射等。 Mesh：模型资源，包含顶点坐标、法线、UV、切线、索引等几何结构数据。 AnimationClip：关键帧动画与曲线数据集合，驱动模型变换、骨骼动画等。 AudioClip：用于存储原始音频数据，支持 PCM、ADPCM、Vorbis 等格式。 Shader / Material：图形着色语言代码与其参数封装体，负责控制表面渲染效果。 Font：字体资源，支持 TrueType、OpenType 等格式。 TextAsset：存储任意文本或二进制文件内容的通用资源。 VideoClip：视频数据资源，供 VideoPlayer 播放使用。 Sprite：2D 图像切片，常用于 UI 元素或 2D 动画。 Prefab：预制体资源，是组件与 GameObject 层级结构的序列化封装。 Scene：场景资源，描述场景中所有对象的状态与引用。 ScriptableObject：自定义数据容器，广泛用于游戏逻辑配置。 AnimatorController / StateMachine：动画控制器与状态机配置资源。 这些资源在运行时均由 C++ 层的原生结构（NativeObject）表示，其 C# 封装对象中通常包含一个 m_CachedPtr 字段，指向真实的数据结构体。 资源导入器 Importer 与转换管线 Unity 的 Importer 系统负责解析外部资源格式，并将其转换为统一的中间表示与持久化形式。不同类型的资源使用不同的 Importer 进行处理。以下是典型资源的转换路径： Importer 支持的原始格式 配置结构（来自 .meta） 中间资源结构（Library 存储） TextureImporter .png, .jpg, .tga, .dds TextureImporterSettings + TextureSettings Texture2D + StreamingInfo ModelImporter .fbx, .obj, .dae RigImportSettings + MeshImportSettings Mesh / Avatar / AnimationClip AudioImporter .mp3, .wav, .ogg AudioImporterSettings AudioClip ShaderImporter .shader ShaderImportSettings Shader + ShaderSubProgram AnimationClipImporter .anim AnimationClipSettings + CurveMappings AnimationClip VideoClipImporter .mp4, .mov VideoImporterSettings VideoClip ScriptImporter .cs MonoImporterSettings MonoScript 导入完成后的资源对象会被序列化并缓存至 Library/ 目录，以优化后续构建和编辑流程。 NativeObject 内存结构分析 所有 Unity 的 C# 资源对象（如 Texture2D）底层都是对原生 C++ 结构体的托管包装，其核心字段 m_CachedPtr 指向内存中的实际数据。 以 Texture2D 为例，其底层结构类似如下： 1234567struct Texture2D : Texture &#123; int m_Width; int m_Height; TextureFormat m_Format; StreamingInfo m_StreamData; // 其他字段略&#125;; 运行时，Unity 的对象系统维护一张对象表，用于管理所有原生对象的生命周期、引用关系及其与托管对象的映射。 内存 PC 内存架构与管理机制 架构特性 分离内存结构：PC 通常配备独立 CPU 主内存（DDR4/DDR5）与 GPU 专用显存（GDDR），资源在 CPU 与 GPU 间通过 PCIe 总线传输，需注意显存带宽和 Host-to-Device 的数据拷贝开销。 地址空间更大：64 位操作系统提供大于 4GB 的虚拟地址空间。现代 Windows/Linux 提供每进程高达 TB 级别的虚拟空间。 NUMA（非一致性内存访问）：多核 CPU 常见于服务器/桌面端，存在多个物理内存节点，跨 NUMA 节点访问将增加延迟。 操作系统内存管理 分页系统：操作系统使用分页机制管理内存，常用 4KB 页，并支持大页（Huge Page）。通过页表（Page Table）实现虚拟地址到物理地址的映射。 页面换出（Swapping）：在内存不足时，系统将部分内存页写入磁盘（Swap File/Swap Partition），释放物理内存用于高优先级任务。Unity 应用在 Swap 下运行可能出现显著卡顿或帧率下降。 内存保护机制：通过页属性标识读/写/执行权限，防止非法访问，如访问空指针或释放后的内存会触发“段错误”（Segfault）。 手机内存硬件与系统层 操作系统的内存申请、使用与释放流程 内存申请（Allocation）：操作系统通过页分配器（如 vm_allocate on iOS, mmap on Linux/Android）按页为应用分配虚拟地址空间，通常以 4KB 页为单位。分配时可能并未实际占用物理内存，只有在访问该页时（写入）才会触发“页错误”并分配真实物理页（按需分配策略）。 内存使用（Access）：当应用访问虚拟内存时，操作系统通过页表（Page Table）和 TLB（Translation Lookaside Buffer）将虚拟地址映射到物理地址。部分内存页可能被标记为只读或共享页，以提高安全性与效率。 内存释放（Deallocation）：当进程释放内存（如通过 free()、munmap() 或系统层回收机制），对应虚拟页会被从页表中移除，物理页被标记为“可回收”。在某些平台上，这些页可能仍暂时保留（作为缓存），直到系统主动回收。 后台回收与内存压力响应：在内存压力高时，系统会回收不活跃页（Inactive Pages）、清除 Cache、或者直接终止后台进程（如 Android 的 LMK，iOS 的 Jetsam）。内存释放过程是懒惰式的，尽可能避免频繁回收和再次分配。 iOS 内存架构与管理机制 统一内存架构：iOS设备通常采用Unified Memory Architecture (UMA)，CPU 和 GPU 共享 LPDDR 系统内存，无需显式拷贝资源至独立显存，有利于资源访问效率，但也意味着 GPU 消耗的内存直接挤占 CPU 可用内存。 无Swap机制：iOS 虽使用虚拟内存地址系统，但不允许 swap 到磁盘，所有内存申请都必须在物理RAM范围内完成。 内存警告机制：应用若占用过多内存，系统会触发 UIApplicationDidReceiveMemoryWarningNotification 或 SwiftUI 的 @Environment(\\.memoryWarning)。若未及时响应释放资源，系统可能直接终止应用。 后台内存淘汰策略：在后台，iOS 会依据 memory pressure 使用 Jetsam 策略驱逐后台进程。Jetsam 是 Darwin 内核中 OOM 终结者，对内存压力进行 PID 层级逐步回收。 Android 内存架构与管理机制 设备差异化严重：Android 设备从低端 512MB 到高端 16GB RAM 均有，需针对低端设备优化路径分离。 Zygote预加载机制：Android 使用 Zygote 机制共享系统框架库，在进程fork时减少初始化成本和内存消耗。 GC管理机制：ART VM 默认采用并发分代垃圾回收（Concurrent Generational GC），分为 Eden、Survivor、Old 区，GC Pause Time 控制至 10ms 以内。大对象（&gt;256K）直接进入老年代。 内存回收接口：应用生命周期中可通过 onTrimMemory() 钩子处理如 TRIM_MEMORY_BACKGROUND / RUNNING_LOW 等状态，及时释放缓存资源。 堆限制查询：可通过 ActivityManager.getMemoryClass() 或 getLargeMemoryClass() 获取堆上限（如 128MB, 256MB 等）。 Unity引擎内存结构与使用方式 Unity 内存划分详解 托管内存（Managed）：C# 层对象，如 GameObject、MonoBehaviour、ScriptableObject 等，受 GC 控制。GC 不可预期，建议使用对象池避免频繁触发。 原生内存（Native）：Unity 内核层 C++ 数据结构：渲染、粒子系统、动画状态机、NavMesh 等。无法被 C# 的 GC 控制，需手动释放资源（如 Mesh、Texture、ComputeBuffer）。 图形内存（Graphics/GPU）：上传到 GPU 的资源，包括贴图、Mesh、ShadowMap、RenderTexture 等。在 OpenGL ES / Metal / Vulkan 下共享系统内存，容易与其他内存抢占资源。 临时内存（Temp Allocator）：帧内临时使用，如命中测试、物理检测、Render Loop中间态等，大小固定（4MB），超出将退化为 Heap Alloc，导致 GC Alloc 增加。 Unity 移动端内存行为 Memory Profiler 分类：分为 Total Reserved、Total Used、Mono、Gfx、Other，便于定位问题。 IL2CPP 与 Mono 差异：IL2CPP 性能更好、GC 控制更紧，代码大小增加，Native 内存增加；Mono 适合编辑器或开发测试环境。 默认内存策略：资源尽量异步加载、使用 Addressables 精细控制生命周期，避免热区资源常驻内存。 Unity 开发中的内存优化技巧 纹理优化 压缩格式选择： iOS：推荐使用 ASTC（质量/大小可控）；老设备兼容使用 PVRTC（压缩质量低）。 Android：优先 ASTC，其次 ETC2，最低 ETC1（无 Alpha）。Unity 可通过平台导入设置区分纹理压缩格式。 MipMap 与 StreamingMip：开启 Mipmap 可节省 GPU 带宽；结合 Texture Streaming 可按需加载低分辨率贴图，显著降低内存占用。 Atlas 图集管理：将小纹理合并为大图集可减少材质切换和 Draw Call，但需控制尺寸，避免合图过大导致 StreamingMip 效率低下。 网格与渲染优化 Mesh Compression：在导入设置中开启 Mesh Compression（如 Medium/High）减少顶点数据占用。 静态合批与动态合批： Static Batching：占用更多内存，但极大减少 Draw Call。 Dynamic Batching：限制顶点数 &lt; 900；小物体适合。 GPU Instancing：对重复物体使用 Instancing 替代合批，节省 CPU → GPU 的通信开销。 遮挡剔除/Frustum Culling：关闭不可见对象渲染，降低 GPU 运算压力与带宽使用。 动画系统优化 Animator Culling Mode：设置为 Cull Update Transforms 可在角色不在视野时停止动画更新。 Bone 限制：控制单角色骨骼数，限制每顶点绑定骨骼数量（&lt;4）。 动画压缩设置：启用 Optimal 压缩方式，移除冗余关键帧，减少曲线数据。 Bake 动画：将运行时动画 Bake 到 Transform 减少实时计算。 Addressables 使用技巧 生命周期管理： 每次 LoadAssetAsync 后应配对 Addressables.Release；否则引用计数未清零，导致内存泄漏。 异步加载场景与资源：使用 Addressables.LoadSceneAsync / InstantiateAsync 异步加载，减少卡顿；场景切换完成后手动 UnloadSceneAsync 和 Release。 多平台资源分包：资源按平台/分辨率拆包（如 HD/SD），避免高配资源在低端设备加载。 GC与堆内存优化 减少GC Alloc： 避免每帧字符串拼接、LINQ。 使用对象池代替频繁 Instantiate/Destroy。 使用 Struct 替代 Class 避免堆分配。 增量GC（Incremental GC）：Unity 2019+ 支持，分帧执行 GC，避免卡顿尖峰。 Memory Profiler：定期生成快照，比较引用链定位泄漏路径（如 ScriptableObject 没有正确释放）。 工具链 Unity Profiler：观察 GC Alloc、Native Mem、Gfx Mem 曲线，分析每帧内存使用。 Memory Profiler（Package）：快照对比、引用路径分析。 Android Profiler / Xcode Instruments：分析 Native 层使用、Java 层 GC/Leaks。 ADB shell：结合 lowmem-killer/stressapptest 模拟 OOM 场景。 平台差异与特别注意事项 Android 特有限制 部分 Android Go 设备堆上限仅 128MB，务必支持资源降级。 部分 GPU（如旧 Mali）不支持 ASTC，需动态切换纹理格式。 Android 的 RenderTexture 默认内存常驻，需主动销毁。 iOS 特性优化 使用 RenderTextureMemoryless.Depth 减少 Tile Memory 压力。 iOS Metal 统一内存访问快，但若分辨率过高，GPU 资源争抢更激烈。 使用 OnLowMemory() 钩子及时清理缓存资源，防止 Jetsam Kill。 代码优化(CPU时间) 明白了。我将整理一份针对 Unity 中使用 C# 和 XLua 的 CPU 优化方案，适用于 3D SLG 游戏，覆盖 PC、iOS 和 Android 平台，并对比 Mono 与 IL2CPP 的差异，同时包括 Unity Profiler 工具在优化过程中的使用建议。 请稍等片刻，我会尽快为你准备好详细内容。 常见C#脚本瓶颈及优化建议 GetComponent/查找：频繁调用 GetComponent、Find、Camera.main 等会遍历对象池，非常耗时，应在 Awake/Start 中缓存引用。如下例所示，将 transform 缓存到字段中避免重复查找： 1234private Transform _transform;void Awake() &#123; _transform = transform; // 缓存 Transform&#125; 对于 Camera.main、标签查找等，同样应缓存或在 Inspector 里引用。 Update 调用开销：每个挂 Update 的 MonoBehaviour 都会产生管理开销，数量过多时影响极大。当游戏中数百或上千个物体需要每帧更新时，建议使用全局管理器统一调度，而不是每个对象独立 Update。如 Unity 官方建议，将需要更新的对象注册到单例管理器中，由管理器在 Update 中遍历调用： 1234567891011121314public class UnitManager : MonoBehaviour &#123; private static readonly List&lt;Unit&gt; _units = new List&lt;Unit&gt;(); public static void Register(Unit u) &#123; _units.Add(u); &#125; public static void Unregister(Unit u) &#123; _units.Remove(u); &#125; void Update() &#123; float dt = Time.deltaTime; foreach (var u in _units) u.CustomUpdate(dt); &#125;&#125;public class Unit : MonoBehaviour &#123; void OnEnable() &#123; UnitManager.Register(this); &#125; void OnDisable() &#123; UnitManager.Unregister(this); &#125; public void CustomUpdate(float dt) &#123; /* 单位逻辑 */ &#125;&#125; 这样能避免大量原生到托管回调切换的开销。 委托和事件：C#的委托在添加/移除回调时会复制内部列表，频繁的订阅/取消订阅会造成大量开销。对于高频率的事件管理，尽量避免直接使用 delegate；可以改用自定义的可快速插入/删除的数据结构（如 List&lt;Action&gt; 或用户管理的事件分发机制）。在更新管理器中使用委托订阅时要注意，如果每帧动态添加/移除，性能会急剧下降。 反射：反射调用极慢，比直接调用慢数百到上千倍。优化思路是绕开反射：可预先缓存 MethodInfo/FieldInfo，或使用 Delegate.CreateDelegate、Expression 树等生成委托来调用。注意 IL2CPP 模式下不支持 IL.Emit 和动态表达式，最好改用委托或指针操作。总之，能用常规代码解决的场景尽量不要使用反射，若必须使用，要做缓存处理。 结构体与装箱：频繁使用值类型（struct）或属性访问可能带来隐性开销。例如反复使用 Vector3 自带字段可能会装箱。优化建议是将多个值型参数打平成静态方法，避免在Lua或 C# 侧创建额外对象。例如： 123456public static class TransformUtil &#123; // C#静态方法一次性设置坐标，比Lua逐字段赋值快得多 public static void SetXYZ(this Transform t, float x, float y, float z) &#123; t.position = new Vector3(x, y, z); &#125;&#125; XLua调用时 transform:setXYZ(x,y,z) 的效率比 transform.position = {x=..} 高数倍。 内存分配与GC：运行时大量分配会触发 GC，影响帧率。应尽量避免在 Update 或热路径中新建对象。例如，不要每帧 new List&lt;&gt;() 或拼接字符串，而应复用容器或使用对象池。如下示例，将列表提前创建并清空： 1234567private static readonly int listCapacity = 100;private readonly List&lt;int&gt; _list = new List&lt;int&gt;(listCapacity);void Update() &#123; _list.Clear(); for(int i=0; i&lt;listCapacity; i++) _list.Add(i); // … 使用_list&#125; 这样避免了每帧构造新列表的 GC.Alloc。另外，应小心使用 foreach 遍历（旧版本Unity会因 Enumerator 装箱导致 GC），常规可改用 for 循环或缓存迭代器。 Lambda与闭包：在 Lambda 表达式中捕获局部变量或实例字段会产生闭包对象，触发额外分配。尽量避免频繁在回调中使用捕获局部变量，如不需要可用静态变量替代。 其他注意点：避免每帧生成大量字符串（可用 StringBuilder）、禁用未使用的组件或脚本、合理拆分场景避免运行时加载过多对象等。 XLua脚本性能优化 C#⇆Lua 交互开销：XLua 等桥接方案每次从Lua侧访问C#对象时，都会经过 ObjectTranslator 的查表、推栈/取值等过程，开销极大。例如 gameobj.transform.position = pos 这样的调用在Lua中会经历多次翻译和分配，导致大量 CPU 时间和 GC 分配。优化方法是减少跨语言调用次数： 缓存C#对象：将常用 C# 对象保存在Lua局部变量中，避免重复查找。 静态函数替代成员：尽可能将逻辑封装为静态方法导出，避免在Lua中每次访问实例成员都要做对象查找。比如写成 LuaUtil.SetPos(obj, x,y,z) 用原生C#方法设置位置，可省去transform中间对象的反复创建，提升明显。 减少参数/返回类型复杂度：尽量使用基本类型作为参数和返回，避免在Lua和C#之间频繁传递 Unity 特有的值类型（如 Vector3、Quaternion）或数组，因为它们需要多次栈操作和内存分配。例如，把 void SetPos(GameObject obj, Vector3 pos) 拆成 void SetPos(GameObject obj, float x, float y, float z)，从测试来看会快得多。 值类型与GC优化：XLua支持在C#代码中为值类型加 [GCOptimize] 属性，用来优化 C#&lt;–&gt;Lua 之间的值类型传递。带 GCOptimize 的普通值类型（纯值域的 struct、枚举及其数组）在传递时可以避免 GC 分配。建议对需要频繁在Lua中传递的自定义 struct 添加 [GCOptimize] 和 [LuaCallCSharp] 标记。 Lua脚本本身优化：在Lua侧也要尽量减少中间创建。使用局部变量而不是全局，每帧循环中避免用table.insert等高开销操作；对于大量运算可考虑在C#侧提前实现为函数，Lua只作函数调用。避免在Lua里频繁创建临时表或字符串。 全局变量与引用：注意Lua持有的C#对象引用会阻止C#垃圾回收。应及时将不再使用的Lua全局置为 nil，或者手动调用 xlua.hotfix(GO,\"OnDestroy\") 断开引用，防止内存泄漏。 语言特性对性能的影响 委托与事件：如上所述，C# 委托在增删回调时会复制列表，事件订阅过多会拖慢性能。对于热路径或大量订阅，考虑采用自定义事件系统或静态回调列表以避免频繁复制。 反射：反射是一项非常昂贵的操作，要在性能敏感场合尽量避免。必要时可缓存反射得到的 MethodInfo/PropertyInfo，或者使用 Delegate、Expression 等方式预编译调用。记住 IL2CPP 不支持运行时动态生成代码（如 Expression.Compile()、IL.Emit），所以优化时优先考虑委托或原生写法。 GC 与内存分配：Unity 的垃圾收集分两代，需要关注分配量。经常会在 Profiler 中看到的 GC.Alloc 源头包括：循环中新建对象、字符串连接、LINQ、闭包、装箱以及 UI 动态生成等。优化要点是减少分配次数：使用对象池、复用集合（List.Clear() 循环复用）、避免频繁 string 拼接（可用 StringBuilder）、慎用 LINQ（会产生临时对象）。例如，每帧产生的垃圾越少，GC 越不频繁，帧率越稳定。 LINQ 和 lambda：LINQ 查询和 new 表达式常创建新对象；Lambda 捕获也会分配闭包实例。对 闭包 来说，如果引用的是局部或实例变量，则会分配；引用静态变量则不会。避免高频场景下使用这些特性。 循环遍历：在 Unity 旧版本中，foreach 可能会因生成枚举器而装箱导致 GC。推荐对简单集合使用 for(int i=0; i&lt;list.Count; i++)，并将 Count 缓存到局部变量，降低开销。 方法调用开销：C# 的虚方法调用比静态调用开销略高。对少量但频繁调用的简单方法，可考虑使用 sealed 类、static 方法或 [MethodImpl(MethodImplOptions.AggressiveInlining)]（在 IL2CPP 中可启用 C++ 级内联）来减少调用开销。此外，上述列表遍历示例也显示，多层属性和方法调用会带来多重函数调度成本。 数据结构：少用二维或多维 C# 数组（type[,]），因为其内部访问需要额外函数开销，性能远低于交错数组（type[][]）。对性能敏感的大量数据使用，应尽量使用一维数组或简单结构。 Mono 与 IL2CPP 的差异 编译方式：Mono 后端使用JIT（即时编译）在运行时将IL编译成本机码，灵活支持热更新和反射；IL2CPP 则是 AOT（提前编译）将 C# 转换为 C++ 再编译，生成本地机器码。IL2CPP 去除了运行时的JIT开销，对多线程和移动平台（尤其是iOS）具有性能和兼容优势。在移动端实践中，IL2CPP 通常比 Mono 运行更快（但编译时间长、包体更大），因此生产环境建议使用 IL2CPP，开发阶段可用 Mono 加快迭代。iOS 平台强制要求使用 IL2CPP（禁止 JIT），Android 平台支持 Mono 32 位和 IL2CPP 64 位。 性能差异：总体上，IL2CPP 经预编译优化后执行效率更高。一些论坛和测评也发现 IL2CPP 在低端机型上表现更好，但实际性能还需根据具体场景基准测试（部分特殊情况中 IL2CPP 可能略慢）。需要注意，IL2CPP 的 AOT 特性限制了某些动态功能：如动态生成代码（IL.Emit、Expression.Compile）不可用，反射性能与 Mono 相近但缺少 JIT 优化。 调优技巧：在 IL2CPP 下，虚调用去虚拟化可以带来性能提升。将没有继承需求的类或方法标记为 sealed，可让编译器使用直接调用代替虚表调用。从 Unity 2020.2 起，还可以用 [MethodImpl(MethodImplOptions.AggressiveInlining)] 强制 C++ 端内联，以减少函数调用和参数复制成本。对于性能敏感的数学函数，可考虑内联或手写高效版本。相比之下，Mono 模式下可用 Hot Reload、原生调试，但无法在 iOS 上运行，仅建议用于快速迭代测试。 性能分析工具与方法 Unity Profiler：在编辑器或真机（必须为开发版）上运行性能分析。使用 CPU Usage 模块查看各部分开销，按类别（渲染、脚本、物理、GC等）归纳每帧时间消耗。选择某一帧后，在 Profiler 的 “详细信息” 面板里切换 Timeline 和 Hierarchy 视图：Timeline 显示该帧内各线程的时间轴，可对比主线程与作业线程的并行关系；Hierarchy 则以层次结构列出所有函数调用及耗时，默认按耗时降序，有助于快速定位耗时热点。 深度性能剖析：启用 Deep Profiling Support 后，Profiler 会对所有脚本函数进行采样，而非仅限 ProfilerMarker 标记的代码。这使得启动等阶段的分析更全面，但会带来额外开销。一般先在核心流程确认大致瓶颈，再在必要时对某几个函数启用深度剖析。分析时可在 Profiler 视图中点击 GC Alloc 等条目，查看调用堆栈，找出垃圾产生的源头。 Timeline视图：在 Profiler CPU 模块的 Timeline 视图中，可以观察每帧各线程的活动分布。比如可直观看到主线程在等待 VSync 或 GPU 时间的情况，也可在作业系统场景下查看 Worker 线程何时被调度。熟悉常见的 Profiler 标签（如 Scripts.Update, Physics.Simulate, VSync 等）有助于判断开销归属。 ProfilerMarker：在关键代码段使用 Unity.Profiling.ProfilerMarker 标记，可以在 Profiler 的 Timeline/Hierarchy 中显示自定义名称，方便细粒度分析。例如： 123456static ProfilerMarker myMarker = new ProfilerMarker(\"CustomLogic\");void CustomLogic() &#123; using (myMarker.Auto()) &#123; // 性能热点代码 &#125;&#125; 其他工具：结合 Timeline Editor（帧捕获工具）分析渲染、GPU 等开销。使用 Memory Profiler 查看内存泄漏。对于网络或 IO，也可使用专门的分析工具。但总体原则是：先用 Profile 找出瓶颈所在，再针对性优化。 渲染优化（CPU端） 总体架构概览 Unity 使用基于 Component 的实体系统（GameObject + Component）组织场景中的渲染对象。典型的渲染对象由以下组件组成： Transform：提供位置、旋转、缩放。 MeshFilter：持有渲染的 Mesh 数据。 MeshRenderer：将网格通过材质渲染到屏幕上。 从 2018.1 起，Unity 支持 Scriptable Render Pipeline（SRP），替代 Built-in 渲染流程。SRP 将渲染控制权下放给开发者（通过 C# 实现的 RenderPipeline 和 RenderPipelineAsset），但底层提交仍由 C++ 实现完成（如 GfxDevice）。 渲染对象的分类与处理规则 Unity 中渲染对象的处理分为几个关键分类维度： 1. 静态 vs 动态 通过 GameObject.isStatic 标志决定是否为静态物体。 静态物体可参与 静态合批（Static Batching），在构建或运行时将多个对象合并为一个网格，大幅减少 Draw Call。 动态物体可使用 动态合批（Dynamic Batching）（受限条件：顶点属性总数 &lt; 900，不能使用多 Pass）。 底层静态合批由 C++ 层的 StaticBatchingUtility 负责，在构建时生成合并网格，并打包成 StaticBatchRoot 节点，运行时通过 StaticBatchRenderer 实现直接渲染。 2. 渲染队列（RenderQueue） 决定绘制顺序，影响透明度处理。 通常划分为： 0–2500：不透明（Opaque） 2501–3000：AlphaTest 3001–5000：透明（Transparent） 底层渲染队列由材质的 shader.renderQueue 决定，在 Material::ComputeRenderQueueWithOffset() 中与 SubShader 的默认值合并处理。 3. 渲染层（SortingLayer + OrderInLayer） 主要用于 2D 渲染排序控制。 对应的底层实现见 Renderer::m_SortingLayerID 和 m_SortingOrder，在渲染排序中通过 GetFinalSortOrder() 函数与摄像机参数一起参与排序逻辑。 4. SRP 分类流程（以 URP 为例） 在 URP 中分类逻辑主要集中在 ForwardRenderer 的 Setup() 和 Render() 中： 使用 ScriptableCullingParameters 对可见对象进行剔除（Frustum Culling + Occlusion Culling）。 结果是 CullingResults.visibleRenderersList。 在 DrawRenderers 时，传入 FilteringSettings 控制 Layer、Queue 等筛选。 使用 SortingSettings 指定排序方式（如 opaque 前向，transparent 后向）。 渲染前的处理流程 Unity 渲染对象在通过图形 API（如 DirectX/Metal/Vulkan）提交前经历如下处理流程： 1. Culling 剔除 通过 CullResults.Cull() 进行视锥体剔除（Frustum Culling）。 可启用遮挡剔除（Occlusion Culling），使用预计算的遮挡体（PVS）或动态 GPU 遮挡（如 Umbra）。 剔除逻辑底层通过 SceneCulling.cpp 和 Cull.cpp 中的 SIMD 加速处理 BoundingBox 与 Frustum 的包围盒交集检测。 2. Filtering 筛选 使用 FilteringSettings 控制：LayerMask、RenderQueueRange、ShaderTag、MotionVector。 对应底层结构为 FilterResults，通过 Renderer::Passes 与 ShaderTagId 做匹配。 3. Sorting 排序 使用 SortingSettings 控制排序规则： CommonOpaque：按材质和状态排序，减少状态切换。 CommonTransparent：按摄像机距离排序（后向前避免混合错误）。 实现上使用 DrawObjectSortFunctions.cpp 中的排序函数对 RenderList 排序。 4. State Setup 渲染状态准备 包括：设置 Camera matrices、Lighting、Lightmap、Global Shader Constants。 若开启 SRP Batcher：所有材质 Uniform 常量使用统一大 Buffer 上传，减少 CBuffer 切换。 渲染状态在 C++ 中使用 SetGlobalConstantBuffer 和 SetPassGlobalState() 设置，最终提交 DrawCallCommand。 5. 合批与实例化（Batching &amp; Instancing） 静态合批：提前合并网格，减少 Draw Call，由 StaticBatchRenderer 提交索引偏移进行绘制。 动态合批：CPU 合并顶点后提交。由 DynamicBatching.cpp 实现，运行时构建临时 VBO/IBO。 GPU Instancing：相同材质的多个对象通过 Material.enableInstancing 一次性绘制。 SRP Batcher：使用固定格式统一结构的 UniformBuffer 进行批处理。 底层由 BatchRendererGroup.cpp 调用 DrawMeshInstanced() 或 DrawInstancedProcedural() 实现。 6. CommandBuffer 构建与提交 所有绘制命令封装为 CommandBuffer（C#）或 ScriptableRenderContext。 最终调用 context.Submit()，封装为 RenderCommandBuffer 结构，提交给 C++ GfxDevice。 GfxDevice（如 GfxDeviceD3D11/GfxDeviceMetal）最终翻译为底层 API DrawCall 并提交 GPU Command Queue。 优化目标与性能考虑 阶段 目的 优化方法 剔除 减少不必要渲染对象 利用 Job System 多线程剔除，使用 GPU Occlusion Culling 筛选 精准筛选目标对象 使用 FilteringSettings 限制 Layer/Queue 排序 降低 GPU Pipeline flush Opaque 优先按材质排序，Transparent 后向前避免混合错误 合批 减少 Draw Call 数 静态合批/动态合批/GPU Instancing/SRP Batcher 状态 减少 CBuffer 绑定 启用 SRP Batcher（每材质使用统一 buffer） 提交 提高 CPU-GPU 并发效率 使用 CommandBuffer 封装所有命令后统一提交 关键源码位置与类 功能 源码/类 Renderer 渲染组件 UnityEngine.MeshRenderer、SkinnedMeshRenderer 剔除 CullResults、ScriptableCullingParameters，底层见 Cull.cpp 渲染指令 ScriptableRenderContext.DrawRenderers()，底层翻译为 RenderCommandBuffer 批处理 BatchRendererGroup、SRP Batcher（RenderGraph） GPU Instancing Material.enableInstancing = true；Shader 中使用 UNITY_INSTANCING_CBUFFER_START/END 宏 SRP 管线主类 RenderPipeline, RenderPipelineAsset，内部走 RenderPipelineManager.DoRenderLoop_Internal() 调度 网络优化 网络通信整体架构概览 在大型多人联网游戏中，通常采用如下架构： 客户端：Unity + C# + Xlua 服务端：C++/Java/Golang 等高性能语言 通信协议：TCP 为主，部分 UDP 或 WebSocket 用于推送、心跳等 数据格式：ProtoBuf / FlatBuffers / 自定义二进制协议 / JSON（调试） 客户端通信模块的职责包括： 创建 socket 并维护连接 数据收发与缓冲管理 协议解析、打包、粘包处理 异常重连与心跳保活 Socket 通信底层流程（以 TCP 为例） Socket 创建与连接建立 12Socket socket = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp);socket.Connect(ip, port); AddressFamily.InterNetwork: 使用 IPv4 协议 SocketType.Stream: 流式 socket，TCP ProtocolType.Tcp: 明确协议 设置 Socket 参数（可优化性能） 123socket.NoDelay = true; // 关闭 Nagle 算法，降低延迟socket.SendBufferSize = 64 * 1024;socket.ReceiveBufferSize = 64 * 1024; SendBufferSize 和 ReceiveBufferSize：设置的是 操作系统内核的 socket 缓冲区大小，而不是应用层 buffer。这影响了 socket 层对数据包的缓存能力。 NoDelay = true 用于关闭 Nagle 算法（详见后文），避免发送小包延迟。 建立连接过程（TCP 三次握手） 客户端发送 SYN 服务端返回 SYN + ACK 客户端发送 ACK 确认 网络数据发送流程 发送数据（Send） 1socket.Send(buffer, offset, length, SocketFlags.None); 底层实际调用 OS 的 send 函数，将数据写入 socket 的 发送缓冲区。 若缓冲区满，会阻塞或返回未写完的字节数（异步 IO 会挂起等待）。 数据组织 数据结构通常为：[协议头(长度+类型)] + [消息体] 避免频繁申请大数组，使用 ArrayPool&lt;byte&gt; 或环形缓冲池复用 批量打包小数据，避免碎片化，提高发送效率 Nagle 算法说明 Nagle 算法的目的：减少网络中小包数量，提高带宽利用率。 原理： 当 socket 中存在未确认的数据包时，暂不发送新的小包，等待 ACK 后再发。 缺点： 会引起 40~200ms 的发送延迟，尤其在 turn-based 游戏中尤为明显。 建议： 默认关闭（socket.NoDelay = true）提升实时性 网络数据接收流程 接收数据（Receive） 1socket.Receive(buffer, offset, size, SocketFlags.None); 数据从 OS 的内核 接收缓冲区 拷贝到应用层 buffer。 应用层通常使用独立线程 / 异步回调持续监听数据。 粘包 / 拆包问题 TCP 是面向字节流的协议，没有消息边界，需自己处理粘包/半包： 协议设计中加 Length 字段（前 4 字节等）来解包 自己维护缓存区，按 length 解析完整消息 性能优化建议 ArrayPool 的底层实现详解 System.Buffers.ArrayPool&lt;T&gt; 是 .NET 为了避免频繁创建和销毁大数组、减少 GC 压力而提供的一种数组租赁机制。 其默认实现为：System.Buffers.DefaultArrayPool&lt;T&gt;，核心结构如下： 内部结构： 通过 T[][] _buckets 管理多个“桶”，每个桶存储特定大小的数组。 每个桶对应一种标准容量（例如 16, 32, 64, ..., 最大支持到 1024*1024 级别）。 每个桶由一个 LockedStack&lt;T[]&gt;（或 ConcurrentBag）管理，支持线程安全的数组归还与租赁。 关键逻辑： Rent(int minimumLength)： 查找大于等于 minimumLength 的最近标准桶；若有可用数组直接返回，否则新建一个。 若请求长度大于 MaximumBufferSize，直接分配新数组。 Return(T[] array, bool clearArray = false)： 根据数组长度判断归还到哪个桶；若桶已满，则该数组被抛弃（等待 GC）。 clearArray = true 会调用 Array.Clear() 清空内容，避免引用保留导致的内存泄漏。 性能特性： 大量减少 byte[], int[] 等大数组频繁分配导致的 LOH（大对象堆）GC 开销。 使用线程本地栈优化热路径性能，避免锁竞争。 分桶策略有效避免数组碎片和大小不均。 使用建议： 配合 Span&lt;T&gt; 或 Memory&lt;T&gt; 使用更高效。 网络通信中，推荐作为接收缓冲区、序列化缓存池等。 优势总结：复用大数组减少 GC，提升网络中数据处理性能，降低内存抖动。 Span / Memory 的底层设计结构 Span： Span&lt;T&gt; 是一种轻量结构体（ref struct），用于表示托管内存中的一段连续区域（如数组、栈上内存、堆上内存的一部分）。 特点： 无堆分配、无 GC 支持 Slice()、CopyTo() 等高效操作 不能存储在字段中或捕获到 lambda 中，因为它可能指向栈上内存 底层结构： 包含：ref T _pointer 和 int _length JIT 会将对 Span 的访问转成指针偏移 + 边界检查 Memory： Memory&lt;T&gt; 与 Span&lt;T&gt; 类似，但是可存储的引用类型 特点： 可作为字段、异步传递 内部包含一个引用和偏移信息 可通过 .Span 转换成 Span&lt;T&gt; 使用 应用场景： 使用 ArrayPool + Span&lt;byte&gt; 构建零 GC 的序列化系统 异步函数中使用 Memory&lt;T&gt; 传递缓存，避免数组拷贝 C# vs C/C++ 网络通信性能对比（基于 Xlua 项目） 场景说明 在 C# + Xlua 结构中，如果将 Socket 网络通信逻辑移到 C/C++ 层，有如下潜在性能提升： 提升点分析 项目 使用 C# 实现 使用 C/C++ 实现 GC 压力 高（byte[]频繁回收） 低（自行管理内存） 字节操作性能 中（需借助 Span, BinaryWriter） 高（直接指针操作） 多线程 受限于托管线程模型 可使用原生多线程 跨 Lua 调用 Xlua 调用 C# 较频繁 可通过 P/Invoke 减少 Lua ↔︎ C# 跳转 数据结构复用 需使用 ArrayPool 或自建池 malloc/free + buffer pool 灵活控制 CPU 指令优化 不可控 可用 SIMD、zero-copy、batch send 等手段 场景适配建议 通信极致高频（如 20ms 一帧、RTS 推送）推荐用 C/C++ 若瓶颈在 IO 线程与序列化，可尝试 C# 调用原生插件（如 FlatBuffers C API） 若逻辑复杂不宜重构，仍可在 C# 中精细优化 buffer 分配与内存布局 附加建议：高并发优化策略 使用异步 I/O (SocketAsyncEventArgs) 或 IOCP（Windows）提升吞吐 网络协议压缩（Snappy / Zstd）降低传输成本， 但需要注意：加密后是高熵（数据重复度低）数据，压缩无意义，所有需要先压缩，再加密。否则压缩毫无意义，反而增加开销 心跳机制定时 + RTT 检测，动态调整重传超时（RTO） 使用双缓冲避免线程读写冲突 将关键协议划分为： 高优先级（心跳、同步帧） 中优先级（状态同步） 低优先级（日志、战报） 实现分通道异步发送（多队列） 参考 Unity Manual","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"优化","slug":"优化","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"性能优化-基础","slug":"Unity/Optimization/性能优化-基础","date":"2024-04-24T04:09:28.000Z","updated":"2025-07-05T03:04:21.599Z","comments":true,"path":"2024/04/24/Unity/Optimization/性能优化-基础/","link":"","permalink":"http://yoursite.com/2024/04/24/Unity/Optimization/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%9F%BA%E7%A1%80/","excerpt":"组成原理 在计算机科学中，组成原理通常指的是计算机系统的基本组成部分，比如主板 (Motherboard)、中央处理器（CPU）、存储器（内存）、输入/输出设备等，并且涉及到它们之间的连接和交互。本节主要介绍一下PC和手机的组成部件，以及它们的作用。 PC 主板 (Motherboard) 主板 (Motherboard)主要由BIOS、总线、扩展插槽、芯片组和I/O端口等组成。 主板图","text":"组成原理 在计算机科学中，组成原理通常指的是计算机系统的基本组成部分，比如主板 (Motherboard)、中央处理器（CPU）、存储器（内存）、输入/输出设备等，并且涉及到它们之间的连接和交互。本节主要介绍一下PC和手机的组成部件，以及它们的作用。 PC 主板 (Motherboard) 主板 (Motherboard)主要由BIOS、总线、扩展插槽、芯片组和I/O端口等组成。 主板图 BIOS BIOS（Basic Input/Output System）是计算机系统中的一个重要组成部分，它是一组固化在计算机主板上的软件程序，负责在计算机启动时初始化硬件设备、进行自检（POST，Power-On Self-Test）、加载操作系统等关键任务。下面是对BIOS的详细介绍： 功能： 初始化硬件：BIOS负责初始化计算机中的各种硬件设备，包括处理器、内存、硬盘、显卡、键盘、鼠标等。 自检（POST）：在计算机开机时，BIOS会进行自检，检测系统中各个硬件设备是否正常工作。 提供基本输入/输出功能：BIOS提供了基本的输入/输出功能，使得计算机能够与外部设备（如键盘、显示器、存储设备）进行通信。 启动操作系统：BIOS负责在计算机启动时加载操作系统的启动程序，将控制权转交给操作系统，使其能够运行。 存储位置： BIOS通常存储在计算机主板上的闪存芯片中，这种闪存通常被称为“BIOS芯片”或“CMOS芯片”。 在一些早期的计算机中，BIOS可能存储在EEPROM（Electrically Erasable Programmable Read-Only Memory）芯片或ROM芯片中。 用户界面： 传统的BIOS具有文本界面，用户可以通过键盘输入命令来配置和管理BIOS设置。 近年来，随着UEFI（Unified Extensible Firmware Interface，统一可扩展固件接口）的普及，许多计算机开始使用UEFI BIOS，它提供了更现代化和图形化的用户界面，使得用户能够更直观地进行设置和配置。 总线 一台电脑由多个电子元器件组成，如CPU，内存，硬盘等，各元器件之间往往需要相互传递数据。 数据传输的公用通道，就是总线。 区分总线承载能力的名词叫带宽,也就是单位时间内能传输的数据量。 常见的总线(通道)有SATA和PCIe。 SATA总线常见的就是SATA3.0，理论最高速度只有6Gbps，此类接口的固态硬盘理论传输速度为600M/s； PCIe总线参见的有PCIe3.0和PCIe4.0,通道数量不同，速度也会不同。以PCIe3.0x4通道总线来说，它的带宽高达32Gbps,此类接口的固态硬盘实际传输速度可以轻松突破1000MB/s,顶级产品甚至可以达到4000MB/s以上； 综上，同为固态硬盘，走的是哪种总线，至关重要。 协议 有了传输的通道， 那么就要定义传输通道将如何传输数据，这种传输数据的规则就叫协议。 目前，硬盘常用的协议，主要有AHCI和NVMe两种。 NVMe与AHCI都是逻辑设备接口的一种标准，不过NVMe相比AHCI在延时性、功耗、IPOS等一些方面性能要强。 不过，并非是说NVMe协议的固态硬盘一定比AHCI协议的速度快，具体的速度还要看走的是哪种总线，以及硬盘本身的性能。 接口 最后，我们来看一下固态硬盘的接口。 上面谈到的总线和协议，都是看不见摸不着的理论，因此，我们需要物理层面的接口，将硬盘与电脑连接起来。 最常见的硬盘接口，主要是SATA接口以及M.2接口。 SATA接口，主流硬盘接口之一，是Serial ATA的缩写，即串行ATA。它是一种电脑总线，主要功能是用作主板和大量存储设备（如硬盘及光盘驱动器）之间的数据传输。 M.2接口，又称为NGFF（老名字），是新一代接口标准，拥有比SATA接口更小的尺寸，同时提供更高的传输性能。按金手指的类型，M.2接口又细分为M-Key和B-Key，B&amp;M-Key。 芯片组 在早期的计算机主板中，北桥和南桥是两个独立的芯片，它们分别负责不同的任务。北桥芯片主要负责与处理器、内存和显卡之间的通信，而南桥芯片则负责与硬盘、USB设备和其他低速设备之间的通信。然而，随着技术的发展，现代计算机主板上已经不再使用北桥和南桥这两个独立的芯片了。取而代之的是将它们集成到单一的芯片组中，例如Intel的Z系列芯片组。 这种集成设计使得处理器、内存、显卡、硬盘、USB设备和其他设备之间的通信更加高效和可靠。通过将北桥和南桥的功能集成到单一芯片组中，主板制造商可以更好地控制硬件的兼容性和性能，同时也可以减少主板上的芯片数量，降低成本和功耗。 虽然北桥和南桥的概念已经逐渐被淘汰，但在一些特殊情况下，仍然需要使用到南桥芯片。例如，某些高级的USB设备可能需要直接与南桥芯片通信，因此在一些高端主板上仍然保留了南桥芯片。但是，这些南桥芯片的功能通常已经被大大简化，只负责一些基本的输入/输出任务。 总之，现代计算机主板上已经不再使用北桥和南桥这两个独立的芯片了。取而代之的是将它们集成到单一的芯片组中，这种设计使得硬件的兼容性和性能得到更好的保障，同时也可以降低成本和功耗。 中央处理器 (CPU) 中央处理器（CPU）是计算机系统中最重要的组件之一，其性能和功能直接影响到计算机的运行速度和能力。以下是CPU的主要参数： 核心数量（Cores）： CPU的核心数量指的是处理器内部包含的独立处理单元的数量。每个核心都可以执行独立的指令流，因此核心数量直接影响到CPU的多任务处理能力和并行计算能力。 多核处理器可以同时处理多个任务，提高系统的整体性能。 线程数量（Threads）： 线程数量指的是CPU能够同时执行的线程数量。线程是指在操作系统中独立调度和执行的基本执行单元。 超线程技术可以使得单个物理核心模拟出多个逻辑核心，从而提高处理器的线程并发能力，增加系统的多任务处理性能。 时钟频率（Clock Speed）： 时钟频率指的是CPU内部时钟的运行速度，通常以赫兹（Hz）为单位表示。时钟频率越高，CPU执行指令的速度就越快。 虽然时钟频率是CPU性能的重要指标之一，但不同架构和制造工艺的CPU之间不能直接比较。因此，时钟频率并不是唯一决定性能的因素。 缓存大小（Cache Size）： 缓存是CPU内部用于临时存储数据和指令的高速存储器。它可以加速CPU对常用数据和指令的访问，提高系统的响应速度和整体性能。 CPU通常包含多级缓存（如L1、L2、L3缓存），缓存大小对CPU的性能和成本都有重要影响。 制造工艺（Manufacturing Process）： 制造工艺指的是CPU芯片的制造工艺技术，包括晶体管的尺寸和布局等。制造工艺的进步可以使得CPU在相同尺寸下集成更多的晶体管，提高性能和能效。 常见的制造工艺包括14纳米、10纳米、7纳米等。 指令集架构（Instruction Set Architecture，ISA）： 指令集架构定义了CPU支持的指令集合和指令执行方式。常见的指令集架构包括x86、x86-64（AMD64）、ARM等。 指令集架构决定了软件在CPU上的运行兼容性和性能表现。 内存 (RAM) 内存（RAM，Random Access Memory）技术包括多种类型，每种类型都有不同的特点、优势和用途。以下是几种常见的内存技术及其区别： DRAM（Dynamic Random Access Memory）： DRAM是一种动态随机存取存储器，它使用电容来存储数据，并且需要周期性地刷新以保持数据的有效性。 主要分为SDRAM（Synchronous DRAM）、DDR（Double Data Rate）、DDR2、DDR3、DDR4和DDR5等几个主要代数。每一代DDR内存都在数据传输速度、功耗和密度等方面有所改进。 区别在于每一代DDR内存的频率、带宽和时序参数有所不同，随着代数的增加，内存的性能和功耗都得到了提升。 SRAM（Static Random Access Memory）： SRAM是一种静态随机存取存储器，它使用触发器（flip-flops）作为存储单元，相比DRAM，SRAM的访问速度更快，但成本更高。 SRAM通常用于高性能的缓存和寄存器文件等需要快速访问的应用，如CPU缓存和高性能存储器。 图形处理器 (GPU) GPU（图形处理器）是一种专门设计用于处理图形和图像相关任务的处理器。它们最初是为了处理计算机图形渲染而设计的，但随着技术的发展，GPU 在其他领域也发挥着越来越重要的作用，如科学计算、深度学习、数据分析等。 GPU主要有以下几个组件： 处理器核心：GPU 中包含大量的处理器核心，每个核心都能够执行特定的计算任务。这些核心通常被设计成多线程，并能够同时处理多个数据。 内存：GPU 配备自己的内存，用于存储图形数据、纹理和其他计算所需的数据。这些内存通常是高速且具有大容量。 显存控制器：负责管理 GPU 内存的访问和分配，以确保高效的数据传输和处理。 图形管线：图形管线是 GPU 中用于处理图形渲染任务的主要组件之一。它包括顶点处理、几何处理、光栅化、像素处理等阶段，用于将图形数据转换成最终的图像输出。 纹理单元：用于处理纹理映射和纹理滤波等任务的单元，以提高图形渲染的质量和效率。 渲染输出单元：负责将处理后的图像数据输出到显示器或存储设备，以供用户观看或后续处理。 存储设备 (Storage) 存储设备是计算机系统中用于存储数据的硬件设备。它们允许用户将数据永久地保存在计算机中，并在需要时进行访问和检索。存储设备通常根据其内部技术、容量、速度和用途等因素进行分类。以下是存储设备的一些常见类型和特点： 主要类型： 硬盘驱动器（HDD）： 使用旋转磁盘和磁头来读写数据。 相对较大的存储容量，但读写速度相对较慢。 适用于大容量存储和长期存储。 固态硬盘（SSD）： 使用闪存存储器来存储数据，无机械运动部件。 读写速度快，响应时间短。 耐用性好，不容易受到震动和冲击的影响。 适用于需要高速读写和响应的应用，如操作系统和应用程序。 光盘驱动器： 使用激光技术读写数据，包括 CD、DVD 和 Blu-ray 等。 适用于光盘媒体上的数据存储和传输。 闪存驱动器（USB 存储设备）： 使用闪存存储器来存储数据。 便携小巧，易于携带和使用。 适用于临时数据传输和备份。 网络存储（NAS）： 使用专用存储设备连接到网络，可以通过网络访问存储的数据。 可以提供共享文件存储、数据备份、远程访问等功能。 手机 手机相比于电脑，集成度就更高。手机也是通过主板链接各个硬件系统，其他中包括： 主板（Motherboard）：主板是手机的核心组件，包含处理器（CPU）、内存（RAM）、存储芯片（ROM）、通信芯片、传感器等重要部件。 显示屏幕（Display Screen）：显示屏通常是手机的最大部分，用于显示图像、文字和视频等内容。它可以是LCD（液晶显示器）、OLED（有机发光二极管）或AMOLED（主动矩阵有机发光二极管）等技术。 外壳（Casing）：外壳是手机的外部框架，用于保护内部电子元件和提供结构支撑。它通常由塑料、金属或玻璃等材料制成。 电池（Battery）：电池提供手机所需的电力。它通常是可充电的锂离子电池，尺寸和容量会根据手机型号和设计而有所不同。 摄像头（Camera）：现代手机通常配备前置摄像头和后置摄像头，用于拍摄照片和视频通话。 扬声器和麦克风（Speakers and Microphones）：扬声器用于播放音频，麦克风用于接收声音并进行通话或录音。 连接器（Connectors）：手机通常具有充电端口、耳机插孔、SIM卡插槽和扩展存储卡插槽等连接器，用于连接外部设备和提供扩展功能。 天线（Antennas）：天线用于接收和发送无线信号，包括Wi-Fi、蓝牙、GPS和移动网络信号等。 主板上核心的就是SOC，全称“System On Chip”，翻译成中文应该叫“片上系统”，通俗来讲，它表示“所有功能集成在一片上”，我们常说的“骁龙855”，“麒麟980”就是SOC。SOC主要包括： 中央处理（CPU），负责执行手机上的各种计算任务。处理器的性能直接影响到手机的运行速度和响应能力。ARM是常见的处理器架构。 图像处理器（GPU），图形处理器负责处理手机上的图形和视觉内容，包括游戏、视频播放和图形用户界面等。它能够加速图形渲染和处理复杂的图形效果。 嵌入式神经网络处理器（NPU），它的全称叫neural-network process units，这个名字听起来很高端，简单来说，它主要负责负责处理涉及神经网络算法和机器学习的海量数据，因为神经网络算法及机器学习需要涉及海量的信息处理，而当下的 CPU / GPU 都无法达到如此高效的处理能力，需要一个独立的处理芯片来做这个事，才有NPU的诞生。现在的“人工智能”AI的概念可以说非常火热，而NPU就是让手机变得更智能，更聪明的必要条件。 图像处理器（ISP），全称Image Signal Processor，不是GPU的“图形处理器”！它负责接收感光原件CMOS的原始数据，对这些数据做出“粗加工”，得到最后我们看到的照片，ISP需要与CMOS匹配。 基带（Baseband）, 基带的核心就是调制解调器（Modem）,这个调制解调器主要的作用就是负责信号传输，所谓调制，就是把需要传输的信号，通过一定的规则调制到载波上面让后通过无线收发器发送出去的工程，解调就是相反的过程，等于说它把基站的语言“翻译成”手机能懂的，让二者能够“顺畅交流”，我们用户就能接打电话，连接网络了。 协处理器(Coprocessor), 这是一种协助中央处理器完成其无法执行，或执行效率、效果低下的处理工作而开发和应用之处理器。这种中央处理器无法执行的工作有很多，比如设备间的信号传输、接入设备的管理等；而执行效率、效果低下的有图形处理、声频处理等。为了进行这些处理，各种辅助处理器就诞生了。 数字信号处理器(DSP), 它全称叫Digital Signal Processor，它不仅仅应用在手机，在雷达、通信、图像处理、医疗电子、工业机器人等高密集计算领域皆有广泛应用，手机上而言，主要负责语音，包括通话和语音输入，也负责一些图像处理的任务。 内存(Memory)，仅提供支持的内存类型，并非代表SOC芯片里也集成了内存。除了内存之外，SOC也能影响所用的闪存类型，比如有的SOC只能用eMMC闪存。 闪存(Flash)，仅提供支持的内存类型, 闪存对应的PC的硬盘。 ARM结构 说到移动平台就不得不提ARM，ARM（Advanced RISC Machines）是一家总部位于英国剑桥的半导体和软件设计公司，成立于1990年。ARM 以设计低功耗、高性能的 RISC（Reduced Instruction Set Computing）架构处理器而闻名。该公司的处理器架构被广泛应用于移动设备、智能手机、平板电脑、物联网设备、汽车电子、嵌入式系统以及工业控制等领域。 ARM 公司的一些关键特点和业务范围： 处理器设计：ARM 设计了一系列低功耗、高性能的处理器架构，包括 Cortex-A、Cortex-R 和 Cortex-M 系列，覆盖了从高性能应用到嵌入式系统的广泛范围。 授权模式：ARM的授权方式分为TLA（技术许可协议）和ALA（架构许可协议）两类：TLA是指客户直接购买Arm的IP来用，可以在上面进行部分修改，比如高通的骁龙系列芯片；ALA则允许客户基于Arm架构下的指令集来自行设计IP，开发定制处理器内核，苹果芯片就是典型代表 生态系统：ARM 拥有庞大的合作伙伴和生态系统，包括芯片设计厂商、芯片制造厂商、软件开发者、系统集成商等。这些合作伙伴共同推动了 ARM 技术的发展和应用，并且为客户提供了全方位的技术支持和解决方案。 物联网和智能化：随着物联网和智能化技术的快速发展，ARM 公司致力于为物联网设备、智能家居、智能城市等领域提供先进的处理器架构和解决方案，推动智能化应用的普及和发展。 全球影响力：ARM 的处理器架构已经成为全球最流行的处理器架构之一，几乎所有的智能手机、平板电脑和物联网设备都采用了 ARM 的处理器架构。ARM 公司在全球范围内拥有广泛的客户和市场影响力。 由ARM公司设计出来的芯片架构就是ARM架构也叫做ARM指令集架构，ARM公司将这些设计以知识产权授权的方式给其他芯片厂商或集成商，不同的授权协议具有不同的权限： 1. TLA（技术许可协议）， 依据售卖的芯片收费，但ARM好像打算改变了收取设计权利金的依据，将从芯片均价改为设备均价。 2. ALA（架构许可协议）， 这种模式的自由度最大，适合那些技术强劲的公司，比较典型的就是苹果公司，他们购买相关指令集后，自己去设计芯片，此外高通，华为也是购买的架构和指令集。 ARM的特点: ARM指令都是32位定长的（ARMv7架构及之前版本都是32位，但是ARMv8架构一部份采用了64位指令集，而2022年6月29号发布的ARMv9版本芯片则全面采用64位指令集） 寄存器数量丰富（37个寄存器（大多）） 普通的Load/Store指令 多寄存器的Load/Store指令 指令的条件执行 单时钟周期中的单条指令完成数据移位操作和ALU操作 通过变种和协处理器来扩展ARM处理器的功能 扩展了16位的Thumb指令来提高代码密度 ARM作为RISC微处理器与CISC微处理器技术对比如下： 上面提到的Cortex-A、Cortex-R， Cortex-M 和ARMv8/9有什么关系？ ARMv8 和 Cortex-A 是 ARM 公司的两个不同概念，但它们之间存在着密切的关系。 ARMv8： ARMv8 是 ARM 公司发布的第八代指令集架构（ISA），它定义了处理器的指令集和执行规范。ARMv8 架构支持 64 位和 32 位的指令集，并引入了许多新的特性和增强功能，如更大的寻址空间、更强的安全性、更高的性能等。ARMv8 架构使得 ARM 处理器能够在 64 位模式下运行操作系统和应用程序，提高了计算能力和系统的扩展性。 Cortex-A： Cortex-A 是 ARM 公司设计的一系列面向高性能应用的处理器核心。这些处理器核心采用了 ARMv8 架构，并且针对不同的应用场景提供了不同的性能和功能。Cortex-A 系列处理器核心通常用于智能手机、平板电脑、服务器等高性能计算设备，以及一些嵌入式系统和物联网设备。 关系： ARMv8 架构定义了处理器的指令集和执行规范，而 Cortex-A 系列处理器核心是基于 ARMv8 架构设计的具体实现。换句话说，Cortex-A 处理器核心是遵循 ARMv8 指令集架构的处理器核心之一。因此，Cortex-A 处理器核心通常被称为 ARMv8 架构的一部分，它们共同构成了 ARM 公司在高性能计算领域的解决方案。 SOC 目前主流的SOC有：高通骁龙（Snapdragon）, 苹果A系列，海思麒麟，联发科天玑和三星Exynos（艾克西诺斯） 高通骁龙（Snapdragon） 高通骁龙（Snapdragon）系列是高通公司推出的移动处理器产品线，被广泛用于智能手机、平板电脑、智能穿戴设备、智能家居产品等移动设备和物联网设备中。 高通骁龙系列的一些主要特点和优势： 高性能处理器核心： 骁龙系列处理器采用了高通自主设计的 Kryo 处理器核心，具有出色的计算性能和能效。这些处理器核心通常根据型号不同分为 Prime 核心、性能核心和节能核心，以平衡性能和功耗。 先进的图形处理器： 骁龙处理器集成了 Adreno 图形处理器，提供了出色的图形性能和游戏体验。Adreno 图形处理器具有强大的图形渲染能力、支持高帧率游戏和流畅的视频播放等特性。 多模式连接技术： 高通骁龙处理器集成了先进的多模式连接技术，包括 LTE、5G、Wi-Fi 和蓝牙等多种连接方式，以提供高速、稳定的网络连接和无缝的通信体验。 人工智能加速器： 最新的骁龙处理器还集成了高通的人工智能引擎（AI Engine），包括神经处理器（NPU）和 DSP（数字信号处理器），用于加速 AI 和机器学习应用，如语音识别、图像处理等。 高清摄像和音频技术： 高通骁龙处理器支持高清摄像和音频技术，包括多摄像头配置、高分辨率视频录制和播放、立体声音频效果等，提供了优质的多媒体体验。 安全和隐私保护： 高通骁龙处理器集成了安全硬件模块和安全软件功能，提供了可靠的设备安全性和隐私保护，包括指纹识别、面部识别、硬件加密等功能。 苹果A系列 苹果A系列芯片拥有一系列特性，这些特性使其在性能、能效、图形处理和人工智能方面表现出色。以下是一些主要的苹果 A 系列芯片特性： 先进的制程工艺： 苹果 A 系列芯片采用先进的制程工艺，如 5 纳米或 7 纳米工艺，这有助于提高芯片的性能和能效，同时减小芯片的尺寸和功耗。 多核心 CPU 设计： 苹果 A 系列芯片通常采用多核心的 CPU 设计，其中包括高性能核心（大核心）和节能核心（小核心）。这种设计能够在处理不同负载时平衡性能和功耗，提高了整体的能效比。 高性能 GPU： 苹果 A 系列芯片集成了强大的图形处理器（GPU），提供了出色的图形处理性能。这些 GPU 通常采用多核心设计，并且支持最新的图形技术和特效，如 Metal 图形引擎。苹果之前使用的是Imagination(POWERVR)，但是现在苹果也在逐渐自研替代。 神经引擎（Neural Engine）： 苹果 A 系列芯片集成了专门用于人工智能（AI）和机器学习（ML）任务的神经引擎。这些引擎能够加速图像识别、语音识别、自然语言处理等 AI 应用，提供了更快的推理速度和更高的效率。 高效的能耗管理： 苹果 A 系列芯片采用了先进的能耗管理技术，包括动态调频、异步处理、功耗优化等，以提高芯片在不同负载下的能效比，延长设备的电池续航时间。 整合式芯片设计： 苹果 A 系列芯片采用了整合式芯片设计，将 CPU、GPU、神经引擎等核心功能集成在一颗芯片上。这种设计能够提高系统的整体性能、减小芯片的尺寸和功耗，并简化设备的硬件布局。 定制化软硬件协同设计： 苹果 A 系列芯片的设计与苹果自家的操作系统（如 iOS）和应用程序（如 Metal 图形引擎）进行了紧密的协同设计和优化，以提供最佳的性能和用户体验。 海思麒麟（HiSilicon Kirin） 海思麒麟（HiSilicon Kirin）是华为旗下的半导体设计公司海思（HiSilicon）推出的一系列移动处理器（SoC）品牌，主要用于华为和荣耀品牌的智能手机、平板电脑和其他移动设备。 海思麒麟芯片的特点和技术亮点： 多核心 CPU 设计： 麒麟芯片通常采用多核心的 CPU 设计，包括高性能的大核心和节能的小核心，以平衡性能和功耗。这种设计能够在不同负载下提供更好的性能和能效比。 强大的图形处理能力： 麒麟芯片集成了强大的图形处理器（ARM Mali GPU），提供了优秀的图形处理能力和游戏性能。这些 GPU 通常支持最新的图形技术和特效，为用户提供更好的视觉体验。 AI 加速器： 麒麟芯片集成了专门的人工智能（AI）加速器，用于加速 AI 和机器学习（ML）任务。这些加速器能够提高图像识别、语音识别、自然语言处理等 AI 应用的性能和效率。 多模式连接技术： 麒麟芯片集成了先进的多模式连接技术，包括 LTE、5G、Wi-Fi 和蓝牙等多种连接方式，以提供高速、稳定的网络连接和无缝的通信体验。 安全和隐私保护： 麒麟芯片集成了安全硬件模块和安全软件功能，提供了可靠的设备安全性和隐私保护，包括指纹识别、面部识别、硬件加密等功能。 联发科天玑（MediaTek Dimensity） 联发科天玑（MediaTek Dimensity）是联发科技术公司推出的一系列移动处理器（SoC）品牌，旨在为智能手机和其他移动设备提供高性能、高效能和先进的连接性能。Dimensity 系列处理器以其强大的性能、集成的 5G 连接和 AI 加速等功能而备受关注。 联发科天玑处理器的特点和技术亮点： 多核心 CPU 设计： Dimensity 系列处理器采用了多核心的 CPU 设计，包括高性能核心和节能核心，以平衡性能和功耗。这种设计能够在处理不同负载时提供更好的性能和能效比。 集成 5G 连接： Dimensity 系列处理器集成了 5G 调制解调器，支持多模式 5G 连接，包括 SA（独立组网）和 NSA（非独立组网），以及 mmWave 和 Sub-6GHz 频段，为用户提供高速、稳定的网络连接。 先进的图形处理能力： Dimensity 系列处理器集成了强大的图形处理器（ARM Mali GPU），提供了优秀的图形处理能力和游戏性能。这些 GPU 通常支持最新的图形技术和特效，为用户提供更好的视觉体验。 人工智能加速器： Dimensity 系列处理器集成了专门的人工智能（AI）加速器，用于加速 AI 和机器学习（ML）任务。这些加速器能够提高图像识别、语音识别、自然语言处理等 AI 应用的性能和效率。 多核心 ISP： Dimensity 系列处理器集成了多核心的图像信号处理器（ISP），支持多摄像头配置、高分辨率图像和视频录制、实时图像处理等功能，提供了出色的摄像和摄录体验。 全球导航卫星系统（GNSS）： Dimensity 系列处理器支持多种全球导航卫星系统，包括 GPS、GLONASS、Galileo、BeiDou 等，以提供精准的定位和导航服务。 三星Exynos（艾克西诺斯） 三星 Exynos（艾克西诺斯）系列处理器是由三星电子公司设计和生产的一系列移动处理器（SoC），主要用于其旗下的智能手机、平板电脑和其他移动设备。Exynos 系列处理器以其强大的性能、丰富的功能和先进的技术而闻名，是三星智能设备的核心组件之一。 三星 Exynos 系列处理器的特点和技术亮点： 多核心 CPU 设计： Exynos 系列处理器采用了多核心的 CPU 设计，包括高性能核心和节能核心，以平衡性能和功耗。这种设计能够在处理不同负载时提供更好的性能和能效比。 强大的图形处理能力： Exynos 系列处理器集成了强大的图形处理器（ARM Mali GPU），提供了优秀的图形处理能力和游戏性能。这些 GPU 通常支持最新的图形技术和特效，为用户提供更好的视觉体验。 AI 加速器： 最新的 Exynos 系列处理器通常集成了专门的人工智能（AI）加速器，用于加速 AI 和机器学习（ML）任务。这些加速器能够提高图像识别、语音识别、自然语言处理等 AI 应用的性能和效率。 图像处理器（GPU） 主流的手机 GPU 包括以下几种： Adreno GPU（由高通设计）：Adreno GPU 是目前市场上使用最广泛的 GPU 之一，广泛应用于高通 Snapdragon 系列移动处理器中。它具有出色的图形处理能力和游戏性能，在智能手机和平板电脑等移动设备上表现突出。 Mali GPU（由 ARM 设计）：Mali GPU 是 ARM 公司设计的一系列图形处理器，被广泛用于三星 Exynos、联发科 Dimensity 等系列处理器中。Mali GPU 以其良好的能效比和优秀的图形处理性能而闻名。 Apple GPU 苹果公司在其 A 系列处理器中采用自家设计的 GPU，以提供卓越的图形处理性能和游戏体验。虽然具体的架构和型号没有公开，但苹果 GPU 在性能和能效上表现出色。 PowerVR GPU（由 Imagination Technologies 设计）：PowerVR GPU 曾经被苹果采用在早期的 iPhone 上，目前在一些联发科处理器中仍有使用。它具有出色的图形渲染能力和游戏性能。 嵌入式神经网络处理器（NPU） 什么是 NPU 人工智能加速器 NPU (Neural-network Processing Unit)是一类基于 DSA (Domain Specific Architecture) 领域专用架构技术的专用于人工智能（特别是人工神经网络、机器视觉、机器学习等）硬件加速的微处理器或计算系统。典型的应用包括机器人学、物联网等数据密集型应用或传感器驱动的任务。相比于 CPU、GPU，NPU 在硬件架构设计时便只针对于人工智能设计，举例来说，HUAWEI Kirin DaVinci Core 集成矩阵计算单元（Cube Unit）、向量计算单元（Vector Unit）和标量计算单元（Scalar Unit），可以通过硬件指令在一个周期内完成3D Cube、Vector向量、Scalar标量的计算，相比于通用处理器，其算力与数据吞吐量之比有数百倍提升，同时功耗维持在较低值。 NPU的诞生 长期以来，应用需求一直牵动着嵌入式技术、芯片技术的发展方向。随着深度学习神经网络的兴起，人工智能、大数据时代的来临，CPU 和 GPU 由于其造价高、功耗高、算力低渐渐难以满足端侧应用需要，面对日渐旺盛的需求和广大的预期市场，设计一款专门用于神经网络深度学习的高效智能处理器显得十分必要，因此NPU应运而生。 从技术角度看，基于卷积神经网络的机器学习技术实际上是一类多层大规模人工神经网络。它模仿生物神经网络而构建，由若干人工神经元结点互联而成。神经元之间通过突触两两连接，突触记录了神经元间联系的权值强弱。由于深度学习的基本操作是神经元和突触的处理，神经网络中存储和处理是一体化的，都是通过突触权重来体现，而在冯·诺伊曼结构中，存储和处理是分离的，分别由存储器和运算器来实现，二者之间存在巨大的差异。当用现有的基于冯·诺伊曼结构的经典计算机(如 X86、ARM 通用处理器和英伟达 GPU )运行神经网络应用时，就不可避免地受到存储和处理分离式结构的制约，数据吞吐量限制算力。因此，DSA 架构的专业芯片 NPU 便应运而生。 NPU 的功能 CPU (central processing unit) 是中央处理器。主要包括运算器（ALU）和控制单元（CU），还包括若干寄存器、高速缓存器和它们之间通讯的数据、控制及状态的总线。CPU 作为计算机系统的运算和控制核心，是信息处理、程序运行的最终执行单元。它主要负责多任务管理、调度，具有很强的通用性，是计算机的核心领导部件，其计算能力并不强，更擅长逻辑控制。 GPU（Graphics Processing Unit）是一种图形处理器，它可以弥补 CPU 在计算能力上的天然缺陷。相对于CPU 较少的内核较多的资源而言，它采用数量众多的计算单元和超长的流水线，善于进行大量重复计算，处理图像领域的运算加速。它的基本思想是并行计算即用多个处理器来共同求解同一问题，将被求解的问题分解成若干个部分，各部分均由一个独立的处理机来并行计算。但是缺陷也很明显，即协调、管理能力弱，无法单独工作，需要 CPU 进行控制调度。虽然 GPU 用于深度学习运算时速度比 CPU 有很大提升，但对于特定应用场景其仍有功耗大，驱动逻辑复杂，性能提升不足等问题。 NPU 与通用处理器设计思路不同。通用处理器考虑到计算的通用性，在提升算力的同时要考虑到数据吞吐量的提升 NPU 针对特定领域设计，无需考虑通用应用对于内存带宽的需求。相较于 CPU 擅长处理任务和发号施令，GPU 擅长进行图像处理、并行计算算，NPU 更擅长处理人工智能任务。NPU 通过突触权重实现存储和计算一体化，从而提高运行效率。NPU 也有不足，如特定的指令集可能只满足部分机器学习的需要，而不支持的指令或多个神经网络的组合计算仍然需要回落 (Fallback) 至通用处理器计算。 NPU 的应用 NPU 目前较多的在端侧应用于 AI 推理计算，在云端也有大量运用于视频编解码运算、自然语言处理、数据分析，部分NPU还能运用于 AI 的训练。 NPU 在端侧的运用 NPU 在端侧的运用较多，如 Apple、MTK、Kirin、清华紫光、瑞芯微等芯片厂商都有在其基于 ARM 架构的 Soc 内集成单独的 NPU 核心，辅助 CPU 完成异构计算；在纯微控制器（MCU）领域，STM32、Arduino、勘智等微控制器也有集成单独的 NPU 核心，方便在微控制器领域集成现代算法。具体的应用有：基于人脸识别的考勤机、基于 DHN（深度哈希网络）的掌纹识别、基于图像分类的自动垃圾分类、自动驾驶汽车、自动跟焦摄像机、监视系统等。其内嵌算法主要以卷积神经网络为主。 NPU 在云端的应用 NPU在云端的应用较少，目前主要以通用 GPU 运算为主，具体来说：百度有用于自然语言处理；华为有用于视频编解码运算；Google 将 TPU 运用于云端训练等。受限于部分 NPU 在架构设计的时候以算力功耗比为第一目标，其指令集较为精简，故用于模型训练较少，可能会遇到不支持的算子等问题。 内存(Memory) 目前手机主力的内存技术LPDDR5X，LPDDR5X是由JEDEC于2021年6月发布，它是一种专用的同步动态随机存取存储器（SDRAM）。与之前的LPDDR5标准相比，LPDDR5X在多个方面实现了改进： 在保持1.1V内核电压不变的情况下，速度从6.4Gbps提升到8.5Gbps 通过采用接收器均衡和发射器预加重技术，改善了信号完整性 全新的自适应刷新管理功能提高了可靠性 电池效率提高了多达20% 闪存(Flash) 目前手机系统主流的存储器件有两种，一种是安卓手机使用的UFS，另外一种是苹果用的NVMe。 UFS简介 UFS，Universal Flash Storage，通用闪存存储。为什么现在主流使用UFS呢？很简单，就是快。我们看下主流2lan的UFS，其顺序读的数据传输速率可以达到4.2GB/s： UFS读取速度 UFS为什么这么快呢，对比手机以前使用的eMMC，有如下优势： UFS采用差分串行传输，而eMMC采用并行数据传输。并行最大的问题是速度上不去，因为一旦时钟频率提升，干扰就变大，信号完整性无法保证。随着时钟频率越来越高，高速串行传输的优势就很明显了。 支持多通道数据传输（目前是两通道），多通道可以让UFS在成本，功耗和性能之间做取舍。 UFS是全双工工作模式，意味着读写可以并行。而eMMC是半双工，读写是不能同时进行的。 UFS支持命令队列，可以异步处理命令，而eMMC无命令队列，只能进行同步处理。 综合串行，多通道，全双工和异步的巨大优势，UFS流行也是大势所趋。 UFS Vs eMMC UFS（全双工+串行）vs eMMC（半双工+并行） NVMe简介 NVMe是专门为高速闪存芯片设计的协议，主要是为企业级和数据中心的PCIe SSD设计的接口标准，来充分发挥闪存的性能。NVMe通讯协议+PCIe总线协议是实现高速SSD性能的基础，为什么这种组合可以充分发挥SSD的性能呢？ 在NVMe之前，除了自成体系的SCSI协议（SAS SSD），其它SSD基本用的是AHCI+SATA协议。其实AHCI和SATA是为HDD服务的，而且SATA是由PATA进化而来，也是使用到了我们前面提到的高速串行的全双工传输。奈何SSD具有更低的延迟和更高的性能，SATA已经严重制约了SSD的速度，此时就需要PCIe了。 相比SATA/SAS，我们先看下PCIe到底有多快： 常见的4 lan的PCIe4.0 SSD，传输速度就可达7GB/s 那么，如果把SATA换成PCIe是不是就可以了，有NVMe什么事情呢？这就需要看SATA的难兄难弟AHCI了，如果不用NVMe，老旧的AHCI同样会严重制约SSD性能。下面的对比图可以看到AHCI与NVMe的差距了： 讲到这里，这些协议错综复杂，大家是不是已经云里雾里了。不着急，我们用下面这张图帮助大家理解他们的关系： 参照SAS SSD的协议栈，我们可以简单明了的看到，SATA和PCIe是物理接口和协议，AHCI和NVMe则是上层软件协议 UFS与NVMe比较 从上面的介绍可以看到，UFS拥有很好的性能，尤其是到了UFS4.0时代，2lan的顺序读可以达到4GB/s。但是，同时我们也可以看到，NVMe作为专为SSD所设计的协议，确实也有着无以伦比的性能，尤其是PCIe6.0，单lan就可以达到恐怖的8GB/s。 那么苹果为什么会采用NVMe而安卓还在继续使用UFS呢？孰优孰劣？从安卓各个厂家的角度来讲，为什么不采用NVMe呢？最关键的一点就是现在UFS的性能已经不比NVMe差了，根据下图，我们以iPhone 14 pro max上的NVMe速率来看，连续读取也只有1500M/s，已经比现在的UFS4.0速度差了很多，这也是安卓厂家能够继续使用UFS的最大驱动力。 所以顺序读写性能：UFS4.0 &gt; NVMe = UFS 3.0 &gt; UFS 2.1 &gt; eMMC 5.1 ahci_vs_nvme HelloWorld的执行过程 本节的目的是分析一个程序从代码到最终执行的全部流程，其中涉及到程序的生成，程序的加载，程序的执行和程序的退出流程。经过本节的内容我们将了解的一个程序的完整生命周期。本节的内容都将基于ARMv8a指令架构的Android操作系统进行分析。 程序的生成 一个可执行文件或者库的生成一般会经历以下几个阶段： 编写源码 预处理 编译 链接 生成可执行文件或库 本节将通过一个简单的程序，展示每个阶段的输出。前一个阶段的输出则是下一个阶段的输入。 主流C/C++编译工具集 现在主流的C/C++的编译器有：GCC, Clang/LLVM和MSVC GCC (GNU Compiler Collection) 概述: GCC 是一个支持多种编程语言的编译器套件，包括 C, C++, Objective-C, Fortran, Ada, 和 Go。它是开源的，并且在很多操作系统上可用，包括 Linux 和 Windows。 特点: 广泛支持多种编程语言。 支持多种平台和体系结构。 强大的优化功能。 支持 C++ 标准（如 C++11, C++14, C++17, C++20）。 Clang/LLVM 概述: Clang 是 LLVM 项目的一部分，提供了一个类似于 GCC 的前端，但其架构更加模块化，易于扩展和嵌入。 特点: 更快的编译速度。 更好的错误和警告信息。 模块化设计，易于扩展。 支持最新的 C++ 标准。 提供静态分析工具（如 Clang Static Analyzer）。 Clang 的组成部分 前端： Clang 的前端处理源代码的词法分析和语法分析。它将源代码转换为抽象语法树（AST），然后进行语义分析和优化。 中间表示（IR）： Clang 将 AST 转换为 LLVM 中间表示（IR）。LLVM IR 是一种底层的、中间的编程语言，用于在编译过程中进行优化和代码生成。 后端： Clang 使用 LLVM 的后端进行机器代码生成。LLVM 后端支持多种目标架构，如 x86、ARM 和 AArch64（ARM64）。 库和工具： Clang 提供了多个库和工具，包括： LibClang：一个 C 接口库，用于访问 Clang 的解析和编译功能。 Clang Static Analyzer：一个静态分析工具，用于在编译时检测潜在的错误。 Clang-Tidy：一个基于 Clang 的代码检查工具，用于发现和修复代码中的常见问题。 Clang-Format：一个代码格式化工具，用于统一代码风格。 MSVC (Microsoft Visual C++) 概述: MSVC 是微软提供的 C 和 C++ 编译器，主要用于 Windows 平台上的开发。它是 Visual Studio 开发环境的一部分。 特点: 与 Windows 操作系统和开发工具高度集成。 强大的 IDE 支持（Visual Studio）。 支持最新的 C++ 标准。 提供调试和分析工具。 Android和iOS都使用LLVM作为默认的编译工具集,所以本文将使用LLVM做为编译工具,并使用Android环境进行测试 源码 这个里我们随便写点c代码，主要看一下： 1. 函数是如何调用的; 2. 栈数据，堆数据和全局数据是如何访问的 3. 顺序，分支和循环代码对应的生成的机器指令生是什么样的; 示例代码（test_func.c）： 12345678910111213141516#include &lt;stdlib.h&gt;#define GLOBAL_DATA_SIZE 10000000int globalData[GLOBAL_DATA_SIZE];// 测试函数调用int RandomAccessTest(int times) &#123; // 访问全局数据 for (int i = 0; i &lt; times; i++) &#123; int randomNumber = rand()%GLOBAL_DATA_SIZE; globalData[randomNumber] = 1; &#125; return 1;&#125; 示例代码（test_main.c）： 12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;#include &lt;memory.h&gt;#define VISITED_GLOBAL_DATA_SIZE 50#define VISITED_DATA_SIZE 4extern int RandomAccessTest(int times);int main()&#123; //使用当前时间作为种子 srand(time(NULL)); // 访问栈上的数据 int statck2[VISITED_DATA_SIZE] = &#123;1,2,3,4&#125;; int statckTotal = 0; for (int i = 0; i &lt; VISITED_DATA_SIZE; i++) &#123; statckTotal += statck2[i]; &#125; // 访问堆数据 int heapDataSize = sizeof(int) * VISITED_DATA_SIZE; int* heap1 = malloc(heapDataSize); memset(heap1, 0, heapDataSize); *(heap1) = 1; *(heap1+1) = 2; int heapTotal = 0; for (int i = 0; i &lt; VISITED_DATA_SIZE; i++) &#123; heapTotal += heap1[i]; &#125; // 分支测试 if (heapTotal &gt;= 10)&#123; RandomAccessTest(VISITED_GLOBAL_DATA_SIZE); &#125; else &#123; printf(\"====HeapTotal=====:%d\", heapTotal); &#125;&#125; 代码编写完后，我们可以直接使用一下命令生成最终的可执行文件 1clang -target aarch64-linux-android21 test_func.c test_main.c -o test 为了更进一步的了解整个的生成过程，我们将使用不的参数来生成中间内容。 预处理 使用如下指令可以生成预处理的结果： 12clang -target aarch64-linux-android21 -E test_func.c -o test_func.iclang -target aarch64-linux-android21 -E test_main.c -o test_main.i 预处理的作用是为了准备源代码以便后续的词法分析（Lexical Analysis）和语法分析（Syntax Analysis）阶段。这些预处理步骤包括： 文件包含处理, 将\"#include\"包含文件插入进来（指插入需要的代码片段，不是整个文件内容贴过来）。 宏替换, 处理#define和#undef宏定义。 条件编译, 处理 #if、#ifdef、#ifndef、#elif 和 #endif 等预处理指令。 注释移除 空格处理 标识符处理, 处理和标识符相关的预处理指令，如 #pragma 和 #error 等，进行相应的处理或者报错 其他预处理指令处理, 处理其他的预处理指令，如 #line、#define、#undef 等，根据其定义执行相应的操作。 预处理结果（部分），如下： 1234567891011121314151617181920212223242526static __inline double strtod_l(const char* __s, char** __end_ptr, locale_t __l) &#123; return strtod(__s, __end_ptr);&#125;static __inline float strtof_l(const char* __s, char** __end_ptr, locale_t __l) &#123; return strtof(__s, __end_ptr);&#125;static __inline long strtol_l(const char* __s, char** __end_ptr, int __base, locale_t __l) &#123; return strtol(__s, __end_ptr, __base);&#125;# 277 \"D:\\\\AndroidSDK\\\\ndk\\\\21.3.6528147\\\\toolchains\\\\llvm\\\\prebuilt\\\\windows-x86_64\\\\bin/../sysroot/usr/include\\\\stdlib.h\" 2 3 4# 33 \"D:\\\\AndroidSDK\\\\ndk\\\\21.3.6528147\\\\toolchains\\\\llvm\\\\prebuilt\\\\windows-x86_64\\\\bin/../sysroot/usr/local/include\\\\stdlib.h\" 2 3# 2 \"test_func.c\" 2int globalData[10000000];int RandomAccessTest(int times) &#123; for (int i = 0; i &lt; times; i++) &#123; int randomNumber = rand()%10000000; globalData[randomNumber] = 1; &#125; return 1;&#125; 编译 编译的目的是生成一个目标文件，生成目标文件的过程大致是： 词法分析（Lexical Analysis）： 编译器首先将预处理后的源文件作为输入，进行词法分析。词法分析器会将源文件中的字符序列转换成一个个的词法单元（Token），每个词法单元代表源代码中的一个基本语法结构（如关键字、标识符、运算符等）。 语法分析（Syntax Analysis）： 词法分析器生成的词法单元序列将被传递给语法分析器。语法分析器根据语法规则检查这些词法单元序列的结构是否符合语言的语法规范。如果源代码符合语法规则，语法分析器将生成一个抽象语法树（Abstract Syntax Tree, AST）。 语义分析（Semantic Analysis）： 编译器接着进行语义分析，这一步骤确保程序语义上的正确性。语义分析器会检查类型、作用域、变量声明等语义信息，并生成中间代码或者直接生成汇编代码。 生成中间代码或汇编代码： 在语义分析阶段之后，编译器会根据语义分析得到的信息，生成中间代码或者直接生成目标平台的汇编代码。如果生成中间代码，后续可能会经过优化等步骤。如果直接生成汇编代码，那么此时就得到了汇编代码文件。 使用如下指令生成汇编结果： 12clang -target aarch64-linux-android21 -S test_func.i -o test_func.sclang -target aarch64-linux-android21 -S test_main.i -o test_main.s 汇编结果(test_func.s)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 .text .file &quot;test_func.c&quot; .globl RandomAccessTest &#x2F;&#x2F; -- Begin function RandomAccessTest .p2align 2 .type RandomAccessTest,@functionRandomAccessTest: &#x2F;&#x2F; @RandomAccessTest&#x2F;&#x2F; %bb.0: sub sp, sp, #32 &#x2F;&#x2F; &#x3D;32 stp x29, x30, [sp, #16] &#x2F;&#x2F; 16-byte Folded Spill add x29, sp, #16 &#x2F;&#x2F; &#x3D;16 stur w0, [x29, #-4] str wzr, [sp, #8].LBB0_1: &#x2F;&#x2F; &#x3D;&gt;This Inner Loop Header: Depth&#x3D;1 ldr w8, [sp, #8] ldur w9, [x29, #-4] cmp w8, w9 cset w8, ge tbnz w8, #0, .LBB0_4&#x2F;&#x2F; %bb.2: &#x2F;&#x2F; in Loop: Header&#x3D;BB0_1 Depth&#x3D;1 bl rand mov w8, #38528 movk w8, #152, lsl #16 sdiv w9, w0, w8 mul w8, w9, w8 subs w8, w0, w8 str w8, [sp, #4] ldrsw x10, [sp, #4] mov x11, #4 mul x10, x11, x10 adrp x11, globalData add x11, x11, :lo12:globalData add x10, x11, x10 mov w8, #1 str w8, [x10]&#x2F;&#x2F; %bb.3: &#x2F;&#x2F; in Loop: Header&#x3D;BB0_1 Depth&#x3D;1 ldr w8, [sp, #8] add w8, w8, #1 &#x2F;&#x2F; &#x3D;1 str w8, [sp, #8] b .LBB0_1.LBB0_4: mov w0, #1 ldp x29, x30, [sp, #16] &#x2F;&#x2F; 16-byte Folded Reload add sp, sp, #32 &#x2F;&#x2F; &#x3D;32 ret.Lfunc_end0: .size RandomAccessTest, .Lfunc_end0-RandomAccessTest &#x2F;&#x2F; -- End function .type globalData,@object &#x2F;&#x2F; @globalData .comm globalData,40000000,4 .ident &quot;Android (6454773 based on r365631c2) clang version 9.0.8 (https:&#x2F;&#x2F;android.googlesource.com&#x2F;toolchain&#x2F;llvm-project 98c855489587874b2a325e7a516b99d838599c6f) (based on LLVM 9.0.8svn)&quot; .section &quot;.note.GNU-stack&quot;,&quot;&quot;,@progbits 优化和目标代码生成（可选步骤）： 在一些编译器中，还会有优化器阶段，它会对中间代码或汇编代码进行优化，以提升程序的性能和效率。最终，优化后的中间代码或者汇编代码将会生成目标机器代码。 使用如下指令将生成汇编结果转换为最终的目标文件： 12clang -target aarch64-linux-android21 -c test_func.s -o test_func.oclang -target aarch64-linux-android21 -c test_main.s -o test_main.o 链接（生成最终可执行文件或库） 使用如下指令将目标文件链接成最终可执行的机器码文件： 1clang -target aarch64-linux-android21 test_func.o test_main.o -o test 目标文件，可执行文件或库格式 可执行文件或库文件格式是指计算机系统中用于存储和加载程序的特定文件格式。不同的操作系统和硬件架构可能支持不同的可执行文件格式。以下是一些常见的可执行文件格式： Linux 可执行文件格式： ELF (Executable and Linkable Format)：Linux 和许多其他类 Unix 操作系统上的主要可执行文件格式、共享库等。 ELF 文件的基本结构 ELF 文件的基本结构由三个部分组成： ELF Header：描述整个文件的组织结构, 包含文件类型、机器架构、入口地址等信息。 Program Header Table：描述程序的各个段（segment），仅在可执行文件和共享库中存在，描述了程序在内存中的映射，每个条目描述一个段，段包含可执行代码、数据等 Section Header Table：描述文件的各个节（section），用于链接和调试。 ELF文件我们在Linux内核源码中找到对应的实现，核心的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// ELF Headertypedef struct &#123; unsigned char e_ident[EI_NIDENT]; // ELF 标识, 前4个字节0x7F、e、l、f uint16_t e_type; // 文件类型 uint16_t e_machine; // 目标体系结构 uint32_t e_version; // 文件版本 Elf32_Addr e_entry; // 入口地址 Elf32_Off e_phoff; // 程序头表偏移量 Elf32_Off e_shoff; // 节头表偏移量 uint32_t e_flags; // 特定处理器标志 uint16_t e_ehsize; // ELF 头大小 uint16_t e_phentsize; // 程序头表项大小 uint16_t e_phnum; // 程序头表项数 uint16_t e_shentsize; // 节头表项大小 uint16_t e_shnum; // 节头表项数 uint16_t e_shstrndx; // 节头字符串表索引&#125; Elf32_Ehdr;// Program Headertypedef struct &#123; // 段的类型 // 0：PT_NULL, 无效段 // 1：PT_LOAD, 加载段此段包含需要加载到内存中的内容，如代码段和数据段 // 2: PT_DYNAMIC, 动态链接信息段，此段包含动态链接所需的信息，如动态符号表、重定位表等） // 3: PT_INTERP, 解释器段，此段包含一个字符串，指定解释器的路径（通常是动态链接器） // 4: PT_NOTE, Note 段，包含一些附加信息，如核心转储文件中的注释段。 // 5: PT_SHLIB, 保留，未使用。 // 6: PT_PHDR, 程序头表段，此段包含程序头表自身。 // 7: PT_TLS, 线程局部存储段，此段用于线程局部存储。 uint32_t p_type; // 段的类型 Elf32_Off p_offset; // 段在文件中的偏移 Elf32_Addr p_vaddr; // 段在内存中的虚拟地址 Elf32_Addr p_paddr; // 段在内存中的物理地址 uint32_t p_filesz; // 段在文件中的大小 uint32_t p_memsz; // 段在内存中的大小 uint32_t p_flags; // 段的标志, 用于标识段的权限 ( 可执行段：0x1, 可写段：0x2, 可读段：0x4) uint32_t p_align; // 段的对齐， 2^&#123;p_align&#125;&#125; Elf32_Phdr;// Section Headertypedef struct &#123; // 节类型 // SHT_NULL (0)：无效节，不使用。 // SHT_PROGBITS (1)： 程序数据段，包含程序代码和数据。 // SHT_SYMTAB (2)： 符号表段，包含符号表信息，通常用于链接。 // SHT_STRTAB (3)： 字符串表段，包含字符串表数据。 // SHT_RELA (4)： 重定位段（带显式添加端），包含重定位条目，每个条目包含一个附加的显式值。 // SHT_HASH (5)： 符号哈希表段，包含符号表的哈希表，用于快速查找符号。 // SHT_DYNAMIC (6)： 动态链接信息段，包含动态链接所需的信息。 // SHT_NOTE (7)： Note 段，包含附加信息。 // SHT_NOBITS (8)： 空段，不占用文件空间，但在内存中分配空间。BSS段 // SHT_REL (9)： 重定位段（不带显式添加端），包含重定位条目，不带显式值。 // SHT_SHLIB (10)： 保留段，未使用。 // SHT_DYNSYM (11)： 动态符号表段，包含动态链接的符号表。 // SHT_INIT_ARRAY (14)： 初始化函数数组段，包含指向初始化函数的指针数组。 // SHT_FINI_ARRAY (15)： 终结函数数组段，包含指向终结函数的指针数组。 // SHT_PREINIT_ARRAY (16)： 预初始化函数数组段，包含指向预初始化函数的指针数组。 // SHT_GROUP (17)： 节组段，包含多个节的分组信息。 // SHT_SYMTAB_SHNDX (18)： 符号表节索引段，包含符号表中的索引。 uint32_t sh_name; // 节的名字(在string表中的索引) uint32_t sh_type; // 节的类型 uint32_t sh_flags; // 节的标志（0x1：该节包含在进程执行过程中可写的数据，0x2： 该节在进程执行过程中分配了内存， 0x4：该节包含可执行的机器指令） Elf32_Addr sh_addr; // 节在内存中的虚拟地址 Elf32_Off sh_offset; // 节在文件中的偏移 uint32_t sh_size; // 节的大小 uint32_t sh_link; // 下一个节的链接索引 uint32_t sh_info; // 节的附加信息 uint32_t sh_addralign; // 节的对齐 uint32_t sh_entsize; // 节项的大小&#125; Elf32_Shdr; 我们也可以使用readelf来查看ELF文件的内容： ELF Header 123456789101112131415161718192021ELF Header: Magic: 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2's complement, little endian 小端 Version: 1 (current) OS/ABI: UNIX - System V ABI Version: 0x0 Type: DYN (Shared object file) Machine: AArch64 Version: 0x1 Entry point address: 0x176C Start of program headers: 64 (bytes into file) ELF Header后紧接着就是Program Header Start of section headers: 5608 (bytes into file) Section 开始位置 Flags: 0x0 Size of this header: 64 (bytes) ELF Header的大小 Size of program headers: 56 (bytes) Program Header大小 Number of program headers: 11 Program Header数量 Size of section headers: 64 (bytes) Section Header大小 Number of section headers: 27 Section Header的数量 Section header string table index: 25 Section Header名字在字符串表中的索引位置There are 27 section headers, starting at offset 0x15e8: Program Headers 截取片段 12345678910111213141516171819202122232425262728293031There are 11 program headers, starting at offset 64Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align PHDR 0x000040 0x0000000000000040 0x0000000000000040 0x000268 0x000268 R 0x8 INTERP 0x0002a8 0x00000000000002a8 0x00000000000002a8 0x000015 0x000015 R 0x1 [Requesting program interpreter: /system/bin/linker64] LOAD 0x000000 0x0000000000000000 0x0000000000000000 0x00076c 0x00076c R 0x1000 LOAD 0x00076c 0x000000000000176c 0x000000000000176c 0x0002f4 0x0002f4 R E 0x1000 LOAD 0x000a60 0x0000000000002a60 0x0000000000002a60 0x000278 0x000278 RW 0x1000 LOAD 0x000cd8 0x0000000000003cd8 0x0000000000003cd8 0x000000 0x2625a08 RW 0x1000 DYNAMIC 0x000a90 0x0000000000002a90 0x0000000000002a90 0x0001d0 0x0001d0 RW 0x8 GNU_RELRO 0x000a60 0x0000000000002a60 0x0000000000002a60 0x000278 0x0005a0 R 0x1 GNU_EH_FRAME 0x000678 0x0000000000000678 0x0000000000000678 0x000034 0x000034 R 0x4 GNU_STACK 0x000000 0x0000000000000000 0x0000000000000000 0x000000 0x000000 RW 0x0 NOTE 0x0002c0 0x00000000000002c0 0x00000000000002c0 0x000098 0x000098 R 0x4 Section to Segment mapping: Segment Sections... 00 01 .interp 02 .interp .note.android.ident .dynsym .gnu.version .gnu.version_r .gnu.hash .hash .dynstr .rela.dyn .rela.plt .rodata .eh_frame_hdr .eh_frame 03 .text .plt 04 .preinit_array .init_array .fini_array .dynamic .got .got.plt 05 .bss 06 .dynamic 07 .preinit_array .init_array .fini_array .dynamic .got .got.plt 08 .eh_frame_hdr 09 10 .note.android.ident None .comment .symtab .shstrtab .strtab Section Headers 截取片段 1234567891011121314151617181920212223242526272829303132333435There are 27 section headers, starting at offset 0x15e8:Section Headers: [Nr] Name Type Address Off Size ES Flg Lk Inf Al [ 0] NULL 0000000000000000 000000 000000 00 0 0 0 [ 1] .interp PROGBITS 00000000000002a8 0002a8 000015 00 A 0 0 1 [ 2] .note.android.ident NOTE 00000000000002c0 0002c0 000098 00 A 0 0 4 [ 3] .dynsym DYNSYM 0000000000000358 000358 0000d8 18 A 8 1 8 [ 4] .gnu.version VERSYM 0000000000000430 000430 000012 02 A 3 0 2 [ 5] .gnu.version_r VERNEED 0000000000000444 000444 000020 00 A 8 1 4 [ 6] .gnu.hash GNU_HASH 0000000000000468 000468 00001c 00 A 3 0 8 [ 7] .hash HASH 0000000000000484 000484 000050 04 A 3 0 4 [ 8] .dynstr STRTAB 00000000000004d4 0004d4 000055 00 A 0 0 1 [ 9] .rela.dyn RELA 0000000000000530 000530 000060 18 A 3 0 8 [10] .rela.plt RELA 0000000000000590 000590 0000c0 18 AI 3 21 8 [11] .rodata PROGBITS 0000000000000650 000650 000026 00 AMS 0 0 4 [12] .eh_frame_hdr PROGBITS 0000000000000678 000678 000034 00 A 0 0 4 [13] .eh_frame PROGBITS 00000000000006b0 0006b0 0000bc 00 A 0 0 8 [14] .text PROGBITS 000000000000176c 00076c 000250 00 AX 0 0 4 [15] .plt PROGBITS 00000000000019c0 0009c0 0000a0 00 AX 0 0 16 [16] .preinit_array PREINIT_ARRAY 0000000000002a60 000a60 000010 00 WA 0 0 8 [17] .init_array INIT_ARRAY 0000000000002a70 000a70 000010 00 WA 0 0 8 [18] .fini_array FINI_ARRAY 0000000000002a80 000a80 000010 00 WA 0 0 8 [19] .dynamic DYNAMIC 0000000000002a90 000a90 0001d0 10 WA 8 0 8 [20] .got PROGBITS 0000000000002c60 000c60 000020 00 WA 0 0 8 [21] .got.plt PROGBITS 0000000000002c80 000c80 000058 00 WA 0 0 8 [22] .bss NOBITS 0000000000003cd8 000cd8 2625a08 00 WA 0 0 8 [23] .comment PROGBITS 0000000000000000 000cd8 00016a 01 MS 0 0 1 [24] .symtab SYMTAB 0000000000000000 000e48 0004e0 18 26 37 8 [25] .shstrtab STRTAB 0000000000000000 001328 0000fe 00 0 0 1 [26] .strtab STRTAB 0000000000000000 001426 0001c0 00 0 0 1Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings), l (large) I (info), L (link order), G (group), T (TLS), E (exclude), x (unknown) O (extra OS processing required) o (OS specific), p (processor specific) ELF文件结构图： ELF文件结构图 注意：段（Segment）与节（Section）的区别。很多地方对两者有所混淆。段是程序执行的必要组成，当多个目标文件链接成一个可执行文件时，会将相同权限的节合并到一个段中。相比而言，节的粒度更小。 macOS 可执行文件格式： Mach-O (Mach Object)：macOS 和 iOS 系统上的可执行文件格式、共享库等。其格式和ELF原理上都是相通的，不做详细赘述。 Windows 可执行文件格式： Portable Executable (PE)：Windows 系统上最常见的可执行文件格式，包括程序、动态链接库 (DLL) 和驱动程序等。其格式和ELF原理上都是相通的，不做详细赘述。 本节，我们窥探了一下Linux目标文件的格式，下面我们将多个目标文件链接成一个可执行的文件。链接的方式主要有两种：静态链接和动态链接。 静态链接 静态链接的过程大致如下： 地址与空间分配（Address and Storage Allocation） 符号解析和重定位（Symbol Resolution and Relocation） 地址与空间分配 在ELF文件结构中，我们可以看到，是由各种节（Section）组成的，每个节都会有对应的文件位置，以及分配的虚拟地址空间。合并成一个后我们要就要分配对一个的文件空间和虚拟的地址空间。 合并 符号解析和重定位 将每个目标文件中的符号（函数，变量等）进行地址调整。那么链接器如何知道哪些指令是要被调整的呢？事实上，我们前面提到的ELF文件中的 重定位表（Relocation Table） 专门用来保存这些与重定位相关的信息。对于可重定位的ELF文件来说，它必须包含重定位表，用来描述如何修改相应的节的内容。对于每个要被重定位的ELF节都有一个对应的重定位表。如果.text节需要被重定位，则会有一个相对应叫.rel.text的节保存了代码节的重定位表；如果.data节需要被重定位，则会有一个相对应的.rel.tdata的节保存了数据节的重定位表。 动态链接 动态链接涉及运行时的链接以及多个文件的装载，必需要有操作系统的支持。因为动态链接的情况下，进程的虚拟地址空间的分布会比静态链接情况下更为复杂，还有一些存储管理、内存共享、进程线程等机制在动态链接下也会有一些微妙的变化。 目前，主流操作系统都支持动态链接。在Linux中，ELF动态链接文件被称为 动态共享对象（DSO，Dynamic Shared Objects），一般以.so为后缀；在Windows中，动态链接文件被称为 动态链接库（Dynamic Linking Library），一般以.dll为后缀。 在Linux中，常用的C语言库的运行库glibc，其动态链接形式的版本保留在 /lib目录下，文件名为 libc.so。整个系统只保留一份C语言动态链接文件libc.so，所有的C语言编写的、动态链接的程序都可以在运行时使用它。当程序被装载时，系统的动态链接器会将程序所需要的所有动态链接库装载到进程的地址空间，并将程序中所有未解析的符号绑定到相应的动态链接库中，并进行重定位。 程序的加载与执行 应用程序从启动到运行的过程涉及多个步骤，包括加载、准备和执行。以下是一个详细的过程描述： 用户请求： 用户通过命令行或图形界面（如双击图标）启动一个应用程序。这一操作向操作系统发出了一个启动应用程序的请求。 操作系统接收请求： 操作系统接收用户的请求，并开始启动应用程序的过程。具体步骤如下： 查找可执行文件： 操作系统在文件系统中查找应用程序的可执行文件。可执行文件通常包含程序的代码和数据。 分配内存： 操作系统为应用程序分配内存空间，包括代码段、数据段、堆、栈等。 代码段存储程序指令，数据段存储全局和静态变量，堆用于动态内存分配，栈用于函数调用和本地变量。 加载可执行文件： 操作系统将应用程序的可执行文件加载到分配的内存空间中。通常使用的是分页或分段机制，逐步将程序代码和数据加载到内存中。 设置进程控制块（PCB）： 操作系统创建一个新的进程控制块（Process Control Block，PCB），其中包含关于新进程的信息，如进程ID（PID）、进程状态、寄存器内容、内存指针等。 PCB用于管理进程并在进程切换时保存和恢复进程状态。 初始化进程环境： 操作系统为新进程初始化必要的环境，包括环境变量、文件描述符（如标准输入、输出、错误）等。 设置程序入口点（通常是main函数的地址）和初始堆栈指针。 加载动态链接库： 如果应用程序依赖于动态链接库（DLL或共享库），操作系统会加载这些库到内存中，并将它们链接到应用程序中。这个过程通常由动态链接器（如Linux上的ld.so）完成。 进入就绪队列： 新进程的PCB被加入到就绪队列中，等待调度器将其分配到CPU上执行。 调度程序调度： 操作系统的调度程序（Scheduler）选择一个就绪进程，并将其分配到CPU上执行。选择策略可能是先来先服务（FCFS）、短作业优先（SJF）、时间片轮转（RR）等。 切换到用户态： 调度程序将进程的上下文信息（如寄存器、程序计数器）加载到CPU，并切换到用户态，开始执行应用程序的代码。 进程从操作系统内核态切换到用户态，正式开始执行应用程序。 执行应用程序： 应用程序开始运行，其主函数被调用，并开始执行其中的指令。 在运行过程中，应用程序可能会进行系统调用，以请求操作系统执行特权操作（如文件读写、网络通信等）。 系统调用处理： 当应用程序进行系统调用时，CPU切换到内核态，操作系统处理请求，然后返回用户态继续执行应用程序。 系统调用的处理包括参数验证、权限检查、实际操作执行等。 进程终止： 当应用程序执行完成或者被用户终止时，它会调用系统调用（如exit）通知操作系统。 操作系统释放进程占用的资源（如内存、文件描述符等），更新进程状态，将PCB从进程表中移除。 操作系统可能会通知父进程（如果有），并可能启动清理或回收资源的操作。 分析工具 硬件参数和访问过程 了解硬件的参数和访问过程，有助于我们理解分析工具中的各种参数，以及他们所代表的意义。 闪存 闪存（Flash Memory）是一种非易失性存储器，广泛用于固态硬盘（SSD）、USB闪存驱动器、SD卡和嵌入式系统中。闪存的访问过程包括读、写和擦除操作。 闪存基本结构 1. 控制器：负责管理闪存的读写操作、地址映射、坏块管理等。 2. 存储单元：由NAND或NOR闪存芯片组成，存储实际的数据。 3. 接口：例如eMMC（Embedded MultiMediaCard）或UFS（Universal Flash Storage），负责与处理器通信。 闪存访问过程 读操作 1. 主机命令： 手机处理器（主机）发出读命令，指定逻辑块地址（LBA），并通过接口（如eMMC或UFS）发送给闪存控制器。 2. 命令解析： 闪存控制器接收并解析读命令和LBA，将逻辑地址（LBA）转换为物理地址（PBA）。 3. 加载数据： 闪存控制器从指定的物理地址读取数据，将数据从闪存芯片加载到控制器的内部数据缓冲区。这涉及读取存储单元中的电荷状态并将其转换为数字数据。 4. 错误校验： 闪存控制器使用错误纠正码（ECC）对数据进行校验和纠错，确保数据的完整性和正确性。 5.数据传输： 数据从闪存控制器的缓冲区通过接口（如eMMC或UFS）传输到主机。 6. 主机接收： 主机接收到数据后，直接将其写入到系统内存（RAM）中。这个过程由主机的内存控制器管理，确保数据在内存中的正确存储位置。 写操作 1. 主机命令： 主机发出写命令，指定LBA和要写入的数据。 2. 命令解析： 闪存控制器解析命令并将LBA转换为PBA。 3. 数据传输： 主机通过接口将数据传输到闪存控制器的内部缓冲区。 4. 写入： 闪存芯片将缓冲区的数据写入指定的存储单元。 5. 错误校验： 使用ECC校验写入的数据。 6. 状态返回： 控制器将操作结果返回给主机。 擦除操作 1. 主机命令： 主机发出擦除命令，指定要擦除的块。 2. 命令解析： 闪存控制器解析命令并将LBA转换为PBA。 3. 擦除操作： 闪存芯片将指定块中的所有存储单元设置为初始状态。 4. 确认擦除： 控制器确认擦除完成。 5. 状态返回： 控制器将操作结果返回给主机。 闪存参数 读写速度： 每秒钟读/写的数据量，通常以MB/s或GB/s为单位。 读写延迟： 执行读/写操作的时间，通常以毫秒（ms）为单位。 IOPS（每秒输入输出操作次数）： 每秒钟执行的I/O操作次数，用于衡量存储设备的性能。 队列深度： 同时处理的I/O操作数量，较高的队列深度可能导致更高的延迟。 I/O大小： 每个I/O操作的数据量，通常以字节（B）、千字节（KB）或兆字节（MB）为单位。 吞吐量： 单位时间内传输的数据量，通常以MB/s或GB/s为单位。 内存 内存结构 在计算机内存（尤其是DRAM，如SDRAM、DDR等）中，内存单元以矩阵的形式组织，每个单元存储一个比特的数据。这些数据单元（存储单元）被组织成一个二维矩阵。每个单元可以通过其行地址和列地址唯一标识。这个结构类似于一个电子表格，每个单元格可以通过其行和列来确定位置。 行地址（Row Address）：用于选择内存矩阵中的某一行。所有的存储单元都按行组织，因此首先需要选择包含目标单元的行。 列地址（Column Address）：用于选择特定行中某一列的地址。与行地址一起唯一标识内存单元的位置。 内存访问过程 内存访问是计算机系统中非常基础和关键的操作过程，它涉及多个步骤和多个组件之间的协同工作。以下是详细描述内存访问过程的步骤： 内存访问的详细步骤 1. 生成内存地址： CPU生成地址：处理器（CPU）根据当前执行的指令生成一个内存地址。这个地址可能是指令中的直接地址，或者是通过地址计算（如基地址加偏移量）生成的。 虚拟地址转换：如果使用虚拟内存，生成的地址是虚拟地址，需要通过地址转换机制转换成物理地址。 2. 地址转换： 页表查询：通过页表（Page Table）将虚拟地址转换为物理地址。页表存储在内存中，但为了加速转换过程，常用的页表项会缓存到转换后备缓冲（TLB，Translation Lookaside Buffer）中。 TLB查找：处理器首先查询TLB。如果找到匹配项，直接使用转换后的物理地址。如果没有找到，则需要访问页表进行转换，并可能更新TLB。 页表遍历：如果TLB没有命中，处理器会访问页表（可能涉及多级页表），获取物理地址。 3. 检查缓存： 一级缓存（L1 Cache）查找：处理器首先查找L1缓存，如果命中，则从L1缓存中读取或写入数据。 二级缓存（L2 Cache）查找：如果L1缓存未命中，继续查找L2缓存。 三级缓存（L3 Cache）查找：如果L2缓存未命中，继续查找L3缓存。 主内存访问： 如果所有缓存都未命中，则需要访问主内存（DRAM）。 4. 内存控制器操作： 行地址选通（RAS）：内存控制器发送行地址并激活行地址选通信号（RAS），选择目标行。 列地址选通（CAS）：内存控制器发送列地址并激活列地址选通信号（CAS），选择目标列。 数据准备：内存控制器等待内存阵列准备好数据。 5. 数据传输： 读取数据：如果是读操作，数据从内存传输到内存控制器，然后通过系统总线传输到处理器缓存或寄存器。 写入数据：如果是写操作，数据从处理器传输到内存控制器，然后写入指定的内存单元。 6. 缓存更新： 缓存写入：如果数据被写入缓存，需要相应地更新缓存内容（包括L1、L2、L3缓存）。 缓存一致性：保持缓存一致性（如通过MESI协议）以确保数据的一致性和正确性。 内存刷新（仅DRAM）： 周期性刷新：DRAM需要周期性刷新操作以保持数据。刷新操作由内存控制器管理，并在后台进行，不直接影响单次内存访问，但会占用内存带宽。 内存参数 1. 内存延迟（Memory Latency） 定义：内存延迟是从处理器发出内存访问请求到第一个字节的数据开始被返回所需要的时间。内存延迟通常用纳秒（ns）来表示。由行地址传输时间， 列地址传输时间， 数据准备时间组成，主要受内存的内部架构和访问机制影响。 意义：低内存延迟意味着处理器能够更快地访问内存数据，从而提高运行速度。 2. 内存访问时间（Memory Access Time） 定义：内存访问时间是从处理器发出内存访问请求到数据完全被处理器接收的总时间，包括内存延迟和数据传输时间。它通常也是用纳秒（ns）来表示。由内存延迟和数据传输时间组成，除了内存内部架构和访问机制外，还受到数据总线速度、数据块大小和传输方式的影响。 意义：较短的内存访问时间提高了处理器的执行效率。 3. 内存循环周期（Memory Cycle Time） 定义：从开始一次内存访问（例如一次读取操作）到内存准备好进行下一次(关闭行等)访问的总时间。这个时间包括访问内存单元、传输数据以及内存准备下一个访问所需的恢复时间。由内存访问时间和恢复时间(内存在完成一次访问后，需要一定时间恢复到可以进行下一次访问的状态)组成， 意义：较短的内存循环周期使得内存可以更频繁地进行访问操作，提高访问效率。 4. 内存访问频率（Memory Access Frequency） 定义：内存访问频率指的是内存可以进行读写操作的频率，通常用MHz或GHz来表示,不同类型的内存具有不同的访问频率。常见的DDR4：频率范围在1600MHz到3200MHz之间。 意义：较高的内存访问频率表示内存可以更频繁地进行读写操作，也意味着更高的传输速率和更高的带宽。 5. 内存带宽（Memory Bandwidth） 定义：内存带宽是指在一定时间内从内存传输到处理器的数据量，通常以GB/s（每秒千兆字节）为单位。 意义：高内存带宽意味着处理器能够更快地读取和写入数据，提高系统整体性能。带宽的计算公式如下： \\[{内存带宽（GB/s）}= {数据传输速率（MT/s）}\\times {总线宽度（字节）} \\times {通道数}\\] 例如，对于DDR4-3200内存，假设是单通道64位总线：内存带宽=3200MT/s×8B×1=25.6GB/s 6. 时钟周期（Clock Cycles） 定义：内存时钟信号的一个完整周期，从一个上升沿到下一个上升沿，或从一个下降沿到下一个下降沿。在一个时钟周期内，内存会执行部分操作或完成一个完整的操作步骤。 意义：较少的时钟周期表示更高效的内存访问。假设一个DDR4内存模块的时钟频率为3200MHz，其对应的时钟周期为： \\[时钟周期= \\frac{1}{时钟频率}\\] \\[时钟周期= \\frac{1}{3200 \\times 10^{6}Hz} ≈ 0.3125 ns\\] 这意味着在3200MHz的频率下，每个时钟周期的时间长度约为0.3125纳秒。 7. 数据传输率（Data Transfer Rate） - 定义：内存在一定时间内传输的数据量。它通常以每秒传输的百万次传输（MT/s）表示，并可以进一步换算成带宽（GB/s）。 - 意义：高数据传输率表示更快的数据移动速度，提高系统性能。数据传输率的计算: 8. 内存命中率（Memory Hit Rate） 定义：内存命中率是指缓存中的数据被请求到的比例。高命中率意味着大多数内存请求可以在缓存中找到。 意义：高内存命中率减少了对主内存的访问次数。 9. 页错误率（Page Fault Rate） - 定义：页错误率是指在虚拟内存系统中，处理器试图访问未加载到物理内存中的页面时发生的错误率。 - 意义：低页错误率意味着较少的磁盘访问。 10. 队列长度（Queue Length） - 定义：内存队列长度指的是在任何给定时间内等待处理的内存请求的数量。这些请求可能是由于处理器需要读取或写入数据到内存而产生的。队列长度是一个重要的性能指标，因为它可以反映系统的负载和效率。 - 意义：较短的队列长度表示更快的请求处理时间。表示内存子系统能够有效地处理内存请求，系统运行更平稳 11. 内存使用率（Memory Utilization） - 定义：内存使用率是指系统总内存的使用情况，通常以百分比表示。 - 意义：高内存使用率可能表示内存压力较大，需优化内存分配和使用策略。 CPU 中央处理器（CPU, Central Processing Unit）是计算机系统的核心组件，负责执行指令和处理数据。它被称为计算机的大脑，因为它执行所有的基本计算任务。 CPU的基本组成部分 1. 运算逻辑单元（ALU, Arithmetic Logic Unit）： 执行所有的算术和逻辑操作，如加、减、乘、除以及逻辑运算（如与、或、非）。 2. 控制单元（CU, Control Unit）： 负责从内存中取指令、解释指令并执行。它控制ALU、寄存器和其他组件的操作。 3. 寄存器（Registers）： CPU内部的高速存储单元，用于暂时存储指令、数据和地址信息。常见寄存器包括程序计数器（PC）、指令寄存器（IR）和累加器（ACC）。 4. 高速缓存（Cache）： 位于CPU和主存之间的高速存储器，用于存储频繁使用的数据和指令，减少访问主存的时间。 5. 总线接口单元（Bus Interface Unit）： 负责CPU与其他组件（如内存、输入/输出设备）之间的数据传输。 CPU执行代码过程 CPU执行代码的过程是一个复杂且高度优化的操作，涉及多个阶段和不同的硬件组件。以下是详细的执行过程，分为几个主要步骤： 1. 取指令（Fetch） 过程：从内存中读取下一条指令。 详细说明： 程序计数器（Program Counter, PC）：指向将要执行的下一条指令的内存地址。 取指单元（Fetch Unit）：从内存中读取指令，并将其放入指令寄存器（Instruction Register, IR）。 2. 解码（Decode） 过程：将取回的指令翻译成CPU能够理解的控制信号。 详细说明： 指令解码器（Instruction Decoder）：分析指令的操作码（Opcode）和操作数（Operands）。 生成控制信号：根据指令的类型，生成相应的控制信号，以驱动后续的执行单元。 3. 读取操作数（Operand Fetch） 过程：从寄存器或内存中读取指令所需的数据。 详细说明： 寄存器读取：如果操作数在寄存器中，直接从寄存器文件中读取。 内存读取：如果操作数在内存中，CPU会发出内存读取请求，将数据从内存中加载到寄存器中。 4. 执行（Execute） 过程：根据指令类型，进行相应的计算或操作。 详细说明： 算术逻辑单元（ALU, Arithmetic Logic Unit）：执行算术和逻辑运算。 浮点单元（FPU, Floating Point Unit）：执行浮点运算。 分支单元（Branch Unit）：处理跳转和分支指令。 特殊指令：如加载、存储、移位等操作。 5. 访问内存（Memory Access） 过程：对于需要访问内存的指令，执行读写操作。 详细说明： 加载指令：将数据从内存加载到寄存器。 存储指令：将数据从寄存器存储到内存。 6. 写回（Write Back） 过程：将执行结果写回寄存器或内存。 详细说明： 结果写回寄存器：执行结果写回到目标寄存器。 结果写回内存：在必要时，将结果写回到内存中。 7. 更新程序计数器（Update PC） 过程：更新程序计数器，以指向下一条指令的地址。 详细说明： 顺序执行：PC通常递增以指向下一条顺序指令。 跳转和分支：如果是跳转或分支指令，PC会更新为目标地址。 CPU执行指令的具体硬件组件 寄存器（Registers）：用于存储临时数据和指令。 缓存（Cache）：加速数据访问，减少对内存的访问延迟。 控制单元（Control Unit）：生成控制信号，协调各个部分的工作。 流水线（Pipeline）：分解指令执行过程，允许多个指令同时在不同阶段执行，提高并行度和吞吐量。 分支预测（Branch Prediction）：预测分支指令的执行路径，减少流水线中断。 流水线执行过程 现代CPU通常采用流水线技术，将指令执行过程分解为多个阶段，允许多个指令同时在不同阶段执行。典型的流水线阶段包括： 取指（Fetch） 解码（Decode） 执行（Execute） 访存（Memory Access） 写回（Write Back） 超标量和超线程技术 超标量（Superscalar）：同时执行多条指令，通过多个执行单元实现。 超线程（Hyper-Threading）：在一个物理核心上同时运行多个线程，提高并行处理能力。 执行流程的优化技术 分支预测（Branch Prediction）：减少分支指令导致的流水线中断。 动态调度（Dynamic Scheduling）：根据资源可用性和指令依赖关系，动态调整指令执行顺序。 投机执行（Speculative Execution）：在确认分支路径前，提前执行可能的指令路径。 CPU参数 1.时钟速度（Clock Speed） 定义：CPU的时钟频率，通常以千兆赫兹（GHz）表示。 作用：时钟速度直接影响CPU每秒钟可以执行的指令数。更高的时钟速度通常意味着更快的处理速度，但也需要考虑功耗和散热。 2. 指令每周期（IPC, Instructions Per Cycle） 定义：CPU每个时钟周期内可以执行的指令数。 作用：高IPC表示CPU在相同时钟速度下能够完成更多工作，反映了CPU架构的效率。 3. 核心数量（Number of Cores） 定义：CPU内部的独立处理单元数量。 作用：多核CPU能够并行处理多个任务，有助于提升多任务处理和多线程应用的性能。 4. 线程数量（Number of Threads） 定义：CPU可以同时处理的线程数。 作用：支持超线程技术（如Intel的Hyper-Threading）可以进一步提高并行处理能力，尤其是在多线程应用中。 5. 缓存大小（Cache Size） 定义：CPU内部的高速缓存容量，包括L1、L2和L3缓存。 作用：较大的缓存可以减少内存访问延迟，提高数据访问速度，从而提升整体性能。 6. 内存带宽（Memory Bandwidth） 定义：CPU与系统内存之间的数据传输速率，通常以GB/s表示。 作用：更高的内存带宽可以加快数据传输，减少内存瓶颈，尤其对数据密集型应用有重要影响。 7. 内存延迟（Memory Latency） 定义：CPU从内存请求数据到接收到数据所需的时间。 作用：较低的内存延迟可以减少等待时间，提高整体系统响应速度。 8. 分支预测准确率（Branch Prediction Accuracy） 定义：CPU预测程序中分支指令（如条件跳转）的准确率。 作用：高分支预测准确率可以减少流水线冲刷，提升指令执行效率。 9. 专用加速器（Dedicated Accelerators） 定义：CPU中集成的专用硬件单元，如图形处理单元（GPU）、神经处理单元（NPU）等。 作用：专用加速器能够显著提升特定任务的性能，如图形渲染、AI计算等。 10. 功耗（Power Consumption） 定义：CPU运行时的电能消耗，通常以瓦特（W）为单位。 作用：较低的功耗可以延长电池续航时间，减少散热需求，但可能会限制性能。 11. 热设计功耗（TDP, Thermal Design Power） 定义：CPU在高负载下的最大功耗，通常以瓦特（W）表示。 作用：TDP越高，通常意味着CPU在高负载下可以保持更高的性能，但也需要更好的散热解决方案。 12. 系统总线速度（System Bus Speed） 定义：CPU与其他系统组件（如内存、I/O设备）之间的数据传输速率。 作用：更高的总线速度可以提高数据传输效率，减少瓶颈。 13. 上下文切换时间（Context Switch Time） 定义：CPU在不同任务之间切换时所需的时间。 作用：较短的上下文切换时间可以提高多任务处理效率。 14. 平均负载（Average Load） 定义：CPU在一定时间内的平均工作负载。 作用：平均负载可以反映系统在日常使用中的性能表现，过高的平均负载可能表示系统瓶颈。 15. 使用率（Utilization） 定义：CPU在特定时间段内的使用百分比。 作用：高使用率通常表示CPU处于高负载状态，但持续的高使用率可能导致过热和性能下降。 16. 吞吐量（Throughput） 定义：CPU在单位时间内可以处理的任务或数据量。 作用：较高的吞吐量表示CPU能够高效处理大量任务，提高整体系统性能。 17. 延迟（Latency） 定义：任务从发出到被处理的时间延迟。 作用：较低的延迟表示系统响应速度快，对于实时应用尤为重要。 网络 计算机网络是由多个部分组成的复杂系统，这些部分共同工作以实现数据传输、资源共享和通信功能。 网络组成 1. 网络设备 a. 终端设备 计算机：包括桌面计算机、笔记本电脑、服务器等，作为网络中的数据源和数据接收者。 移动设备：如智能手机、平板电脑等，可以通过无线连接加入网络。 其他设备：如打印机、IP电话、摄像头等，能够通过网络提供各种服务。 b. 中间设备 路由器（Router）：用于连接不同网络，负责数据包的转发和路由选择。 交换机（Switch）：在局域网（LAN）中用于连接多个设备，基于MAC地址进行数据帧转发。 集线器（Hub）：一种早期的网络设备，广播接收到的数据帧到所有端口（已逐步被交换机取代）。 网关（Gateway）：连接不同网络协议的设备，充当协议转换器。 防火墙（Firewall）：用于监控和控制进出网络的数据流，提供安全防护。 2. 网络介质 a. 有线介质 双绞线（Twisted Pair Cable）：常用于以太网连接，有UTP和STP两种类型。 同轴电缆（Coaxial Cable）：用于有线电视和早期的以太网连接。 光纤电缆（Fiber Optic Cable）：通过光信号传输数据，具有高带宽和长传输距离。 b. 无线介质 无线电波（Radio Waves）：用于Wi-Fi、蓝牙等无线通信。 微波（Microwaves）：用于远程无线通信，如卫星通信。 红外线（Infrared）：用于短距离无线通信，如遥控器。 3. 网络协议 a. 应用层协议 HTTP/HTTPS：用于Web服务的超文本传输协议。 FTP：文件传输协议，用于文件上传和下载。 SMTP/IMAP/POP3：用于电子邮件传输的协议。 DNS：域名系统，用于域名解析。 b. 传输层协议 TCP：传输控制协议，提供可靠的、面向连接的通信。 UDP：用户数据报协议，提供无连接的、不可靠的通信。 c. 网络层协议 IP：互联网协议，负责数据包的寻址和路由选择。 ICMP：互联网控制报文协议，用于诊断网络连接。 d. 数据链路层协议 以太网（Ethernet）：局域网中常用的链路层协议。 PPP：点对点协议，用于拨号连接。 4. 网络架构 a. 拓扑结构 星型拓扑：所有设备连接到中央交换机或集线器。 总线型拓扑：所有设备共享一条通信介质。 环型拓扑：设备连接成一个环，数据沿环传输。 网状拓扑：设备之间相互连接，提供多条路径。 b. 网络类型 局域网（LAN）：覆盖小范围区域，如办公室或家庭。 广域网（WAN）：覆盖大范围区域，如城市或国家。 城域网（MAN）：覆盖中等范围区域，如城市。 个人区域网（PAN）：覆盖个人范围，如蓝牙设备之间的连接。 网络通信过程 一个数据包是如何从客户端到达服务器的过程是一个复杂的、多层次的过程，涉及多个网络协议和设备。以下是一个详细的步骤描述： 1. 应用层 客户端生成请求： 用户在客户端应用（例如浏览器）中输入一个URL并按下回车键。 应用程序生成一个HTTP请求消息，并将其传递到传输层。 2. 传输层 封装为TCP/UDP段： 传输层（通常是TCP或UDP协议）接收应用层数据，将其封装成一个TCP段或UDP数据报。 如果使用TCP协议： TCP段头部包含源端口、目标端口、序列号、确认号、窗口大小等信息。 TCP连接通过三次握手建立。 如果使用UDP协议： UDP数据报头部包含源端口、目标端口、长度和校验和等信息。 3. 网络层 封装为IP数据包： 网络层（IP协议）接收传输层段或数据报，将其封装成一个IP数据包。 IP包头包含源IP地址、目标IP地址、TTL（生存时间）等信息。 路由选择算法确定数据包的最佳路径。 4. 数据链路层 封装为帧： 数据链路层接收IP数据包，将其封装成一个数据帧。 帧头部包含源MAC地址、目标MAC地址和帧校验序列（FCS）。 数据帧通过物理层传输到下一跳。 5. 物理层 传输数据： 物理层将数据帧转换为电信号或光信号，通过传输介质（如以太网、光纤或无线电波）发送到下一跳设备（如交换机或路由器）。 6. 交换机 局域网传输： 如果客户端和服务器在同一局域网（LAN）内，交换机会根据目标MAC地址将数据帧转发到目标服务器。 如果不在同一局域网内，数据帧会被转发到网关路由器。 7. 路由器 广域网传输： 路由器接收到数据帧，将其解封装为IP数据包。 路由器使用路由表查找目标IP地址的最佳路径，并将IP数据包封装成新的数据帧，发送到下一跳路由器。 此过程在多个路由器间进行，直至数据包到达目标局域网。 8. 目标局域网 局域网传输： 数据包进入目标局域网后，交换机根据目标MAC地址将数据帧转发到目标服务器。 9. 目标服务器 数据解封装： 服务器网络接口卡（NIC）接收数据帧，数据链路层解封装为IP数据包。 网络层将IP数据包解封装为TCP段或UDP数据报。 传输层根据目标端口号将数据传递到相应的应用程序。 10. 应用层 服务器处理请求： 服务器上的应用程序（例如Web服务器）接收并处理HTTP请求，生成响应消息。 响应消息经过相同的封装和传输过程返回给客户端。 网络参数 1. 吞吐量（Throughput） 定义：单位时间内成功传输的数据量，通常以比特每秒（bps）、千比特每秒（kbps）、兆比特每秒（Mbps）或千兆比特每秒（Gbps）表示。 影响因素：网络带宽、传输协议、网络拥塞等。 衡量方法：使用网络性能测试工具（如iPerf、Speedtest）进行测量。 2. 延迟（Latency） 定义：数据从源到目的地所需的时间，通常以毫秒（ms）表示。 影响因素：网络路径的长度和复杂性、路由器和交换机的处理时间、网络拥塞等。 衡量方法：使用ping命令测量往返时间（RTT）。 3. 抖动（Jitter） 定义：数据包之间传输延迟的变化量，通常以毫秒（ms）表示。 影响因素：网络拥塞、路由器和交换机的处理时间波动等。 衡量方法：使用工具（如ping、iPerf）测量连续数据包的延迟变化。 4. 数据包丢失率（Packet Loss Rate） 定义：传输过程中丢失的数据包比例，通常以百分比表示。 影响因素：网络拥塞、链路错误、硬件故障等。 衡量方法：使用ping或其他网络诊断工具进行数据包传输测试。 5. 连接建立时间（Connection Establishment Time） 定义：从发起连接请求到连接成功建立所需的时间，通常以毫秒（ms）表示。 影响因素：服务器响应时间、网络延迟等。 衡量方法：使用网络监控工具（如Wireshark）分析连接建立过程。 6. 响应时间（Response Time） 定义：客户端发出请求到收到服务器响应的时间，通常以毫秒（ms）表示。 影响因素：服务器处理能力、网络延迟、网络拥塞等。 衡量方法：使用Web性能测试工具（如Pingdom、GTmetrix）进行测量。 7. 并发连接数（Concurrent Connections） 定义：在同一时间段内网络上可以同时处理的连接数量。 影响因素：服务器的处理能力、网络带宽、协议栈实现等。 衡量方法：使用负载测试工具（如Apache JMeter）进行测量。 8. 带宽利用率（Bandwidth Utilization） 定义：实际使用的带宽与可用带宽的比例，通常以百分比表示。 影响因素：网络应用的传输需求、网络的总带宽容量等。 衡量方法：使用网络监控工具（如NetFlow、SNMP）进行测量。 9. 会话维持时间（Session Duration） 定义：一个会话从开始到结束的时间长度，通常以秒或分钟表示。 影响因素：应用特性、用户行为、网络稳定性等。 衡量方法：使用应用监控工具（如Google Analytics、New Relic）进行测量。 10. 错误率（Error Rate） 定义：传输过程中发生错误的比例，通常以百分比表示。 影响因素：网络噪声、链路错误、硬件故障等。 衡量方法：使用网络诊断工具（如Wireshark、ping）分析数据传输错误。 11. 流量分析（Traffic Analysis） 定义：分析网络上传输的数据流量，以便了解网络使用情况和发现潜在问题。 影响因素：网络应用的类型、用户行为、网络架构等。 衡量方法：使用网络流量分析工具（如Wireshark、NetFlow Analyzer）进行分析。 12. 服务质量（Quality of Service, QoS） - 定义：衡量网络服务在传输过程中对不同类型流量的优先级管理和性能保证。 - 影响因素：网络配置、QoS策略等。 - 衡量方法：使用网络监控和管理工具（如Cisco QoS）进行测量和管理。 性能分析工具 在Unity开发过程中，性能分析是确保游戏运行流畅和优化资源使用的重要步骤。市面上有很多分析工具，它们可以帮助我们识别和解决性能瓶颈。本节的主要目的是熟悉各种分析工具以及相关参数。 Unity Profiler Unity Profiler是Unity自带的性能分析工具，功能强大，支持扩展可自定义统计数据。 Unity Profiler界面概述 主界面 A:Profiler的模块窗口，包括CPU,GPU,Rendering,Memory,自定义等模块 B:工具栏，从左往右依次是：连接目标，数据记录开关，帧控制，清理，播放时清理开关，深度捕获开关，调用栈，打开profile数据，保存数据按钮，帮助按钮，设置按钮 深度捕获开关：开启后Unity将在每个函数（除Native函数）中添加捕获标记。 调用栈:可以在不开启深度捕获的情况下，捕获每个函数的内存分配 C:帧图表区域，此区域显示每个模块捕获的性能数据图表。 D:模块详细信息，选择一个模块后，此区域显示模块的详细数据，每个模块显示的内容不一样。 资源模块 资源模块主要包含两个：文件访问分析模块（File Access Profiler module）和资源加载分析模块（Asset Loading Profiler module），如下图： 文件访问分析模块 此模块显示有关应用程序中文件活动的信息，例如 Unity 执行的读写操作数或打开的文件句柄数（针对特定帧或捕获的所有帧）。您可以使用此信息来帮助确定应用程序执行文件操作的效率。此模块可以捕获有关您构建的应用程序文件夹结构中任何文件的文件操作的信息，或者如果您在 Unity 编辑器中运行 Profiler，则可以捕获 Unity 项目文件夹中任何文件的文件操作的信息。 文件访问分析模块 Files Opened: 此帧期间成功打开的文件总数。 Files Closed：此帧期间成功关闭的文件总数。 File Seeks：此帧期间修改文件指针位置的次数。 Reads in Flight：此帧期间正在进行的读取操作总数。 File Handles Open：此帧期间任何时候保持打开状态的文件句柄总数。这包括Unity在同一帧内打开和关闭的文件。 资源加载分析模块 此模块显示有关应用程序如何加载资源的信息，包括按区域细分的读取操作。详细信息窗口提供了对在分析期间捕获的每个资源加载标记的深入了解。您可以使用此信息来了解应用程序加载资源的效率，并确定任何特定问题。 资源加载分析模块 Texture Reads: 从AsyncReadManager请求加载纹理字节数。 Virtual Texture Reads: 从AsyncReadManager请求加载虚拟纹理字节数。 Mesh Reads：从AsyncReadManager请求加载Mesh字节数。 Audio Reads：从AsyncReadManager请求音频字节数。 Scripting Reads：通过脚本API从AsyncReadManager请求的字节数。 Entities Reads: Entities包中的脚本，从AsyncReadManager请求的字节数。 Other Reads: 除开上面的这些分类外，向AsyncReadManager请求的字节数。 为了更好的理解文件的相关操作，我们通过一个简单的程序来分析文件的打开，定位，读写和关闭的操作。 操作系统如何处理文件 先写一个简单的文件操作程序，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main() &#123; // 打开文件以写入模式 FILE *file = fopen(\"example.txt\", \"w\"); if (file == NULL) &#123; perror(\"无法打开文件\"); return 1; &#125; // 写入一些文本到文件 fprintf(file, \"Hello, World!\\n\"); fputs(\"This is a test file.\\n\", file); // 关闭文件 fclose(file); // 重新打开文件以读取模式 file = fopen(\"example.txt\", \"r\"); if (file == NULL) &#123; perror(\"无法打开文件\"); return 1; &#125; // 定位到第二个字符开始读 if (fseek(file, 2, SEEK_SET) != 0) &#123; perror(\"fseek 失败\"); fclose(file); return 1; &#125; // 读取并打印文件内容 char buffer[256]; while (fgets(buffer, sizeof(buffer), file) != NULL) &#123; printf(\"%s\", buffer); &#125; // 关闭文件 fclose(file); return EXIT_SUCCESS;&#125; 进程结构 我们通过Linux源码来看一下，一个进程的大致结构，这个结构体非常大，我们看几个核心的字段： 123456789101112131415161718192021222324//进程的结构体struct task_struct &#123; struct thread_info thread_info; // 线程信息 unsigned int __state; // 进程的状态 pid_t pid; // 进程ID pid_t tgid; // 线程组ID struct timespec start_time; // 进程启动时间 struct timespec real_start_time; // 真实启动时间 struct mm_struct *mm; // 进程的内存描述符，进程的内存布局由 struct mm_struct 管理，包含虚拟内存区域和页表 struct fs_struct *fs; // 文件系统信息 struct files_struct *files; // 打开的文件信息&#125;;// 文件系统信息struct fs_struct &#123; struct path root; // 根目录 struct path pwd; // 当前工作目录&#125;;// 文件描述符表struct files_struct &#123; struct fdtable __rcu *fdt; // 文件描述符表 struct fdtable fdtab; // 文件描述符&#125;; 从进程的结构体中，我们可以发现 struct files_struct *files，中存储了打开的文件信息。接下来我们去窥探一下文件的打开过程。 打开过程 在Linux内核中，open系统调用的核心代码位于fs/open.c文件中。open函数是用户空间程序用来打开文件的系统调用。以下是核心函数的详细说明： 系统调用入口 (sys_open) 概述： sys_open 是 open 系统调用的入口函数。它的作用是处理来自用户空间的 open 系统调用请求。 代码： 1234SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)&#123; return do_sys_open(AT_FDCWD, filename, flags, mode);&#125; - SYSCALL_DEFINE3 宏定义了 open 系统调用，接收三个参数：文件名、标志和模式。 - do_sys_open 函数是核心处理函数，处理具体的文件打开操作。 核心处理函数 (do_sys_open) 概述： do_sys_open 是核心处理函数，负责执行实际的文件打开操作。它处理文件路径解析、权限检查、文件描述符分配等。 代码： 123456789long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode)&#123; struct open_flags op; int fd = build_open_flags(flags, mode, &amp;op); if (fd) return fd; return do_filp_open(dfd, filename, &amp;op);&#125; build_open_flags：解析和验证传入的标志和模式，构建 open_flags 结构体。 do_filp_open：执行具体的文件打开操作，并返回文件指针。 构建打开标志 (build_open_flags) 概述： build_open_flags 函数解析并构建文件打开标志和模式。 代码： 12345int build_open_flags(int flags, umode_t mode, struct open_flags *op)&#123; // ... 解析标志和模式，进行权限检查等 ... return 0;&#125; 解析传入的 flags 和 mode 参数，并进行权限检查。 文件打开操作 (do_filp_open) 概述： do_filp_open 函数处理文件路径解析、权限检查，并最终打开文件。 代码： 12345678910111213141516struct file *do_filp_open(int dfd, const char *pathname, const struct open_flags *op)&#123; struct nameidata nd; struct file *filp; // 初始化 nameidata 结构体 set_nameidata(&amp;nd, dfd, pathname); // 解析路径名 filp = path_openat(&amp;nd, op, 0); if (!IS_ERR(filp)) return filp; return ERR_PTR(-ENOENT);&#125; set_nameidata：初始化 nameidata 结构体，用于路径解析。 path_openat：解析路径并执行文件打开操作。 路径解析和打开 (path_openat) 概述： path_openat 函数解析文件路径并执行具体的打开操作。 代码： 12345678910struct file *path_openat(struct nameidata *nd, const struct open_flags *op, unsigned flags)&#123; struct file *file; // 解析路径 nd-&gt;flags = op-&gt;lookup_flags; file = do_last(nd, file, op); return file;&#125; do_last：执行路径解析的最后一步，并打开文件。 最后一步解析和打开 (do_last) 概述： do_last 函数完成路径解析的最后一步，并执行文件打开操作。 代码： 12345678910111213141516171819202122232425static struct file *do_last(struct nameidata *nd, struct file *file, const struct open_flags *op)&#123; struct dentry *dentry; struct inode *dir; // 获取目录 inode dir = nd-&gt;path.dentry-&gt;d_inode; // 创建或打开文件 dentry = lookup_open(nd, op, dir); if (IS_ERR(dentry)) return ERR_CAST(dentry); // 分配并初始化 file 结构体 file = alloc_empty_file(op-&gt;open_flag, current_cred()); if (IS_ERR(file)) return file; file-&gt;f_path = nd-&gt;path; file-&gt;f_inode = dentry-&gt;d_inode; file-&gt;f_op = fops_get(dentry-&gt;d_inode-&gt;i_fop); return file;&#125; - lookup_open：在目录中查找或创建文件。 - alloc_empty_file：分配并初始化 file 结构体。 - 初始化 file 结构体的各个字段，包括路径、inode 和文件操作指针。 通过上述代码和解释，可以看到 open 系统调用在 Linux 内核中经过了多个步骤和函数调用，包括路径解析、权限检查、文件描述符分配等。这些步骤协同工作，确保文件能够正确地被打开，并为进程提供所需的文件访问功能, 打开的文件会放在进程的struct files_struct *files字段中。接下来我们再看一下文件指针的移动。 定位 定位的过程就是一个移动指针的过程，我们可以看一下struct file 结构体，它表示一个打开的文件，包含文件的状态和操作函数等信息。 12345678910struct file &#123; struct path f_path; // 文件路径信息 struct inode *f_inode; //结构体用于表示文件系统中每个文件或目录的索引节点。 const struct file_operations *f_op; // 文件操作函数指针 atomic_long_t f_count; //引用计数 unsigned int f_flags; // 文件标志 fmode_t f_mode; // 文件模式（读、写等） loff_t f_pos; // 文件指针当前的读写位置 //...&#125;; 定义的大致流程如下： 用户空间流程： fseek 函数：用户程序调用 fseek 函数。 调用 lseek：fseek 调用 lseek 系统调用。 内核空间流程： 系统调用入口 (sys_lseek)：处理文件描述符，调用 vfs_llseek。 虚拟文件系统层 (vfs_llseek)：调用具体文件系统的 llseek 实现或默认 llseek 实现。 默认 llseek 实现 (default_llseek)：计算并更新文件指针。 通过上述过程，fseek 函数实现了对文件指针的调整，从而支持文件的随机访问。内核中处理文件偏移调整的核心代码确保了这一过程的正确性和高效性。 读 在Linux内核中，读取的文件内容通过多个层次的缓存机制存储和管理，主要涉及以下几个关键组件和数据结构： 页缓存（Page Cache） 地址空间（Address Space） 缓冲区头（Buffer Head） 页缓存（Page Cache） 页缓存是Linux内核用于缓存文件数据的主要机制。当文件被读取时，文件的内容首先会被缓存到页缓存中。页缓存是由多个内存页（通常是4KB）组成的。 相关数据结构： struct page：表示一个物理内存页。 struct address_space：表示文件或设备的地址空间。 页缓存读取过程： 当用户进程通过系统调用（如 read）请求读取文件时，内核首先检查页缓存中是否存在请求的数据。 如果数据在页缓存中，则直接从页缓存返回数据。 如果数据不在页缓存中，则从磁盘读取数据到页缓存中，然后返回给用户进程。 地址空间（Address Space） 每个文件或设备都有一个地址空间（address_space），用于管理该文件或设备的页缓存。 struct address_space 结构体： 123456789101112struct address_space &#123; struct inode *host; /* 关联的 inode */ struct radix_tree_root page_tree; /* 用于存储页缓存的树结构 */ spinlock_t tree_lock; /* 保护 page_tree 的自旋锁 */ unsigned int i_mmap_writable; /* 可写映射计数 */ struct rb_root i_mmap; /* 区域映射的红黑树 */ struct list_head i_mmap_nonlinear; atomic_t truncate_count; /* 文件截断计数 */ unsigned long nrpages; /* 页缓存中的页数 */ pgoff_t writeback_index; /* 写回索引 */ // 其他字段...&#125;; 缓冲区头（Buffer Head） 在较老的文件系统中（如 ext2），缓冲区头（buffer_head）结构体用于管理磁盘块和页缓存之间的关系。现代文件系统（如 ext4）更多地依赖直接的页缓存管理。 struct buffer_head 结构体： 12345678struct buffer_head &#123; struct buffer_head *b_next; /* 缓冲区链表 */ unsigned long b_blocknr; /* 逻辑块号 */ unsigned short b_size; /* 缓冲区大小 */ char *b_data; /* 指向缓冲区数据的指针 */ struct block_device *b_bdev; /* 关联的块设备 */ // 其他字段...&#125;; 从文件读取内容的典型流程，包括页缓存的使用： 系统调用接口：用户进程调用 read 系统调用请求读取文件。 文件系统层：文件系统的 read 方法处理读取请求。 页缓存检查：文件系统检查页缓存中是否已有请求的数据。 如果数据在页缓存中，则直接返回数据。 如果数据不在页缓存中，则从磁盘读取数据到页缓存，然后返回给用户进程。 返回用户空间：读取的数据被拷贝到用户进程的缓冲区中。 关闭 关闭文件时，内核执行的关键步骤包括： 获取并释放文件描述符：从文件描述符表中删除对应的项。 减少引用计数：调用 fput 减少文件指针的引用计数。 最终清理：如果引用计数为0，调用 __fput 进行文件指针的最终清理，包括刷新数据、通知文件系统、清除 inode、释放路径、调用文件系统的 release 方法，最终释放文件结构。 内存模块 在Unity中，有两种方法可以分析应用程序的内存使用情况： 内存分析器模块：内置的分析器模块，提供应用程序使用内存的基本信息。 内存分析器包：是一个可以添加到项目中的Unity包。它会向Unity编辑器添加一个额外的内存分析器窗口，然后我们可以使用它更详细地分析应用程序中的内存使用情况。也可以存储和比较快照以查找内存泄漏，或者查看内存布局以查找内存碎片问题。 内存分析器模块会直观显示应用中分配的总内存量的计数器。可以使用内存模块查看已加载对象的数量以及每个类别中它们总共占用的内存量等信息。还可以查看每帧的GC分配数量。 在编辑器模式下的内存分析可能和实际的运行在目标平台上捕获到的内存存在差异，这主要是因为，当我们在编辑器中分析应用程序时，内存分析器模块报告的内存使用量比在目标设备上构建的应用程序的类似分析结果更高。这是因为Unity编辑器使用特定对象占用额外的内存，以及编辑器窗口本身使用额外的内存。 额外内存的主要原因是Unity将对象（如纹理）视为编辑器中启用的读/写功能，并在CPU上保留每个纹理的额外副本。这实际上使编辑器中报告的纹理内存使用量翻倍；为了更准确地了解纹理的内存使用情况，需要在目标平台上运行的应用程序的构建版本进行分析。 此外，由于Unity也无法将Profiler本身占用的内存与播放模式的内存完全分开，因此Profiler使用的内存会显示在Profiler窗口中。 内存分析器模块 内存分析器模块 内存分析器包 内存分析器包-概述 内存分布概述 内存分析器包-Unity对象 详细的Unity的对象 内存分析器包-所有内存 所有的内存对象 内存分析器包 可以对两次的内存快照进行比较，那确定是否有内存泄漏。 内存分析器包文档 操作系统如何管理内存 在Linux操作系统中，当进程启动时，内存分配和管理是通过多个步骤和系统调用来完成的。 进程创建与内存分配的概述 当一个进程被创建时，通常是通过fork()系统调用来复制一个现有进程的内存空间。内存分配涉及到以下几个关键点： 虚拟内存空间的创建 页表的建立与管理 实际物理内存的分配 关键的数据结构 2.1 进程描述符（task_struct） 每个进程在Linux中都有一个task_struct结构体，该结构体保存了进程的所有信息，包括内存管理信息。这个结构体在include/linux/sched.h中定义。 12345struct task_struct &#123; ... struct mm_struct *mm; // 进程的内存描述符 ...&#125;; 2.2 内存描述符（mm_struct） mm_struct是进程内存管理的核心结构，它描述了进程的虚拟地址空间。它在include/linux/mm_types.h中定义。 1234567struct mm_struct &#123; struct vm_area_struct *mmap; // 线性区链表 unsigned long start_code, end_code, start_data, end_data; // 记录代码段，数据段，堆，栈等的内存区域 ... pgd_t *pgd; // 页全局目录 ...&#125;; 内存管理的主要流程 3.1 进程创建（fork()）时的内存分配 当 fork() 被调用时，Linux会为新进程创建一个新的task_struct以及一个新的mm_struct。 copy_mm() 函数被调用以复制父进程的内存管理结构。该函数在 kernel/fork.c 中定义。 1234567891011static struct mm_struct *copy_mm(unsigned long clone_flags, struct task_struct *tsk)&#123; ... new_mm = allocate_mm(); // 分配新的 mm_struct 结构 ... if (mm) &#123; mm_dup(new_mm, oldmm); &#125; ... return new_mm;&#125; mm_dup()函数会复制父进程的mm_struct到子进程，同时通过copy_page_range()来复制父进程的页表。 3.2 虚拟内存区域（vm_area_struct）的管理 虚拟内存区域 (vm_area_struct) 结构体描述了进程地址空间的连续区间，每个区间对应着不同的权限和用途（如代码段、数据段、堆、栈等）。 123456struct vm_area_struct &#123; struct mm_struct *vm_mm; // 关联的内存描述符 unsigned long vm_start; // 起始地址 unsigned long vm_end; // 结束地址 ...&#125;; 这些结构在include/linux/mm_types.h中定义。 3.3 物理内存的分配 实际的物理内存分配通常发生在需要访问某个页面（页缺失）的时刻。Linux使用按需分页机制，初始进程并不会立即分配所有物理内存。 当发生页缺失时，内核会通过 do_page_fault() 处理页缺失中断，并调用 alloc_page() 或 __get_free_pages() 来分配物理页面。alloc_page() 在 mm/page_alloc.c 中定义。 1234struct page *alloc_page(gfp_t gfp_mask)&#123; return __alloc_pages(gfp_mask, 0);&#125; __alloc_pages() 是分配物理页的核心函数，负责找到合适的内存区域并标记为已分配。 malloc和realloc实现 malloc和realloc是用户空间内存分配函数，通常用于在C程序中动态分配和调整内存块的大小。在Linux系统中，malloc和realloc 的具体实现是通过C标准库（glibc）提供的，而不是直接由内核实现的。这些函数的底层实现涉及到内存管理函数，如brk和mmap，以便在进程的虚拟地址空间中分配内存。 malloc的实现 malloc函数用于从堆中分配指定大小的内存块。其具体实现涉及以下几个步骤： 1.1 内存分配算法 malloc在内部分配内存时，通常使用一些分配算法，如first fit、best fit 或 worst fit。glibc实现通常使用binning技术，将不同大小的内存块放入不同的“垃圾桶”（bins）中，以便更快速地找到合适的内存块。 1.2 核心数据结构 glibc的malloc实现使用了多个核心数据结构，其中最重要的是malloc_state和malloc_chunk： malloc_state: 代表一个分配器的状态，包括用于管理内存块的bins。 malloc_chunk: 表示内存块的头部，用于记录块的大小和状态（是否已使用等）。 这些数据结构在glibc的malloc/malloc.c文件中定义。 1.3 实现细节 当malloc被调用时，glibc会首先检查是否有合适大小的空闲内存块。如果没有，则调用sbrk()或mmap()从操作系统请求新的内存。 12345678void *malloc(size_t size) &#123; mstate ar_ptr; mchunkptr victim; ... victim = _int_malloc(ar_ptr, bytes); ... return chunk2mem(victim);&#125; _int_malloc是malloc的内部实现函数，它负责从适当的bin中查找或分配内存块。 realloc的实现 realloc函数用于调整已分配内存块的大小。其实现相对复杂，因为它不仅需要调整内存块的大小，还可能需要将数据移动到新位置。 2.1 内存调整逻辑 如果现有的内存块足够大，则直接缩小或扩展该块。 如果现有的内存块无法满足要求，realloc 会分配一个新的内存块并将旧内存的数据复制到新块中。 2.2 实现细节 realloc的实现也是在glibc的malloc/malloc.c 文件中完成的： 12345678void *realloc(void *ptr, size_t size) &#123; mchunkptr oldp; // 指向原始块的指针 void *newp; // 新分配块的指针 ... newp = _int_realloc(ar_ptr, oldp, bytes); ... return newp;&#125; _int_realloc是realloc的内部实现函数，它处理内存块的调整： 如果新块比旧块小，直接修改块的大小。 如果新块比旧块大且相邻块有足够的空间，扩展现有块。 如果扩展不可能，则分配新块并复制数据。 Linux内核的支持 malloc和realloc的底层依赖于Linux内核的系统调用，例如： brk()：用于调整数据段的结尾，从而扩展或缩小堆。malloc在小规模内存分配时使用brk()。 mmap()：用于直接映射内存区域。malloc 在大规模内存分配时使用 mmap()，并且这种分配方式不受堆的限制。 12345void *sbrk(intptr_t increment) &#123; ... new_brk = do_brk(old_brk, increment); ...&#125; do_brk在mm/mmap.c中定义，是实际执行堆扩展的函数。 free实现 free()函数在C语言中用于释放先前通过malloc()、calloc()(内存数据重置为0)、realloc()等函数分配的动态内存。它的具体实现依赖于底层的内存管理机制，并且在C标准库中通常通过glibc提供。free()的实现涉及对已分配内存块的管理、合并空闲块以及可能的内存释放回操作系统等操作。 free()的概述 free()函数的主要功能是将动态分配的内存块标记为可用，并将其返回到内存分配器的空闲列表中，以供后续内存分配使用。它不会修改指针本身（即，不会将指针置为 NULL），并且不会清除内存内容，只是将内存块释放。 free()的实现概述 free()的实现主要包含以下步骤： 检查指针有效性: 检查传递给 free() 的指针是否为空或无效。 获取内存块信息: 从指针推导出内存块的头部信息，通常通过指针减去一定的偏移量来获得内存块的头部信息。 合并相邻的空闲块: 如果被释放的内存块与相邻的内存块都是空闲的，内存分配器会尝试将这些块合并，以减少内存碎片。 更新空闲列表: 将释放的内存块插入到空闲列表或合适的 bin 中，以便在后续的内存分配请求中再次使用。 具体实现细节 下面详细说明free()的实现，基于glibc中的malloc实现。glibc的内存分配器基于ptmalloc，它是一种基于dlmalloc的分配器。 3.1 核心数据结构 glibc使用malloc_chunk结构体来描述内存块，它的定义通常在malloc.c中： 123456struct malloc_chunk &#123; size_t prev_size; // 上一个块的大小（仅在前一个块空闲时有效） size_t size; // 当前块的大小和状态标志位 struct malloc_chunk* fd; // 下一个空闲块 struct malloc_chunk* bk; // 上一个空闲块&#125;; prev_size: 如果前一个块空闲，则保存其大小。 size: 当前块的大小及一些状态标志位（如是否空闲）。 fd和bk: 空闲链表中的前后指针。 3.2 free() 的实现 glibc中free()的实现大致如下（在 malloc.c 中）： 1234567891011121314151617181920212223242526272829void free(void* ptr) &#123; mstate ar_ptr; mchunkptr p; // 指向被释放块的指针 INTERNAL_SIZE_T size; // 被释放块的大小 mchunkptr nextchunk; // 下一个块的指针 INTERNAL_SIZE_T nextsize; // 下一个块的大小 int nextinuse; // 如果指针为空，直接返回 if (ptr == 0) return; // 获取内存块的头部信息 p = mem2chunk(ptr); // 获取块的大小 size = chunksize(p); // 检查该块是否为空闲块 if (!chunk_is_mmapped(p)) &#123; // 更新空闲列表，将当前块插入 unlink(p, bck, fwd); consolidate(p); // 尝试合并相邻的空闲块 &#125; else &#123; // 如果是通过 mmap 分配的块，直接释放 munmap_chunk(p); &#125;&#125; 3.3 关键函数说明 mem2chunk(ptr): 将用户指针转换为内存块的头部指针（即malloc_chunk结构体指针）。通常通过从ptr指针向前偏移来获得。 chunksize(p): 获取内存块的大小，包括头部信息。通常通过访问malloc_chunk结构体的size字段来获得。 unlink(p, bck, fwd): 从空闲列表中移除当前块。如果当前块已经空闲，意味着它可能被错误地多次释放或者存在内存管理上的问题。 consolidate(p): 尝试合并当前块与前后相邻的空闲块，以减少内存碎片。这有助于将多个小的空闲块合并成一个更大的块，从而更好地利用内存。 munmap_chunk(p): 如果内存块是通过mmap()分配的（通常用于分配大块内存），则直接使用munmap()释放该内存块。 CPU模块 CPU使用率分析器模块包含一个图表，显示应用程序在哪些地方花费了时间。它概述了应用程序在哪些重要方面花费了时间，例如渲染、其脚本和动画。 模块详细信息窗口有三个视图模式： - Timeline： 显示特定帧的计时明细，以及帧长度的时间轴。这是唯一可用于同时查看所有线程的计时以及帧内发生计时的视图模式，以便您可以关联线程之间的计时（例如，作业系统工作线程在主线程上的系统对其进行调度后启动）。 - Hierarchy： 按内部层次结构对时间数据进行分组。此选项以降序列表格式显示应用程序调用的元素，默认按所用时间排序。您还可以按分配的脚本内存量 ( GC Alloc ) 或调用次数对信息进行排序。要更改对表格进行排序的列，请点击表格列的标题。 - Raw Hierarchy： 以与发生计时的调用堆栈类似的层次结构显示计时数据。在此模式下，Unity 会单独列出每个调用堆栈，而不是像在层次结构视图中那样合并它们。 CPU分析模块 前面说的文件和内存资源都属于进程的资源，这些资源可以被进程中的线程进行共享访问，每个进程创建时都会相应的创建一个主线程。 从Timeline视图模式看，Unity创建多个线程，分别处理不同的任务，主要有：主线程（Main Thread）、渲染线程（Render Thread）、工作线程（Job Thread）和加载线程（Loading Thread）等。 下面看一下进程与线程的关系，以及线程的创建，执行和销毁相关的系统调用。 进程与线程 在Linux操作系统中，进程管理线程的方式、线程的创建、执行和销毁是通过一系列系统调用和内核机制实现的。这些操作涉及到Linux内核的调度器、进程控制块（PCB）、线程控制块（TCB）等关键概念。 进程与线程的关系 在Linux中，线程可以被视为一种特殊的进程，称为“轻量级进程”（Lightweight Process, LWP）。多个线程共享同一个进程的资源（如内存空间、文件描述符等），但每个线程有自己独立的栈、寄存器和线程控制块（TCB）。 进程：在Linux中，进程是资源分配的基本单位。 线程：线程是调度的基本单位，它们共享进程的资源，但可以独立执行。 Linux内核将线程和进程统一管理，线程本质上是通过clone()系统调用创建的一个进程，只是它共享了父进程的某些资源。 线程的创建 线程的创建主要依赖于clone()系统调用，它在Linux内核中是创建新进程或线程的核心机制。线程可以通过pthread_create()函数或直接使用 clone()系统调用来创建。 2.1 clone()系统调用 clone()是Linux中创建新进程或线程的核心系统调用。它允许新创建的进程/线程共享其父进程的资源，如内存地址空间、文件描述符、信号处理等。clone()的行为由传递的标志位（flags）决定。 123#include &lt;sched.h&gt;int clone(int (*fn)(void *), void *child_stack, int flags, void *arg, ...); fn: 新线程执行的函数。 child_stack: 新线程的栈指针。 flags: 指定资源共享的标志，如 CLONE_VM（共享内存空间），CLONE_FS（共享文件系统信息）等。 arg: 传递给线程函数的参数。 2.2 do_fork() 函数 clone()调用内核中的do_fork()函数来实际创建新线程（或进程）。do_fork()负责创建新的task_struct，并初始化线程的各种资源。 1234567891011long do_fork(unsigned long clone_flags, unsigned long stack_start, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr, unsigned long tls)&#123; struct task_struct *p; int trace = 0; long nr; p = copy_process(clone_flags, stack_start, stack_size, child_tidptr, NULL, trace); ...&#125; 2.3 copy_process()函数 copy_process()是创建新线程的核心函数。它复制父进程的task_struct并进行必要的初始化，包括设置线程的状态、分配内核栈、处理信号等。 12345678910111213141516struct task_struct *copy_process(struct pid *pid, int trace, int node, struct kernel_clone_args *args)&#123; struct task_struct *p; struct task_struct *retval; ... p = dup_task_struct(current, node); ... // 共享或分配资源 if (clone_flags &amp; CLONE_VM) p-&gt;mm = current-&gt;mm; ... // 设置栈指针 p-&gt;stack = (unsigned long)child_stack; ... return p;&#125; dup_task_struct(): 复制当前进程的 task_struct。 p-&gt;mm = current-&gt;mm: 如果 CLONE_VM 标志设置，子线程与父线程共享内存空间。 线程的执行 线程创建后，内核调度器将其放入就绪队列，等待调度器分配CPU执行。线程的执行过程与普通进程相同。 3.1 调度器 Linux调度器是内核负责分配CPU时间给线程的组件。调度器决定哪个线程获得CPU执行，并处理线程的上下文切换。 12345678void __sched schedule(void)&#123; struct task_struct *prev, *next; ... next = pick_next_task(rq, prev); ... context_switch(rq, prev, next);&#125; pick_next_task(): 选择下一个要执行的线程。 context_switch(): 执行上下文切换，保存当前线程的状态并加载下一个线程的状态。 3.2 上下文切换 上下文切换涉及保存当前线程的CPU寄存器、程序计数器等状态，并加载要执行的线程的状态。Linux通过switch_to()函数完成上下文切换。 1234567891011#define switch_to(prev, next, last) \\do &#123; \\ struct task_struct *__prev = (prev); \\ struct task_struct *__next = (next); \\ ... asm volatile(\"push %%rbp\\n\\t\" // 保存栈基址寄存器 ... \"jmp __switch_to\\n\\t\" ... : \"=a\" (last) : \"S\" (__prev), \"D\" (__next));&#125; while (0) switch_to(): 完成实际的上下文切换，将 CPU 执行权转移到 next 线程。 线程的销毁 线程的销毁发生在线程完成其任务并退出时。线程的销毁涉及资源的释放、状态的更新和通知父线程。 4.1 do_exit()函数 do_exit()是线程或进程退出时调用的核心函数。它负责清理线程的资源，将线程标记为僵尸状态，并通知父线程。 1234567891011121314151617void do_exit(long code)&#123; struct task_struct *tsk = current; ... // 清理线程资源 exit_mm(tsk); exit_files(tsk); ... // 将线程状态设置为 TASK_DEAD tsk-&gt;state = TASK_DEAD; ... // 通知父进程 notify_parent(tsk, tsk-&gt;exit_signal); ... // 进行最后的上下文切换 schedule();&#125; exit_mm(): 释放内存资源。 exit_files(): 关闭打开的文件。 notify_parent(): 通知父进程线程已经退出。 4.2 release_task()函数 release_task()负责最终清理僵尸线程的资源，并将其从系统中移除。 123456789void release_task(struct task_struct *p)&#123; ... // 从任务链表中移除 list_del_rcu(&amp;p-&gt;tasks); ... // 释放 task_struct 结构体 free_task_struct(p);&#125; list_del_rcu(): 将任务从链表中删除。 free_task_struct(): 释放 task_struct 相关资源。 渲染模块 显示渲染统计数据以及有关CPU和GPU渲染内容的信息，我们可以通过这些统计数据来衡量场景中不同区域的资源强度，这对于优化很有帮助。 该图表显示应用程序渲染的批次（Batches）、SetPass调用、三角形和顶点的数量。下方窗口显示更多渲染统计数据。 渲染模块 为了更好的理解上面的这些参数，我们将以伪代码的方式说明一下Unity渲染对象的流程。 在Unity中，渲染流程涉及多个步骤和概念，其中包括SetPass Calls、Draw Calls和Batches。理解这些概念及其在渲染流程中的作用，对优化Unity项目的性能至关重要。 渲染流程简述 场景处理: Unity根据摄像机的视角，计算出当前帧内所有可见的物体（Renderers）。 SetPass Calls: 针对每个不同材质的物体，Unity会发起一次SetPass Call，将材质（Shader，参数和贴图等）绑定到GPU。 Draw Calls: 对每个可见物体，Unity向GPU发送渲染指令，通过Draw Call绘制物体。 Batching: Unity尝试通过合批技术(动态，静态，GPU Instancing等)，将多个Draw Calls合并为一个，以减少开销。 SetPass Calls 定义: SetPass Call是指在渲染管线中，Unity将材质（Material）和着色器（Shader）切换到GPU的过程。每次需要更换材质或着色器时，都会产生一次SetPass Call。 源码分析: 在Unity内部，SetPass Call通常是通过调用Material.SetPass()来实现的。这会将当前材质的渲染状态绑定到GPU，从而为后续的Draw Call做好准备。 性能影响: SetPass Calls相对昂贵，因为它们涉及GPU状态的切换，频繁的SetPass Calls会显著增加渲染开销。 优化: 合并相同材质的对象以减少SetPass Calls。 使用图集（Texture Atlas）来减少材质切换。 Draw Calls 定义: Draw Call是指从CPU向GPU发出的一次渲染指令，用于绘制一个或多个三角形。每次调用Graphics.DrawMesh或Renderer.Render等渲染函数时都会产生Draw Call。 源码分析: 在Unity的渲染过程中，Draw Call通常是通过以下流程产生的： 123456789101112// 伪代码说明Draw Call的处理流程foreach (var renderer in visibleRenderers)&#123; if (renderer.isVisible) &#123; // 准备材质和Shader renderer.material.SetPass(0); // 触发一个绘制 Graphics.DrawMesh(renderer.mesh, renderer.transform.localToWorldMatrix, renderer.material, renderer.gameObject.layer); &#125;&#125; 性能影响: Draw Calls是Unity渲染流程中的核心部分，过多的Draw Calls会导致CPU和GPU之间的通信瓶颈。 优化: 使用动态合批（Dynamic Batching）和静态合批（Static Batching）。 合并网格（Mesh）以减少Draw Calls。 Batches 定义: Batches是指将多个Draw Calls合并为一个，以减少CPU与GPU之间的通信次数。在Unity中，合批技术可以分为静态合批（Static Batching）、动态合批（Dynamic Batching）和GPU Instancing。 源码分析: 静态合批: 当多个静态对象使用相同的材质时，Unity会将它们的网格合并为一个以减少Draw Calls。 动态合批: 适用于动态对象，小型网格的对象在满足一定条件时会被合并为一个Draw Call。 GPU Instancing: 适用于同一个网格实例的多次渲染，将同一网格对象的多个实例合并为一个Draw Call。 1234567// GPU Instanceingfor (int i = 0; i &lt; instanceCount; i++)&#123; matrixArray[i] = Matrix4x4.TRS(positions[i], Quaternion.identity, Vector3.one);&#125;materialPropertyBlock.SetMatrixArray(\"_Matrices\", matrixArray);Graphics.DrawMeshInstanced(mesh, 0, material, matrixArray, instanceCount, materialPropertyBlock); 性能影响: 合批可以显著减少Draw Calls的数量，从而提高渲染性能。 优化: 对于静态物体，启用静态合批。 确保动态物体使用相同的材质和网格，以便动态合批生效。 使用GPU Instancing渲染大量相同的对象。 UI模块 UI和UI细节分析器模块提供有关Unity在应用程序中布局和渲染用户界面所花费的时间和资源的信息。我们可以使用此模块了解Unity如何处理应用程序的UI批处理，包括批处理对象的原因和方式。我们可以使用此模块找出UI的哪个部分导致性能缓慢，或者在拖动时间轴时预览UI。 UI模块 Layout: Unity执行UI布局过程所花费的时间。这包括Horizo​​ntalLayoutGroup、VerticalLayoutGroup和GridLayoutGroup所做的计算。 Render: UI花费多少时间完成其渲染部分。这是直接渲染到图形设备或渲染到主渲染队列的成本。 Batches: 显示批处理在一起的绘制调用总数。 Vertices: 用于渲染UI部分的总顶点数。 Markers: 显示事件标记。Unity会在用户与UI交互时记录标记（例如，单击按钮或更改滑块值），然后将其绘制为图表上的垂直线和标签。 模块详细信息面板 Object：在分析期间，应用程序使用的UI画布列表。双击某一行可突出显示匹配的对象场景。 Self Batch Count： Unity为画布生成了多少个批次。 Cumulative Batch Count：Unity为画布及其所有嵌套画布生成了多少个批次 Self Vertex Count：该画布正在渲染多少个顶点。 Cumulative Vertex Count：此画布和嵌套画布正在渲染多少个顶点 Batch Breaking Reason：Unity拆分批次的原因。有时Unity可能无法将对象一起进行批处理。常见原因包括： 不与Canvas共面，批处理需要对象的矩形变换与画布共面（未旋转）。CanvasInjectionIndex ，其中存在 CanvasInjectionIndex， 组件并强制进行新批次，例如当它在其余组件之上显示组合框的下拉列表时。 不同的材质实例、矩形裁剪、纹理或A8TextureUsage，其中Unity只能将具有相同材质、遮罩、纹理和纹理通道等完全相同的对象进行批处理。 GameObject Count：此批次中有多少个 GameObject GameObjects：批次中的游戏对象列表。 UGUI的绘制过程，参见UGUI源码分析 物理模块(2D/3D) 物理学分析器模块显示物理系统在项目中处理的物理信息。这些信息可以帮助您诊断和解决与项目场景中的物理相关的性能问题或意外差异。 直接上2张图吧，这个内容比较直观，手游项目中也不重度使用物理系统，官方文档Physics Profiler module。 3D物理模块 物理模块 2D物理模块 2D物理模块 Unity的3D物理系统使用的是NVIDIA的PhysX，2D物理系统使用的是开源的Box2D。 自定模块 Unity为我们提供了自定捕获性能的接口，并可以在Unity Profiler窗口中进行查看，也可以根据内建的Counter和自定义的Counter创建自定Profiler模块，并且可以自定义详细面板，也为Native代码提供接口收集性能数据和监听Profiler中触发的事件。 自定义模块 核心概念 自定义模块中主要有三个概念：记录(Recorder)、标记(Marker)和计数器(Counter)。 记录(Recorder), 记录(Recorder)是用于记录标记(Marker)或计数器(Counter)产生的测量数据。 标记(Marker), 在需要捕获性能的代码块中创建Marker，可以捕获代码块的执行时间，可以在Profiler的CPU模块中查看器性能参数。 计数器(Counter), 计数的主要用于统计每帧中次数，比如:对象创建的数量，执行的操作次数等。 API接口 Unity性能分析API, 主要分为两个部分： - Native API， Native API在Unity.Profiling包中，是对C++API的直接封装。核心类是ProfilerUnsafeUtility，通过它可以直接创建标记(Marker)和计数器(Counter)，返回的是一个对象的指针IntPtr。 - C# API， C#的API是对Native API的再次封装，方便使用，主要有两个：一个在UnityEngine.Profiling包中，另一个是个外部包Unity Profiling Core。 记录(Recorder) ProfilerRecorderHandle, 记录(Recorder)的句柄，通过此类的ProfilerRecorderHandle.GetAvailable可以获取所有可用的记录句柄，并且可以通过ProfilerRecorderHandle.GetDescription函数获取所有句柄的描述信息。 ProfilerRecorder， 记录的原始数据，可以通过此类获取自定义标记（Marker）和计数器（Counter）的采样数据。 ProfilerRecorderSample， 记录里存储的值。 Recorder，和ProfilerRecorder一样，感觉是为了兼容以前的接口而保留的类。 标记(Marker) ProfilerMarker， 是封装了对标记（Marker）相关的操作，ProfilerMarker内部直接调用ProfilerUnsafeUtility.CreateMarker创建标记（Marker）,并管理返回的指针IntPtr。 ProfilerMarker&lt;T...&gt;, 是由外部包Unity Profiling Core提供，与Unity内部的ProfilerMarker区别是可以在采用的时记录Meta数据，模板T就是记录的数据类型，内部也是对ProfilerUnsafeUtility的直接调用。 CustomSampler， 和ProfilerMarker一样，功能几乎一样，感觉是为了兼容以前的接口而保留的类。 计数器(Counter) ProfilerCounter, 是一个计数器类，它将计数值，直接通过Marker的Meta数据存储 ProfilerCounterValue, 此类也是一个计数器，它的不同之处在于，不依赖Marker,直接通过ProfilerUnsafeUtility.CreateCounterValue创建，并记录了数据的地址，内部直接使用*m_Value = value， m_Value是ProfilerUnsafeUtility.CreateCounterValue返回的地址。但是本质上还是存在Marker的Meta数据中。 其他类 Profiler，此类设计的目标是表示Profiler本身的，虽然，现在基本上都用上面讨论的这些类来记录性能数据，但是此类里面记录了一些内存统计信息，可以方便我们在任何使用访问，因为Unity内部管理的着这些内存所有本身不存在统计开销。不要使用这里面的BeginSample和EndSample等，开销比上述的內开销大。也可以通过此类将性能分析器的数据以日志的方式输出到指定文件中。输出的问题可以直接通过Unity的Profiler工具打开查看和直接捕获的数据一致。 创建Marker和Counter 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class TestCode : MonoBehaviour&#123; // Marker private static ProfilerMarker _marker; private static ProfilerMarker&lt;int&gt; _markerParam; private static CustomSampler _customSampler; // Counter private static ProfilerCounter&lt;int&gt; _profilerCounter; private static ProfilerCounterValue&lt;int&gt; _profilerCounterValue; void Start() &#123; _marker = new ProfilerMarker(ProfilerCategory.Scripts, \"TestMarker\"); _markerParam = new ProfilerMarker&lt;int&gt;(ProfilerCategory.Scripts, \"TestMarkerParam\", \"Param1\"); _customSampler = CustomSampler.Create(\"CustomMarker\", true); _profilerCounter = new ProfilerCounter&lt;int&gt;(ProfilerCategory.Scripts, \"Counter\", ProfilerMarkerDataUnit.Count); _profilerCounterValue = new ProfilerCounterValue&lt;int&gt;(ProfilerCategory.Scripts, \"CounterValue\", ProfilerMarkerDataUnit.Count); &#125; void Update() &#123; TestMarker(); TestCounter(); &#125; void TestMarker() &#123; // ProfilerUnsafeUtility.BeginSample(markerHandle); // using (_markerParam.Auto(50)) // using (_marker.Auto(\"test\")) _customSampler.Begin(); _marker.Begin(\"testValue\"); &#123; long total = 0; for (int i = 0; i &lt; 1400000; i++) &#123; total += i; &#125; &#125; _marker.End(); _customSampler.End(); // ProfilerUnsafeUtility.EndSample(markerHandle); &#125; static void TestCounter() &#123; _profilerCounter.Sample(Time.frameCount); _profilerCounterValue.Value = Time.frameCount; _profilerCounterValue.Sample(); &#125;&#125; 获取记录(Recorder)数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public class TestCode : MonoBehaviour&#123; private static IntPtr markerHandle; private ProfilerCategory myCategory; private ProfilerRecorder myMarkerRecorder; private string statsText; void Start() &#123; //枚举所有记录 EnumerateProfilerStats(); unsafe &#123; string categoryName = \"CategoryName\"; // fixed (char* p = categoryName) &#123; myCategory = new ProfilerCategory(categoryName);// ProfilerUnsafeUtility.CreateCategory(p, categoryName.Length, ProfilerCategoryColor.Scripts); markerHandle = ProfilerUnsafeUtility.CreateMarker(\"MyMarker\", myCategory, MarkerFlags.Default, 0); myMarkerRecorder = ProfilerRecorder.StartNew(myCategory, \"MyMarker\"); &#125; &#125; // 查看Profiler的统计信息 TestProfiler() &#125; void Update() &#123; var sb = new StringBuilder(500); sb.AppendLine($\"MyMarker: &#123;myMarkerRecorder.LastValue&#125;\"); statsText = sb.ToString(); &#125; void OnGUI() &#123; GUI.TextArea(new Rect(10, 30, 250, 50), statsText); &#125; // Recorder struct StatInfo &#123; public ProfilerCategory Cat; public string Name; public ProfilerMarkerDataUnit Unit; &#125; // 枚举Profiler记录 static unsafe void EnumerateProfilerStats() &#123; var availableStatHandles = new List&lt;ProfilerRecorderHandle&gt;(); ProfilerRecorderHandle.GetAvailable(availableStatHandles); var availableStats = new List&lt;StatInfo&gt;(availableStatHandles.Count); foreach (var h in availableStatHandles) &#123; var statDesc = ProfilerRecorderHandle.GetDescription(h); var statInfo = new StatInfo() &#123; Cat = statDesc.Category, Name = statDesc.Name, Unit = statDesc.UnitType &#125;; if (statInfo.Name == \"MyMarker\" || (statInfo.Name ==\"MyCounterValue\")) Debug.LogError(\"找到了MyMarker\"); availableStats.Add(statInfo); &#125; availableStats.Sort((a, b) =&gt; &#123; var result = string.Compare(a.Cat.ToString(), b.Cat.ToString()); if (result != 0) return result; return string.Compare(a.Name, b.Name); &#125;); var sb = new StringBuilder(\"Available stats:\\n\"); foreach (var s in availableStats) &#123; sb.AppendLine($\"&#123;(int)s.Cat&#125;\\t\\t - &#123;s.Name&#125;\\t\\t - &#123;s.Unit&#125;\"); &#125; Debug.Log(sb.ToString()); &#125; //查看Profiler的统计信息 void TestProfiler() &#123; Debug.LogError($\"Profiler.supported:&#123;Profiler.supported&#125;\"); Debug.LogError($\"Profiler.enabled:&#123;Profiler.enabled&#125;\"); Debug.LogError($\"Profiler.areaCount:&#123;Profiler.areaCount&#125;\"); Debug.LogError($\"Profiler.logFile:&#123;Profiler.logFile&#125;\"); Debug.LogError($\"Profiler.enableAllocationCallstacks:&#123;Profiler.enableAllocationCallstacks&#125;\"); Debug.LogError($\"Profiler.enableBinaryLog:&#123;Profiler.enableBinaryLog&#125;\"); Debug.LogError($\"Profiler.maxUsedMemory:&#123;Profiler.maxUsedMemory / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.usedHeapSizeLong:&#123;Profiler.usedHeapSizeLong / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.GetCategoriesCount:&#123;Profiler.GetCategoriesCount()&#125;\"); Debug.LogError($\"Profiler.GetTempAllocatorSize:&#123;Profiler.GetTempAllocatorSize() / (1024*1024)&#125;\"); // Debug.LogError($\"Profiler.GetTempAllocatorSize:&#123;Profiler.GetTotalFragmentationInfo()&#125;\"); Debug.LogError($\"Profiler.GetMonoHeapSizeLong:&#123;Profiler.GetMonoHeapSizeLong() / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.GetMonoUsedSizeLong:&#123;Profiler.GetMonoUsedSizeLong() / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.GetRuntimeMemorySizeLong:&#123;Profiler.GetRuntimeMemorySizeLong(this)&#125;\"); Debug.LogError($\"Profiler.GetTotalAllocatedMemoryLong:&#123;Profiler.GetTotalAllocatedMemoryLong() / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.GetTotalReservedMemoryLong:&#123;Profiler.GetTotalReservedMemoryLong() / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.GetAllocatedMemoryForGraphicsDriver:&#123;Profiler.GetAllocatedMemoryForGraphicsDriver() / (1024*1024)&#125;\"); Debug.LogError($\"Profiler.GetTotalUnusedReservedMemoryLong:&#123;Profiler.GetTotalUnusedReservedMemoryLong() / (1024*1024)&#125;\"); &#125;&#125; 创建自定Profiler模块 主要有两个部分组成： 1. Profiler模块 2. 模块详细界面 Profiler模块,代码如下： 12345678910111213141516171819202122232425262728293031using Unity.Profiling;using Unity.Profiling.Editor;[System.Serializable][ProfilerModuleMetadata(\"Tank Effects\")] public class CustomProfilerModule : ProfilerModule&#123; static readonly ProfilerCounterDescriptor[] k_Counters = new ProfilerCounterDescriptor[] &#123; new ProfilerCounterDescriptor(GameStats.TankTrailParticleCountName, GameStats.TanksCategory), new ProfilerCounterDescriptor(GameStats.ShellExplosionParticleCountName, GameStats.TanksCategory), new ProfilerCounterDescriptor(GameStats.TankExplosionParticleCountName, GameStats.TanksCategory), &#125;; // 确保ProfilerCategory.Scripts和ProfilerCategory.Memory分类在此模块激活时自动激活 static readonly string[] k_AutoEnabledCategoryNames = new string[] &#123; ProfilerCategory.Scripts.Name, ProfilerCategory.Memory.Name &#125;; public CustomProfilerModule() : base(k_Counters, ProfilerModuleChartType.Line, k_AutoEnabledCategoryNames) &#123; &#125; public override ProfilerModuleViewController CreateDetailsViewController() &#123; return new CustomDetailsViewController(ProfilerWindow); &#125;&#125; 模块详细界面，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960using Unity.Profiling.Editor;using UnityEditor;using UnityEditorInternal;using UnityEngine.UIElements;public class CustomDetailsViewController : ProfilerModuleViewController&#123; Label m_TankTrailParticleCountLabel; Label m_ShellExpCountLabel; Label m_TankExpCountLabel; public CustomDetailsViewController(ProfilerWindow profilerWindow) : base(profilerWindow) &#123; &#125; protected override VisualElement CreateView() &#123; var view = new VisualElement(); m_TankTrailParticleCountLabel = new Label() &#123; style = &#123; paddingTop = 8, paddingLeft = 8 &#125; &#125;; view.Add(m_TankTrailParticleCountLabel); m_ShellExpCountLabel = new Label() &#123; style = &#123; paddingTop = 8, paddingLeft = 8 &#125; &#125;; view.Add(m_ShellExpCountLabel); m_TankExpCountLabel = new Label() &#123; style = &#123; paddingTop = 8, paddingLeft = 8 &#125; &#125;; view.Add(m_TankExpCountLabel); ReloadData(); ProfilerWindow.SelectedFrameIndexChanged += OnSelectedFrameIndexChanged; return view; &#125; protected override void Dispose(bool disposing) &#123; if (!disposing) return; ProfilerWindow.SelectedFrameIndexChanged -= OnSelectedFrameIndexChanged; base.Dispose(disposing); &#125; void ReloadData() &#123; var selectedFrameIndexInt32 = System.Convert.ToInt32(ProfilerWindow.selectedFrameIndex); var value = ProfilerDriver.GetFormattedCounterValue(selectedFrameIndexInt32, GameStats.TanksCategory.Name, GameStats.TankTrailParticleCountName); m_TankTrailParticleCountLabel.text = $\"The value of '&#123;GameStats.TankTrailParticleCountName&#125;' in the selected frame is &#123;value&#125;.\"; value = ProfilerDriver.GetFormattedCounterValue(selectedFrameIndexInt32, GameStats.TanksCategory.Name, GameStats.ShellExplosionParticleCountName); m_ShellExpCountLabel.text = $\"The value of '&#123;GameStats.ShellExplosionParticleCountName&#125;' in the selected frame is &#123;value&#125;.\"; value = ProfilerDriver.GetFormattedCounterValue(selectedFrameIndexInt32, GameStats.TanksCategory.Name, GameStats.TankExplosionParticleCountName); m_TankExpCountLabel.text = $\"The value of '&#123;GameStats.TankExplosionParticleCountName&#125;' in the selected frame is &#123;value&#125;.\"; &#125; void OnSelectedFrameIndexChanged(long selectedFrameIndex) &#123; ReloadData(); &#125;&#125; Native代码的支持 Native接口可以让C/C++代码调用Profiler的接口，以及在C/C++中接受Profiler发送的事件，实例代码入下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111#pragma once#include \"pch.h\"#include &lt;IUnityLog.h&gt;#include &lt;IUnityProfiler.h&gt;#include &lt;IUnityProfilerCallbacks.h&gt;#include &lt;stdio.h&gt;static FILE* s_file = NULL;static const UnityProfilerMarkerDesc* s_MyPluginMarker = NULL;static IUnityProfiler* s_UnityProfiler;static bool s_IsDevelopmentBuild;static IUnityProfilerCallbacks* s_UnityProfilerCallbacks = NULL;static IUnityLog* s_unityLogger = nullptr;static void UNITY_INTERFACE_API MyProfilerCreateMarkerCallback(const UnityProfilerMarkerDesc* markerDesc, void* userData);static void UNITY_INTERFACE_API MyProfilerEventCallback(const UnityProfilerMarkerDesc* markerDesc, UnityProfilerMarkerEventType eventType, unsigned short eventDataCount, const UnityProfilerMarkerData* eventData, void* userData);static void UNITY_INTERFACE_API MyProfilerPushMarker(const char* name);static void UNITY_INTERFACE_API MyProfilerPopMarker(const char* name);// 在插件加载时，Unity调用extern \"C\" void UNITY_INTERFACE_EXPORT UNITY_INTERFACE_API UnityPluginLoad(IUnityInterfaces * unityInterfaces)&#123; s_file = fopen(\"_output_.txt\", \"w\"); if (s_file == NULL) &#123; printf(\"无法打开文件\\n\"); &#125; // 发送到Unity s_unityLogger = unityInterfaces-&gt;Get&lt;IUnityLog&gt;(); s_UnityProfiler = unityInterfaces-&gt;Get&lt;IUnityProfiler&gt;(); if (s_UnityProfiler == nullptr) return; s_IsDevelopmentBuild = s_UnityProfiler-&gt;IsAvailable() != 0; // Unity发送过来 s_UnityProfilerCallbacks = unityInterfaces-&gt;Get&lt;IUnityProfilerCallbacks&gt;(); s_UnityProfilerCallbacks-&gt;RegisterCreateMarkerCallback(&amp;MyProfilerCreateMarkerCallback, NULL); s_UnityProfiler-&gt;CreateMarker(&amp;s_MyPluginMarker, \"NativeCallUnity\", kUnityProfilerCategoryOther, kUnityProfilerMarkerFlagDefault, 0);&#125;// 在插件卸载时，Unity调用extern \"C\" void UNITY_INTERFACE_EXPORT UNITY_INTERFACE_API UnityPluginUnload()&#123; if (s_file != NULL) &#123; fclose(s_file); s_file = NULL; &#125; s_unityLogger = nullptr; s_UnityProfiler = nullptr; s_UnityProfilerCallbacks-&gt;UnregisterCreateMarkerCallback(&amp;MyProfilerCreateMarkerCallback, NULL); s_UnityProfilerCallbacks-&gt;UnregisterMarkerEventCallback(NULL, &amp;MyProfilerEventCallback, NULL); s_UnityProfilerCallbacks = nullptr;&#125;//调用Unityextern \"C\" UINT64 UNITY_INTERFACE_EXPORT UNITY_INTERFACE_API NativeCallUnity()&#123; if (s_IsDevelopmentBuild) s_UnityProfiler-&gt;BeginSample(s_MyPluginMarker); UINT64 sum = 0; for (size_t i = 0; i &lt; 2000000; i++) &#123; sum += 1; &#125; if (s_IsDevelopmentBuild) s_UnityProfiler-&gt;EndSample(s_MyPluginMarker); return sum;&#125;//Unity Profiler发送到Native的事件static void UNITY_INTERFACE_API MyProfilerEventCallback(const UnityProfilerMarkerDesc* markerDesc, UnityProfilerMarkerEventType eventType, unsigned short eventDataCount, const UnityProfilerMarkerData* eventData, void* userData)&#123; switch (eventType) &#123; case kUnityProfilerMarkerEventTypeBegin: &#123; MyProfilerPushMarker(markerDesc-&gt;name); break; &#125; case kUnityProfilerMarkerEventTypeEnd: &#123; MyProfilerPopMarker(markerDesc-&gt;name); break; &#125; &#125;&#125;static void UNITY_INTERFACE_API MyProfilerCreateMarkerCallback(const UnityProfilerMarkerDesc* markerDesc, void* userData)&#123; s_UnityProfilerCallbacks-&gt;RegisterMarkerEventCallback(markerDesc, &amp;MyProfilerEventCallback, NULL); UNITY_LOG_ERROR(s_unityLogger, markerDesc-&gt;name);&#125;static void UNITY_INTERFACE_API MyProfilerPushMarker(const char* name) &#123; // 本地记录Marker采样开始(调用Log会死递归) //UNITY_LOG_ERROR(s_unityLogger, \"==============MyProfilerPushMarker\"); fprintf(s_file, \"Push Name:%s \\n\", name); fflush(s_file);&#125;static void UNITY_INTERFACE_API MyProfilerPopMarker(const char* name) &#123; // 本地记录Marker采样结束 //UNITY_LOG_ERROR(s_unityLogger, \"==============MyProfilerPopMarker\"); fprintf(s_file, \"Pop Name:%s \\n\", name); fflush(s_file);&#125; Lua Profiler Lua Profiler是一个开源的Lua性能分析工具，提供了CPU和内存情况的统计和分析。 LuaProfiler捕获性能数据的基本原理是在Lua函数中插入采样代码，通过Hook技术截获luaL_loadbuffer函数，在加载出来的lua代码中调用InsertSample函数插入采样代码，核心代码： 1234567891011121314151617181920212223public static readonly string LOCAL_PROFILER = \"local BeginMikuSample = require('MikuLuaProfiler').LuaProfiler.BeginSample \" + \"local EndMikuSample = require('MikuLuaProfiler').LuaProfiler.EndSample \" + \"local miku_unpack_return_value = require('miku_unpack_return_value') local MikuMainChunkFun = function(...) \";#region parsepublic static string InsertSample(string value, string name)&#123; LLex l = new LLex(new StringLoadInfo(value), name); string sampleStr = string.Format(\"&#123;0&#125;BeginMikuSample(\\\"[lua]:require &#123;1&#125;,&#123;1&#125;&amp;line:1\\\")\", LOCAL_PROFILER, name); l.InsertString(0, sampleStr); int lastPos = 0; int nextPos = l.pos; l.Next(); int tokenType = l.Token.TokenType; lastPos = nextPos; nextPos = l.pos; InsertSample(l, ref lastPos, ref nextPos, tokenType, false); l.InsertString(l.Length, \"\\n end return MikuMainChunkFun(...)\"); return l.code;&#125; 上面的操作对于使用者来说时透明的，在开始记录数据时，正常运行游戏就可以在函数中插入采用代码了。有时我们也需要查看某个函数中特定部分代码的性能数据，可以通过如下代码插入自定义的采样： 1234local LuaProfiler = MikuLuaProfiler.LuaProfilerLuaProfiler.BeginSampleCustom(\"profiler name\")-- your codeLuaProfiler.EndSampleCustom() Snapdragon Profiler Snapdragon Profiler是高通官方开发的性能分析工具，它可以帮助我们分析CPU, GPU, DSP, 内存, 电量, 热量和网络数据，以便我们可以查找并修复性能瓶颈。Snapdragon Profiler主要有四种数据捕获模式： 实时(Realtime), 实时视图可以轻松地在时间线上关联系统资源使用情况。提供了22个类别的150多种不同的硬件性能计数器。 踪迹捕获(Trace Capture), 跟踪捕获模式允许您在时间线上可视化内核和系统事件，以分析 CPU、GPU 和 DSP 中的低级系统事件。查看 CPU 调度和 GPU 阶段数据，以了解您的应用程序将时间花在何处。 快照捕捉(Snapshot Capture), 快照捕获模式允许您从OpenGL ES或Vulkan应用程序捕获和调试渲染的帧。可以查看和编辑shader并实时在设备上预览，查看和调试像素历史记录。如果不能看到Shader，需要清理一下App的缓存，下载资源不用清 采样捕获(Sampling Capture), 采样捕获模式允许您记录应用程序的调用栈图以分析消耗的CPU时间。调用栈图以火焰图的形式可视化。此模式必须要配置NDK13及以上，在AndroidManifest.xml中android:debuggable必须为true(Root除外) 基本的操作和面板说明，可以看Snapdragon Profiler手册。 Snapdragon Profiler不仅可以用来分析性能，也可以捕捉其他游戏的帧，根据帧的绘制情况可以很容易分析出其设计，包括Shader源码都可以获取到 参数说明： CPU Core Load， 是指CPU每个核心的工作负载或使用情况的百分比。CPU是多核心处理器，通常一个CPU会包含多个核心，每个核心可以独立执行指令和处理任务。CPU Core Load代表了每个核心在特定时间内执行工作的强度。 CPU Core Frequency， CPU Core Frequency（CPU 核心频率）是指CPU每个核心在每秒钟内执行指令的速度，通常以GHz（千兆赫兹）或 MHz（兆赫兹）为单位。核心频率越高，处理指令的速度就越快。 CPU Core Utilization， CPU Core Utilization（CPU 核心利用率）是指每个CPU核心在特定时间段内被实际使用的百分比，表示该核心的繁忙程度。它反映了系统的任务在某个核心上执行的效率，以及该核心处理资源的利用率。与“Core Load”相比“Core Load”更侧重于当前任务对核心的实际占用，而“Utilization”一般指的是一段时间内的平均使用率。 GPU Bus Busy， GPU Bus Busy 是指 GPU（图形处理单元）与其他组件（如 CPU 和内存）之间的数据总线的繁忙程度，通常以百分比表示。这一指标反映了 GPU 数据总线在特定时间内用于数据传输的活跃程度。 Avg bytes/Fragment（平均每个片段的字节数）， 是一个性能指标，用于衡量在图形渲染等上下文中，传输的每个数据片段的平均字节大小。 Avg Bytes/Vertex（每个顶点的平均字节数）， 是GPU内存统计中的一个重要指标，用于衡量在图形渲染过程中，每个顶点的数据大小。这一指标通常应用于3D图形处理，特别是在处理顶点缓冲区时。 Avg Frame Time（平均帧时间）， 表示在特定时间段内，生成和显示一帧图像所需的平均时间，通常以毫秒（ms）为单位表示。 Clocks/Second（每秒钟时钟周期数）， 是一个用于描述GPU性能的重要指标。它表示GPU在一秒钟内可以执行的时钟周期数，通常以赫兹（Hz）为单位表示。 Read Total (Bytes/sec)， 用于衡量 GPU 从内存中读取数据的总速率，单位为字节每秒（Bytes/sec）。这个指标可以帮助评估 GPU 的内存带宽和性能。 SP Memory Read (Bytes/Second)， 是一个专门用于衡量GPU中的 Shader Processor（着色器处理器）从内存中读取数据的速率的性能指标，单位为字节每秒（Bytes/sec）。这个指标反映了着色器在处理图形和计算任务时，从内存获取数据的效率。 Texture Memory Read BW (Bytes/Second)， 是一个用于衡量GPU从纹理内存 中读取数据的带宽的性能指标，单位为字节每秒（Bytes/sec）。这个指标专门关注 GPU 在图形渲染过程中访问纹理数据的效率。 Vertex Memory Read (Bytes/Second)， 是一个用于衡量 GPU 从 顶点内存 中读取数据的速率的性能指标，单位为字节每秒（Bytes/sec）。这个指标专注于 GPU 在处理图形渲染时访问顶点数据的效率。 Write Total (Bytes/sec)， 是一个用于衡量 GPU 向内存写入数据的总速率的性能指标，单位为字节每秒（Bytes/sec）。这个指标反映了 GPU 在处理图形渲染或计算任务时，将数据写入内存的效率。这包括所有写入操作，比如更新纹理、缓冲区和其他资源。 Avg Preemption Delay（平均抢占延迟）， 是一个用于衡量GPU任务被抢占后，恢复执行所需平均时间的性能指标，通常以毫秒（ms）为单位表示。这一指标对于理解 GPU 的抢占机制和性能表现至关重要。 Preemption/Second， 是一个用于衡量 GPU 每秒发生的抢占事件数量的性能指标。这一指标有助于评估 GPU 任务调度的频率和效率。 Prims Clipped， 是一个用于衡量 GPU 在图形渲染过程中被裁剪的图元（Primitives）数量的性能指标。图元通常指的是基本的几何体，如点、线和三角形。 Prims Trivially Rejected， 是一个用于衡量 GPU 在图形渲染过程中被简单拒绝的图元（Primitives）数量的性能指标。这些图元在被处理之前就被识别为不需要进一步处理。简单拒绝指的是 GPU 在早期阶段判断某些图元（例如点、线、三角形）在渲染过程中不影响最终图像，因而直接拒绝这些图元，而无需进行更复杂的计算。这种拒绝通常基于图元的边界框与视口的关系。 Average Polygon Area，（平均多边形面积）是一个用于衡量渲染过程中所处理的多边形的平均面积的性能指标。该指标通常以像素为单位，反映了在图形渲染中，GPU 处理的多边形的大小和复杂性。 Average Vertices/Polygon（平均每个多边形的顶点数）， 是一个用于衡量在图形渲染过程中，每个多边形（通常是三角形或其他类型的多边形）平均包含的顶点数量的性能指标。这个指标有助于理解场景的几何复杂性和GPU处理的效率。 Pre-clipped Polygon/Second， 是一个用于衡量 GPU 在图形渲染过程中每秒处理的预裁剪多边形（Polygons）的数量的性能指标。该指标主要反映了 GPU 在进行图元处理前，经过初步裁剪的多边形数量，通常用于评估 GPU 的处理能力和渲染效率。预裁剪是指在多边形进入完整的渲染管线之前，GPU 对其进行初步的裁剪。这通常基于多边形与视口（viewport）或其他裁剪区域的关系，以确定哪些多边形是可见的，哪些是可以被丢弃的。 Reused Vertices/Second，是一个用于衡量 GPU 在图形渲染过程中每秒重新使用的顶点（Vertices）数量的性能指标。该指标反映了 GPU 在处理图元（如三角形、线段等）时，能够重复利用已存在顶点的效率。 Anisotropic Filtered，是一个用于衡量在图形渲染过程中，应用各向异性过滤的纹理像素（Texels）数量的性能指标。该指标通常用于评估 GPU 在处理纹理时的效果，尤其是在处理倾斜表面时的图像质量。 Non-Base Level Textures， 是一个用于衡量在图形渲染过程中，使用的非基础级别(非一层)纹理数量的性能指标。这个指标主要反映了 GPU 在处理纹理时，涉及的不同层级的纹理数据。 Shader ALU Capacity Utilized， 是一个用于衡量 GPU 在图形渲染过程中，着色器算术逻辑单元（ALU）的利用率的性能指标。这个指标反映了 GPU 在执行着色器程序时，算术逻辑单元的实际使用情况与其最大处理能力之间的比率。 Shader Busy， 是一个用于衡量 GPU 在图形渲染过程中，着色器处于忙碌状态的时间占总时间的比例的性能指标。这个指标反映了 GPU 着色器执行任务的效率和资源利用情况。 Shader Stalled， 是一个用于衡量 GPU 着色器在执行过程中因各种原因而处于等待或阻塞状态的时间比例的性能指标。这个指标反映了 GPU 着色器执行效率的下降及其可能受到的限制。 Texture Pipes Busy， Texture Pipes Busy 是一个用于衡量 GPU 纹理处理管道（Texture Pipes）在图形渲染过程中忙碌状态的时间占总时间比例的性能指标。这个指标反映了 GPU 在处理纹理采样和纹理过滤操作时的效率和利用情况。 Time ALUs Working， 是一个用于衡量 GPU 着色器中的算术逻辑单元（ALUs）实际执行计算任务的时间的性能指标。该指标反映了 GPU 在图形渲染过程中 ALU 的利用效率。 Time Compute， 是一个用于衡量 GPU 在图形渲染过程中用于计算操作的总时间的性能指标。这个指标反映了 GPU 执行着色器计算任务的时间开销，尤其是在处理复杂的计算着色器（Compute Shaders）时。 Time EFUs Working， 是一个用于衡量 GPU 中的执行功能单元（Execution Functional Units, EFUs）实际用于执行计算任务的时间的性能指标。该指标反映了 GPU 在处理着色器程序时，EFUs 的利用效率。EFUs 是 GPU 中专门用于执行各种类型运算的单元，包括整数运算、浮点运算和其他特定功能的计算。EFUs 可以被视为 ALUs 的更广泛类别，负责处理着色器中的各种计算任务。 Time Shading Fragments， 是一个用于衡量 GPU 在处理和渲染片段（Fragment）时所花费的总时间的性能指标。这个指标主要反映了 GPU 在片段着色阶段的计算效率和性能。 Time Shading Vertices， 是一个用于衡量 GPU 在处理和渲染顶点（Vertex）时所花费的总时间的性能指标。这个指标主要反映了 GPU 在顶点着色阶段的计算效率和性能。 Wave Context Occupancy， 是一个用于衡量 GPU 在执行着色器程序时，波前（Wavefront 或 Warp）上下文占用的效率指标。该指标反映了 GPU 在处理并发执行的着色器线程时，资源的利用情况和计算效率。 波前（Wavefront/Warp）：在现代 GPU 中，计算通常是以波前或 Warp 的形式进行的。一个波前包含多个线程（通常是 32 个或 64 个），这些线程同时执行相同的指令，但可以在不同的数据上操作。波前的并行处理能够提高计算效率。 上下文占用：Wave Context Occupancy 指的是在某一时间段内，实际活动的波前线程数量与理论上可以支持的最大线程数量的比率。这个值通常以百分比表示，反映了 GPU 在执行着色器时的并行利用程度。 ALU/Fragment， 是一个用于衡量每个片段（Fragment）处理所需的算术逻辑单元（ALUs）操作数量的性能指标。这个指标反映了在片段着色阶段中计算的复杂性和资源的利用程度。 Fragment ALU Instructions/Sec， 是一个用于衡量 GPU 每秒钟处理的片段着色器中的算术逻辑单元（ALU）指令数量的性能指标。这个指标反映了 GPU 在片段着色阶段的计算性能和吞吐量。 Fragments Shaded， 是一个用于衡量 GPU 在特定时间段内处理和着色的片段（Fragment）数量的性能指标。这个指标反映了 GPU 在片段着色阶段的工作负载和效率。 Textures/Fragment， 是一个用于衡量在片段着色过程中，每个片段所涉及的纹理样本（Texture Samples）数量的性能指标。这个指标反映了片段着色器在处理图形时对纹理的访问频率和复杂性。 Textures/Vertex， 是一个用于衡量在顶点着色过程中，每个顶点所涉及的纹理样本（Texture Samples）数量的性能指标。这个指标反映了顶点着色器在处理图形时对纹理的访问频率和复杂性。 Vertex Instructions/Second， 是一个用于衡量 GPU 每秒钟处理的顶点着色器中的指令数量的性能指标。这个指标反映了 GPU 在顶点处理阶段的计算性能和吞吐量。 Vertices Shaded/Second， 是一个用于衡量 GPU 每秒钟处理和着色的顶点（Vertices）数量的性能指标。这个指标反映了 GPU 在顶点着色阶段的工作负载和效率。 Instruction Cache Miss， 是一个用于衡量 GPU 在执行指令时缓存未命中（Cache Miss）的次数的性能指标。这一指标反映了指令缓存的效率，以及对 GPU 性能的潜在影响。 Stalled on System Memory， 是一个用于衡量 GPU 在执行过程中由于访问系统内存而导致的停顿（Stall）次数或持续时间的性能指标。这一指标反映了 GPU 访问系统内存时的延迟和瓶颈程度。 Texture Fetch Stall， 是一个用于衡量 GPU 在进行纹理获取（Texture Fetch）时由于等待纹理数据而导致的停顿（Stall）次数或持续时间的性能指标。这一指标反映了纹理访问过程中的延迟和可能的瓶颈。 Texture L1 Miss， 是一个用于衡量 GPU 在访问一级纹理缓存（L1 Cache）时未命中的次数的性能指标。这个指标反映了纹理访问过程中的缓存效率，以及可能对 GPU 性能造成的影响。 Vertex Fetch Stall， 是一个用于衡量 GPU 在进行顶点获取（Vertex Fetch）时由于等待顶点数据而导致的停顿（Stall）次数或持续时间的性能指标。这一指标反映了顶点访问过程中的延迟和潜在的瓶颈。 L1 Texture Cache Miss Per Pixel， 是一个用于衡量每个像素的 L1 纹理缓存未命中（Miss）次数。这一指标反映了在图形渲染过程中，GPU 在处理每个像素时从 L1 纹理缓存未命中的情况，进而影响渲染效率。 Rx Bytes (TCP)， 是一个用于衡量通过 TCP 协议接收的字节数的性能指标。这个指标反映了系统在一段时间内通过网络接收的 TCP 数据量。Rx: Receive（接收） Tx Bytes (TCP)， 是一个用于衡量通过 TCP 协议发送的字节数的性能指标。这个指标反映了系统在一段时间内通过网络发送的 TCP 数据量。Tx: Transmit（发送） Xcode Instruments Xcode Instruments 是一个功能强大的性能分析和调试工具，广泛用于iOS和macOS应用的开发过程。它提供多种工具来帮助开发者分析应用的性能、内存占用、能源使用等。Instruments主要有以下几个模块： Time Profiler 功能：分析代码的执行时间，定位性能瓶颈。 主要参数： CPU Usage: 显示 CPU 的使用情况。 Call Tree: 展示方法调用的层级结构。 Self (%): 每个方法自身消耗的 CPU 时间百分比。 Total (%): 包括调用链中所有方法消耗的总时间百分比。 Hide System Libraries: 隐藏系统方法，仅显示用户代码。 Sampling Interval: 设置采样间隔时间，默认值通常为 1ms。 Allocations 功能：跟踪内存分配，帮助发现内存泄漏和高内存消耗。 主要参数： Persistent Bytes: 持久占用的内存字节数。 Transient Bytes: 瞬时分配的内存字节数。 Heap Growth: 堆内存增长趋势。 Address: 内存分配的地址。 Category: 内存分配的类型（如堆分配、栈分配等）。 Leaks 功能：检测内存泄漏，定位泄漏的对象。 主要参数： Leaked Bytes: 泄漏的内存字节数。 Responsible Frame: 泄漏对象的调用栈。 Process: 泄漏发生的进程。 Energy Log 功能：分析应用的能源使用情况，优化电量消耗。 主要参数： Energy Impact: 显示应用的能源影响得分（数值越高越耗电）。 CPU Activity: 处理器活动状态。 Network Activity: 网络使用情况。 Disk I/O: 磁盘读写操作。 Core Animation 功能：分析界面渲染性能，优化动画效果。 主要参数： FPS (Frames Per Second): 显示当前帧率，理想值为 60 FPS。 Animation Jank: 记录动画的卡顿次数。 Render Server: 图形渲染服务的资源消耗。 System Trace 功能：全系统级别的性能分析，包括 CPU、GPU、内存和线程的使用情况。 主要参数： Thread State: 每个线程的状态（如运行中、等待中）。 CPU Load: 显示每个核的负载情况。 System Calls: 系统调用的次数和耗时。 Network 功能：分析网络请求和响应，优化网络使用。 主要参数： Request Duration: 网络请求的持续时间。 Response Size: 响应的数据大小。 Domain: 请求的域名。 Protocol: 网络协议（如 HTTP、HTTPS）。 Disk I/O 功能：分析磁盘读写性能，定位高频读写操作。 主要参数： Read/Write Operations: 读写操作的数量。 Bytes Read/Written: 读写的数据字节数。 File Path: 文件路径。 Threads 功能：分析线程的活动，优化多线程使用。 主要参数： Active Threads: 活跃线程的数量。 Blocked Threads: 阻塞线程的数量。 Thread Name: 线程的名称。 Context Switches: 线程上下文切换的次数。 Metal System Trace 功能 分析 Metal API 的调用链及执行效率。 监控 GPU 和 CPU 在渲染管道中的任务分配。 优化命令缓冲区（Command Buffer）及绘制调用（Draw Calls）。 主要参数 Command Buffers 显示每个命令缓冲区的执行时间和内容。 Start/End Time：缓冲区的执行起止时间。 Execution Time：缓冲区执行所用的时间。 Command Encoder：命令编码器的类型（如渲染、计算、拷贝）。 GPU Activity Active Time：GPU 执行命令的活跃时间。 Idle Time：GPU 空闲时间，用于识别是否存在 GPU 瓶颈。 Draw Calls 每帧的绘制调用次数。 绘制调用越多，GPU 处理负担越重，需尽量减少。 Shader Execution Vertex Shader Time：顶点着色器的执行时间。 Fragment Shader Time：片段着色器的执行时间。 Compute Shader Time：计算着色器的执行时间。 Resource Usage Texture Bindings：绑定到渲染管道的纹理数量。 Buffer Uploads：上传到 GPU 的缓冲区大小。 Heap Allocation：Metal 堆内存分配情况。 优化建议 合并绘制调用（Batching），减少 Draw Calls 的次数。 使用压缩纹理格式（如 ASTC）优化 GPU 内存和带宽。 优化着色器代码，避免复杂的动态分支（如 if 和 for）。 减少命令缓冲区的频繁切换。 GPU Driver 功能 分析 GPU 渲染任务的耗时。 检测 Metal 应用的 GPU 瓶颈，如高频的纹理切换或缓冲区更新。 识别帧延迟（Frame Latency）问题。 主要参数 Frame Latency 每一帧从命令提交到 GPU 渲染完成的延迟时间。 较高的延迟可能表示资源竞争或命令缓冲区堵塞。 GPU Load GPU 的利用率，用于识别是否存在过载。 如果 GPU 长时间负载接近 100%，需优化渲染任务。 Fragment Processing Time 片段着色器的处理时间，用于分析像素级渲染任务。 Vertex Processing Time 顶点着色器的处理时间，通常用于分析模型的复杂性。 Resource Binding Texture Binding Count：绑定的纹理数量。 Buffer Binding Count：绑定的缓冲区数量。 优化建议 控制纹理和缓冲区的动态切换频率，尽量复用资源。 减少复杂的几何数据，优化顶点和索引缓冲区。 提前上传资源到 GPU，减少渲染时的内存访问延迟。 OpenGL ES Analyzer 功能： 捕获 OpenGL API 调用并分析调用性能。 检测渲染管道中的瓶颈。 主要参数： Draw Calls: 渲染调用的次数。 Shader Compilation Time: 着色器编译的耗时。 State Changes: OpenGL 状态切换的次数。 Framebuffer Switches: 帧缓冲区切换的次数。 参考 电脑110 华硕B760主板说明书 主板北桥芯片为何消失了？南桥消失了也不奇怪 【从零开始：现代PC】第三章：主板 浅谈智能手机硬件原理与常识一：性能篇 手机硬件科普 10分钟掌握手机配置知识（纯干货） 小米14 Pro拆解 掌中核心——手机SOC基础知识科普 Snapdragon 8 Gen 3 Mobile Platform 如何看待ARM将要求使用ARM CPU后必须绑捆使用其它ARM产品（GPU、NPU、ISP等）？ 人们常说的ARM究竟是什么意思？ 手机Gpu参数和架构 手机gpu型号排名 嵌入式 NPU 发展概况 手机主流存储器件的分析与发展 了解如何使用 Unity 和 Arm 分析工具解决移动端游戏性能问题 【技术精讲】游戏CPU性能分析及工具 Mobile App Performance Testing: Tools and Checklist Armv8/armv9架构入门指南 ARM Compiler armasm User Guide Arm GPU Best Practices Developer Guide ARM UAL Guide ARM Assembly Language Tools RASPBERRY PI ASSEMBLER System V Application Binary Interface - DRAFT - 24 April 2001 NDK 使用入门 Getting Started with the LLVM System 详解三大编译器：gcc、llvm 和 clang 人人都应该知道的CPU缓存运行效率 ARM 版的Clang的使用 Linux 内核源码 计算机那些事(4)——ELF文件结构 Unity Manual - Analysis","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"优化","slug":"优化","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"游戏后处理效果","slug":"Unity/Graphics/游戏后处理效果","date":"2024-04-21T01:46:04.000Z","updated":"2025-04-26T11:06:24.130Z","comments":true,"path":"2024/04/21/Unity/Graphics/游戏后处理效果/","link":"","permalink":"http://yoursite.com/2024/04/21/Unity/Graphics/%E6%B8%B8%E6%88%8F%E5%90%8E%E5%A4%84%E7%90%86%E6%95%88%E6%9E%9C/","excerpt":"后处理是指，在正常渲染管线结束后，对渲染出来的结果进行加工，以此来模拟各种效果。 颜色 颜色(color) 对应电磁波的可见光波段，是被后期处理的波长信息。颜色既是物体的客观属性——确定的波长，又带有大脑的主观属性——不同的个体对特定波长的电磁波敏感程度不同，感受的颜色也有差异。 为了表示色彩，人们建立了一维、二维、三维甚至四维空间坐标模型，这些色彩模型称为颜色空间。颜色空间多达百种，常见的有如下5种。 颜色数据表示（Linear，LogC） 在影像制作和后期处理中，Linear、LogC和Gamma是三种关键的概念，它们描述了不同的图像数据处理和表示方法。理解它们之间的区别对于正确处理图像和视频数据非常重要。 Linear（线性）: Linear指的是一种线性响应的色彩空间，其中记录的图像亮度值直接对应于场景中的实际光照强度。在线性色彩空间中，如果场景中一个区域的光照强度是另一个区域的两倍，那么记录的数值也会是两倍。这种表示方式使得图像的色彩混合和处理在数学上更加直接和简单，但由于人眼对亮度的感知是非线性的，线性空间通常不适用于最终图像的显示。 LogC（对数） LogC是ARRI摄影机特有的一种对数色彩空间，它旨在通过对数曲线来模拟人眼对亮度的非线性感知，使得在有限的比特深度下能够捕获更宽的动态范围。LogC色彩空间特别适合于记录高动态范围的场景，因为它能够有效地保留高光和阴影中的细节。然而，LogC图像在没有经过适当的色彩校正或应用LUT（查找表）之前，看起来会显得非常低饱和和低对比度。 Gamma Gamma校正是一种用于调整图像亮度的非线性操作，旨在使图像在特定显示设备上的显示更符合人眼的感知特性。Gamma校正可以被视为在图像数据和最终显示之间的一个桥梁，用于调整图像的整体亮度和对比度。不同的显示设备和媒体标准可能会使用不同的Gamma值，如sRGB标准使用大约2.2的Gamma值。 总结 Linear色彩空间： 最适合图像的处理和合成，需要在最终输出前转换到适合观看的色彩空间。 LogC色彩空间： 用于捕获和记录高动态范围的图像，需要在后期处理中进行色彩校正。 Gamma校正： 用于调整图像的显示，以符合人眼对亮度的非线性感知和特定显示设备的要求。","text":"后处理是指，在正常渲染管线结束后，对渲染出来的结果进行加工，以此来模拟各种效果。 颜色 颜色(color) 对应电磁波的可见光波段，是被后期处理的波长信息。颜色既是物体的客观属性——确定的波长，又带有大脑的主观属性——不同的个体对特定波长的电磁波敏感程度不同，感受的颜色也有差异。 为了表示色彩，人们建立了一维、二维、三维甚至四维空间坐标模型，这些色彩模型称为颜色空间。颜色空间多达百种，常见的有如下5种。 颜色数据表示（Linear，LogC） 在影像制作和后期处理中，Linear、LogC和Gamma是三种关键的概念，它们描述了不同的图像数据处理和表示方法。理解它们之间的区别对于正确处理图像和视频数据非常重要。 Linear（线性）: Linear指的是一种线性响应的色彩空间，其中记录的图像亮度值直接对应于场景中的实际光照强度。在线性色彩空间中，如果场景中一个区域的光照强度是另一个区域的两倍，那么记录的数值也会是两倍。这种表示方式使得图像的色彩混合和处理在数学上更加直接和简单，但由于人眼对亮度的感知是非线性的，线性空间通常不适用于最终图像的显示。 LogC（对数） LogC是ARRI摄影机特有的一种对数色彩空间，它旨在通过对数曲线来模拟人眼对亮度的非线性感知，使得在有限的比特深度下能够捕获更宽的动态范围。LogC色彩空间特别适合于记录高动态范围的场景，因为它能够有效地保留高光和阴影中的细节。然而，LogC图像在没有经过适当的色彩校正或应用LUT（查找表）之前，看起来会显得非常低饱和和低对比度。 Gamma Gamma校正是一种用于调整图像亮度的非线性操作，旨在使图像在特定显示设备上的显示更符合人眼的感知特性。Gamma校正可以被视为在图像数据和最终显示之间的一个桥梁，用于调整图像的整体亮度和对比度。不同的显示设备和媒体标准可能会使用不同的Gamma值，如sRGB标准使用大约2.2的Gamma值。 总结 Linear色彩空间： 最适合图像的处理和合成，需要在最终输出前转换到适合观看的色彩空间。 LogC色彩空间： 用于捕获和记录高动态范围的图像，需要在后期处理中进行色彩校正。 Gamma校正： 用于调整图像的显示，以符合人眼对亮度的非线性感知和特定显示设备的要求。 颜色空间 XYZ颜色空间 CIE XYZ色彩空间是一种基于人类视觉响应的色彩模型，于1931年由国际照明委员会（CIE，Commission Internationale de l'Éclairage）提出。它是第一个基于人类视觉实验数据的数学定义色彩空间，旨在提供一种不依赖于特定设备的色彩表示方法，从而允许在不同设备和媒介之间准确地转换和比较颜色。 三色刺激值 三色刺激值并不是指人类眼睛对短、中和长波（S、M和L）的反应，而是一组称为X、Y和Z的值，约略对应于红色、绿色和蓝色（但要留意X、Y和Z值并不是真的看起来是红、绿和蓝色，而是从红色、绿色和蓝色导出来的参数），并使用CIE 1931 XYZ颜色匹配函数来计算。两个由多种不同波长的光混合而成的光源可以表现出同样的颜色，这叫做“同色异谱”（metamerism）。当两个光源对标准观察者（CIE 1931标准色度观察者）有相同的视现颜色的时候，它们即有同样的三色刺激值，而不管生成它们的光的光谱分布如何。 xy色度图 因为人类眼睛有响应不同波长范围的三种类型的颜色传感器，所有可视颜色的完整绘图是三维的。但是颜色的概念可以分为两部分：明度和色度。例如，白色是明亮的颜色，而灰色被认为是不太亮的白色。换句话说，白色和灰色的色度是一样的，而明度不同。 CIE Yxy色彩空间故意设计得Y参数是颜色的明度或亮度的测量。颜色的色度接着通过两个导出参数x和y来指定，它们是所有三色刺激值X、Y和Z的函数,规范化下的三个值中的两个： \\[ x = \\frac{X}{X+Y+Z} \\\\ y = \\frac{Y}{X+Y+Z} \\\\ z = \\frac{Z}{X+Y+Z} = 1-x-y \\] 导出的色彩空间用x, y, Y来指定，它叫做CIE xyY色彩空间并在实践中广泛用于指定颜色。 X和Z三色刺激值可以从色度值x和y与Y三色刺激值计算回来： \\[ X = \\frac{Y}{y} x \\\\ Z = \\frac{Y}{y}(1-x-y) \\] xy色度图 具体公式，参见：CIE 1931 XYZ色彩空间 LMS颜色空间 LMS颜色空间基于人眼对光的生理响应，其中L、M、S分别代表长波长（红色）、中波长（绿色）和短波长（蓝色）的锥状细胞。这三种类型的锥细胞是人眼感知颜色的基础。LMS颜色空间尝试模拟这种生理机制，以便在图像处理、色彩校正和视觉研究中更准确地反映和操作色彩。 - L（Long wavelengths）：长波锥细胞，主要对红色光敏感。 - M（Medium wavelengths）：中波锥细胞，主要对绿色光敏感。 - S（Short wavelengths）：短波锥细胞，主要对蓝色光敏感。 在图像处理和色彩管理中，将RGB或其他色彩空间转换到LMS色彩空间可以帮助模拟和理解人类的色彩感知过程，进而进行更精确的色彩校正和调整。 RGB颜色空间 RGB色彩空间基于三原色学说：视网膜存在三种视锥细胞，分别含有对红、绿、蓝三种光线敏感的视色素，当一定波长的光线作用于视网膜时，以一定的比例使三种视锥细胞分别产生不同程度的兴奋，这样的信息传至大脑中枢就产生某一种颜色的感觉。 RGB颜色空间 RGB颜色模型的优点是： 易于理解； 便于硬件实现，现代显示屏一般基于RGB模型； 引入位分辨率(颜色深度) ，指一个像素中，每个颜色分量的比特数。位分辨率决定了色彩等级，例如8位颜色深度，每个颜色分量就有256种可能。 RGB颜色模型的缺点是： 三个分量均用于表示色调，即如果改变某一个分量的数值，这个像素的颜色就发生了改变。在颜色定位等工程中，使用RGB模型就要同时考虑、、三个变量，较为复杂。 CMY/CMYK颜色空间 CMY是工业印刷采用的颜色空间。它与RGB对应。简单的类比RGB来源于是物体发光，而CMY是依据反射光得到的。具体应用如打印机：一般采用四色墨盒，即CMY加黑色墨盒 CMY是青（Cyan）、洋红或品红（Magenta）和黄（Yellow）三种颜色，由于三原色得不到纯黑色，CMYK则是打印时加上墨色(black ink)，例如青色可以通过蓝色和绿色光相加得到，则白色通过青色时，没有红色分量。底色为白色进行色彩减法可以得到各种颜色。 Lab颜色空间 Lab色彩空间基于人对颜色的感觉设计，具有感知均匀性(Perceptual Uniform) ，即如果参数L、a、b变化幅度一样，则人视觉上的变化幅度也差不多。 Lab颜色空间 在Lab模式下，通道向量由三个部分组成： 亮度(Lightness) a颜色分量：代表从绿色到红色的分量 b颜色分量：代表从蓝色到黄色的分量 Lab同样容易调整——调节亮度仅需关注L通道，调节色彩平衡仅需关注a和b通道。此外，Lab还具有色域广阔、设备无关等性质。 HSV/HSB颜色空间 HSV颜色空间比RGB更接近人们对彩色的感知经验，非常直观地表达颜色的色调、饱和度和明暗程度。 在HSV模式下，通道向量由三个部分组成： 色调、色相(Hue) ：与光波的波长有关，它表示人的感官对不同颜色的感受，如红色、绿色、蓝色等，它也可表示一定范围的颜色，如暖色、冷色等。 饱和度(Saturation) ：表示颜色的纯度，纯光谱色是完全饱和的，加入白光会稀释饱和度。饱和度越大，颜色看起来就会越鲜艳，反之亦然。 明度(Value, Brightness) ：指某种颜色的透光量。与亮度(Lightness) 不同，亮度特指被白光稀释的浓度，任何颜色的高亮都趋于白色，但每种高明度颜色都不同。 HSV/HSB颜色空间 由于HSV可以单独处理色调值，而不会影响到明度和饱和度；或者单独改变明度、饱和度而不影响颜色本身，因此在图像处理中，HSV常用于颜色定位追踪、提取色彩直方图等。 HSV模型的缺点是目前很少有硬件支持，需要从RGB或其他色彩空间进行转换。 HSI/HSL颜色空间 HSV颜色空间比RGB更接近人们对彩色的感知经验，非常直观地表达颜色的色调、饱和度和明暗程度。 在HSI模式下，通道向量由三个部分组成： 色调H(Hue)： 与光波的波长有关，它表示人的感官对不同颜色的感受，如红色、绿色、蓝色等，它也可表示一定范围的颜色，如暖色、冷色等。 饱和度S(Saturation)： 表示颜色的纯度，纯光谱色是完全饱和的，加入白光会稀释饱和度。饱和度越大，颜色看起来就会越鲜艳，反之亦然。 亮度I(Intensity, Lightness)： 对应成像亮度和图像灰度，是颜色的明亮程度。 HSI/HSL颜色空间 由于HSV可以单独处理色调值，而不会影响到明度和饱和度；或者单独改变明度、饱和度而不影响颜色本身，因此在图像处理中，HSV常用于颜色定位追踪、提取色彩直方图等。 HSV模型的缺点是目前很少有硬件支持，需要从RGB或其他色彩空间进行转换。 HSV/HSB与HSI/HSL颜色空间对比 HSV和HSL二者都把颜色描述为在圆柱坐标系内的点，这个圆柱的中心轴底部为黑色，顶部为白色，而它们中间是灰色渐变，绕这个轴的角度对应于“色相”，到这个轴的距离对应于“饱和度”，而沿着这个轴的高度对应于“明度”或“亮度”。 这两种表示在目的上类似，但在方法上有区别。二者在数学上都是圆柱，但HSV（色相、饱和度、明度）在概念上可以被认为是颜色的倒圆锥体（黑点在下顶点，白色在上底面圆心），HSL在概念上表示了一个双圆锥体和圆球体（白色在上顶点，黑色在下顶点，最大横切面的圆心是半程灰色）。注意尽管在HSL和HSV中“色相”指称相同的性质，它们的“饱和度”的定义是明显不同的。 因为HSL和HSV是设备依赖的RGB的简单变换，(h, s, l)或 (h, s, v)三元组定义的颜色依赖于所使用的特定红色、绿色和蓝色“加法原色”。每个独特的RGB设备都伴随着一个独特的HSL和HSV空间。但是 (h, s, l)或 (h, s, v)三元组在被约束于特定RGB空间比如sRGB的时候就更明确了。 HSV模型在1978年由埃尔维·雷·史密斯创立，它是三原色光模式的一种非线性变换，如果说RGB加色法是三维直角座标系，那么HSV模型就是球面座标系。 HSV和HSI对比 HSV和HSI对比 HSV出现的动机 大多数电视机、显示器、投影仪通过将不同强度的红、绿、蓝色光混合来生成不同的颜色，这就是RGB三原色的加色法。通过这种方法可以在RGB色彩空间生成大量不同的颜色，然而，这三种颜色分量的取值与所生成的颜色之间的联系并不直观。 艺术家有时偏好使用HSV或HSL而不选择三原色光模式（即RGB模型）或 印刷四分色模式（即CMYK模型），因为它类似于人类感觉颜色的方式，具有较强的感知度。RGB和CMYK分别是加法原色和减法原色模型，以原色组合的方式定义颜色，而HSV以人类更熟悉的方式封装了关于颜色的信息：“这是什么颜色？深浅如何？明暗如何？”。 但是色彩属性和物理学中的光谱并不是完全对应的，物理学的人类可见光谱是有两个端点的直线形，并不能形成一个环。当然每种颜色都可以找到相应的光波长，但都有一个范围，并不是单一的波长。明度一般和具体某种颜色的光波能量相当，但和整个光谱的能量无关（因为每种波长的光的能量都不相同）。HSV颜色空间在技术上不支持到辐射测定中测量的物理能量谱密度的一一映射。所以一般不建议做在HSV坐标和物理光性质如波长和振幅之间的直接比较。 抗锯齿（Anti-aliasing, AA） 有很多抗锯齿技术，都是在后处理阶段进行的，在分析URP后处理源码前，先了解一下抗锯齿相关的技术。 抗锯齿（Anti-aliasing, AA）技术是用来减少和消除图形渲染中的锯齿现象，即在边缘和细节部分出现的不平滑、断续的像素效果。锯齿通常发生在边缘的像素没有完全覆盖物体的情况下，导致视觉上的不连续性。以下是一些常见的抗锯齿技术及其原理： 全屏抗锯齿（FSAA，Full Screen Anti-Aliasing） 增加渲染分辨率，然后将图像缩小到目标分辨率。通过这种方式，每个最终像素中包含了更多的信息，从而平滑了边缘。FSAA是一种简单直接的方法，但对性能的影响较大，因为它需要渲染更多的像素。 全屏抗锯齿（FSAA，Full Screen Anti-Aliasing）是一种较早的抗锯齿技术，其基本思想是在比最终输出分辨率更高的分辨率上渲染场景，然后将这个高分辨率的图像缩小（下采样）到目标分辨率，以此来减少锯齿效果。虽然FSAA概念简单，但直接实现（如超采样抗锯齿，SSAA）在性能上可能非常昂贵，因为它要求渲染更多的像素。以下是FSAA在图形管线和GPU中的一般实现原理： 图形管线中的FSAA实现 渲染分辨率调整：首先，渲染目标（RenderTarget）的分辨率被设置为目标显示分辨率的倍数。例如，如果目标是1080p（1920x1080），则可能在4K（3840x2160）分辨率上进行渲染，这实质上是对每个维度进行了2倍的超采样。 场景渲染：图形管线按照这个高分辨率渲染整个场景，包括3D模型、纹理、光照等。这个过程涉及标准的渲染步骤，如几何处理、栅格化、片段着色等，但每个步骤处理的像素数量明显增加。 下采样：渲染完成后，图形管线执行一个下采样（或称为图像缩放）步骤，将高分辨率的渲染结果缩减到目标分辨率。这个过程通常使用滤波算法（如双线性或双三次滤波）来合并像素，以保留细节的同时减少锯齿。 GPU硬件中的FSAA支持 硬件加速的下采样：现代GPU提供了硬件加速的图像缩放功能，可以高效地执行高到低分辨率的图像下采样。这有助于减轻FSAA对性能的影响，尤其是在下采样步骤。 专用的渲染缓冲区：为了支持高分辨率渲染，GPU可能提供专用的高容量渲染缓冲区。这些缓冲区设计用来存储更多的像素数据，并且能够快速进行图像处理操作。 实现细节与考虑 性能考量：FSAA（特别是当直接以超采样的形式实现时）对GPU的计算能力和内存带宽要求较高。由于需要渲染更多的像素，这可能导致帧率下降，尤其是在复杂场景和高分辨率设置下。 图像质量：FSAA可以提供非常高质量的抗锯齿效果，因为它不仅影响场景的边缘，也影响场景内部的细节，如纹理的平滑度。 应用场景：由于其性能成本，FSAA（特别是SSAA）可能不适合所有应用。在性能敏感的应用中（如VR或某些游戏），可能会考虑使用其他抗锯齿技术，如MSAA或FXAA。 超采样抗锯齿（SSAA，Super-Sample Anti-Aliasing） 类似于FSAA，SSAA通过在较高分辨率下渲染图像，然后缩小到目标分辨率来实现抗锯齿。与FSAA不同的是，SSAA通常采用更高级的采样算法和滤波技术，以获得更好的图像质量。SSAA对性能的影响非常大，通常只在对图像质量有极高要求的情况下使用。 多重采样抗锯齿（MSAA，Multi-Sample Anti-Aliasing） 在边缘区域采样多个像素，然后将这些采样的颜色值平均化以确定最终像素的颜色。MSAA专注于边缘，不会对整个场景的每个像素进行多重采样，因此相比FSAA，它对性能的影响较小。 多重采样抗锯齿（MSAA）是一种在图形管线和GPU硬件层面实现的抗锯齿技术，旨在减少边缘的锯齿现象而对性能影响尽可能小。MSAA的工作原理涉及多个阶段，从几何处理到像素着色，最后到像素写入帧缓冲区。下面是MSAA在图形管线和GPU中的实现细节： 几何处理阶段 多重采样缓冲区准备：在GPU中，为实现MSAA，首先需要创建一个多重采样缓冲区。这个缓冲区相比常规的颜色缓冲区有更多的样本点，用于存储每个像素的多个颜色值和深度信息。例如，4x MSAA意味着每个像素将包含4个独立的样本点。 几何图元的栅格化：当几何图元（如三角形）被栅格化（转换为像素网格）时，图形管线会为每个像素生成多个覆盖该像素的图元片段（片段是像素在渲染过程中的中间表示）。每个片段对应于像素内的一个样本点。 像素着色阶段 片段着色器执行：对于MSAA，片段着色器可能会被执行多次，每个样本点一次，或者根据具体实现，可能只执行一次并共享结果。不过，通常情况下，片段着色器的执行结果会存储在每个样本点中，允许细节丰富和光滑的边缘渲染。 最终像素写入 样本点合并（Resolve）：在所有相关的几何图元被处理，且每个像素的样本点都被着色后，图形管线会执行一个合并（resolve）步骤，这一步骤将每个像素内的样本点的颜色值合并成一个单一的颜色值。这通常通过简单的平均值计算完成，但也可以包括更复杂的滤波算法。 写入帧缓冲区：合并后的像素值最终被写入到帧缓冲区中，完成整个渲染过程。 GPU支持和优化 硬件级支持：现代GPU设计有专门的硬件支持MSAA，包括专用的多重采样缓冲区和高效的样本点处理机制。这种硬件级支持使得MSAA能够在保持较高渲染质量的同时，最小化对性能的影响。 内存和带宽优化：虽然MSAA增加了内存使用和带宽需求（因为需要为每个像素存储多个样本点的数据），但GPU厂商实现了多种优化技术，如压缩技术和智能缓冲区管理，以减少这些开销。 快速近似抗锯齿（FXAA，Fast Approximate Anti-Aliasing） FXAA是一种屏幕空间算法，它在图像的最终阶段处理，通过分析像素的亮度变化来识别边缘，并对边缘进行平滑处理。FXAA对性能的影响较小，实现简单，但可能会导致图像细节的轻微模糊。 快速近似抗锯齿（Fast Approximate Anti-Aliasing, FXAA）是一种屏幕空间的抗锯齿技术，旨在以较低的性能成本减少图像中的锯齿现象。与传统的多重采样抗锯齿（MSAA）相比，FXAA在图形管线的后期处理阶段实施，不需要对每个几何图元进行多次采样，因而对性能的影响较小。以下是FXAA在图形管线和GPU中的实现概览： 图形管线中的FXAA实现 渲染场景： 首先，场景被正常渲染到一个帧缓冲区中，不应用任何抗锯齿技术。 后期处理阶段： 场景渲染完成后，FXAA作为后期处理效果应用。这意味着FXAA操作是在图像已经渲染完成的基础上进行的，利用片段着色器对已经渲染好的图像进行处理。 边缘检测： FXAA首先通过分析像素颜色的局部梯度来识别图像中的边缘。这一步通常涉及比较当前像素与其邻近像素的颜色差异。 锯齿平滑： 一旦边缘被识别，FXAA算法会沿着边缘方向平滑锯齿，方法是对边缘附近的像素进行混合。这种方式旨在减少锐利边缘处的颜色梯度，从而减少视觉上的锯齿现象。 色彩校正： 在某些实现中，FXAA还可能包括对处理后图像的色彩进行微调，以保持图像的色彩真实性。 在GPU上的实现 FXAA是通过片段着色器在GPU上实现的。由于其算法主要基于像素操作，FXAA非常适合在GPU上执行，能够充分利用GPU并行处理像素的能力。实现FXAA通常涉及以下步骤： 编写FXAA着色器： 开发者会编写一个片段着色器，该着色器包含FXAA算法的实现逻辑。这个着色器会对每个像素应用FXAA算法，包括边缘检测和锯齿平滑。 应用着色器： 在渲染流程的后期处理阶段，将FXAA着色器应用于已渲染的图像。这通常通过将渲染好的场景作为纹理输入到后期处理管线，并执行FXAA着色器。 输出结果： FXAA处理后的图像被输出到屏幕或下一阶段的渲染目标。 性能优化 FXAA的设计考虑到了性能优化，通过减少算法复杂度和精心设计的着色器代码来降低对性能的影响。相比于MSAA等传统抗锯齿技术，FXAA提供了一个性能成本较低的抗锯齿解决方案，特别适用于性能敏感的应用场景。 子像素抗锯齿（SMAA，Subpixel Morphological Anti-Aliasing） SMAA是一种高级的屏幕空间抗锯齿技术，结合了MSAA、FXAA和其他技术的优点。它使用局部对比度检测来识别边缘，并使用多种策略（包括子像素级处理）来平滑边缘。SMAA旨在在保持高性能的同时提供较高的图像质量。 每种抗锯齿技术都有其优势和局限性，选择哪一种取决于特定的应用场。 子像素抗锯齿（Subpixel Morphological Antialiasing, SMAA）是一种高效且灵活的抗锯齿技术，它结合了多种抗锯齿技术的优点，如MSAA（多重采样抗锯齿）和FXAA（快速近似抗锯齿），但旨在提供更高的图像质量和更低的性能成本。SMAA通过分析图像来识别锯齿边缘，并采用各种策略对这些边缘进行平滑处理。以下是SMAA在图形管线和GPU上的一般实现流程： SMAA的实现步骤 边缘检测： SMAA首先在图像中识别出锯齿边缘。这通常通过分析颜色、亮度或深度差异来实现，以找到可能产生锯齿的像素边界。边缘检测可以使用多种算法，包括基于梯度、Sobel算子或自定义滤波器。 模式识别： 识别出边缘后，SMAA通过比较这些边缘与一系列预定义的模式（或模板）来进行分类。这些模式对应于常见的锯齿形状和分布，使SMAA能够更精确地确定如何对特定的锯齿边缘进行处理。 子像素处理： SMAA利用子像素级的信息来改善锯齿边缘的渲染质量。这涉及到对锯齿边缘附近的像素进行细微调整，以模拟更高分辨率下的图像细节和边缘平滑效果。 形态学合并： 最后，SMAA应用形态学操作来平滑边缘和合并处理过的像素，最终产生抗锯齿效果。这一步骤利用了先前的模式识别结果，以确保边缘处理既平滑又自然，减少了过度模糊或其他视觉伪像。 在GPU和图形管线中的实现 Shader实现： SMAA主要在GPU的Shader阶段实现，通常作为一系列的后处理步骤。这包括顶点Shader用于设置屏幕空间的四边形，以及片元Shader来执行边缘检测、模式识别、子像素处理和形态学合并。 后处理管线： 在现代图形API（如DirectX 11/12、Vulkan、OpenGL）中，SMAA通常作为渲染管线的后处理阶段进行，处理完所有的3D渲染和其他视觉效果后，再应用SMAA来改善最终的图像质量。 性能优化： SMAA的实现通常涉及对性能的优化，以最小化对帧率的影响。这可能包括优化Shader代码、减少不必要的纹理采样、使用高效的数据结构和算法等。此外，SMAA允许不同级别的质量设置，开发者可以根据性能需求选择合适的级别。 兼容性和灵活性 SMAA的设计允许它在各种硬件和平台上有效运行，从高端PC显卡到移动设备。这得益于它对资源的有效管理和对性能的细致优化，使其成为当前广泛使用的 时间抗锯齿（TAA，Temporal Anti-Aliasing） TAA利用连续帧之间的信息来平滑边缘和去除闪烁。它结合了当前帧和之前帧的数据，通过时间上的信息融合来减少锯齿。TAA能够提供非常平滑的图像质量，但可能引入一些运动模糊，特别是在快速移动的场景中。 时间抗锯齿（Temporal Anti-Aliasing, TAA）是一种先进的抗锯齿技术，通过结合多个帧的数据来减少锯齿，同时尽量保持图像细节。与传统的抗锯齿技术如MSAA（多重采样抗锯齿）或FSAA（全屏抗锯齿）不同，TAA不仅仅关注单一帧内的像素处理，而是利用时间维度上的信息来改善图像质量。这种方法在现代游戏和应用中越来越受欢迎，尤其是对于动态场景。 TAA的工作原理 运动矢量：TAA技术首先需要捕捉场景中对象的运动。这通过使用运动矢量图（Motion Vector Pass）来实现，运动矢量图记录了场景中每个像素点从前一帧到当前帧的移动情况。运动矢量通常由GPU的顶点着色器计算，并在渲染管线的早期阶段生成。 历史帧混合：TAA将当前帧的图像与之前帧的图像进行混合。通过对运动矢量的分析，TAA能够确定如何将当前帧的像素与历史帧的像素相结合，以减少锯齿并平滑边缘。这一步骤需要精确的运动跟踪和合适的权重分配，以确保图像的连贯性和减少运动模糊。 锐化和噪声处理：由于TAA在混合过程中可能引入模糊，因此在处理完混合后，通常会应用一定程度的锐化来保持图像的清晰度。同时，TAA也可能引入一些图案噪声，特别是在场景快速变化时，因此可能需要额外的噪声抑制步骤。 GPU和图形管线中的实现 运动矢量的生成：现代GPU提供了高效的方式来计算和存储运动矢量。这通常在顶点着色器或几何着色器中实现，通过比较当前帧和前一帧的顶点位置来生成。 图像处理和混合：TAA的混合处理主要在片段着色器和后处理阶段进行。GPU需要处理大量的纹理读取操作，包括访问当前帧的像素数据和历史帧的像素数据，然后基于运动矢量进行适当的混合。 后处理效果：TAA通常作为一系列后处理效果的一部分实现，在最终图像输出前应用。这意味着TAA可以与其他图像处理效果（如色彩校正、HDR渲染等）一起在GPU的后处理管线中执行。 考虑因素 运动估计的准确性：TAA的效果很大程度上依赖于运动矢量的准确性。不准确的运动跟踪可能导致图像拖影或其他视觉伪像。 性能和质量平衡：TAA在提升图像质量的同时，相对于其他抗锯齿技术，对性能的影响较小。但是，它需要合理的资源分配和优化，特别是在高分辨率渲染或要求高帧率的应用中。 URP后处理源码分析 在URP中后处理源码主要由三个部分组成： Volume系统, 用于管理后处理组件参数，以及参数的混合。 URP的PostProcessPasses，用于管理后处理的相关Pass(ColorGradingLutPass和PostProcessPass)，Pass会将Volume系统提供的参数设置给Shader中使用。 各种后处理Shader(PostProcessData持久化对象存储了后处理中使用的各种Shader和纹理) Volume系统 Volume系统由两个部分组成: 持久化类, 用于存储各种后处理组件的参数，核心类:VolumeProfile，VolumeComponent和VolumeParameter 运行时类, Volume表示一个后处理对象；VolumeStack记录所有Volume对象中，所有后处理组件根据权重混合后的最终结果；VolumeManager管理所有的Volume对象并在Update中，根据优先级，权重和混合距离对每个Volume中相同后处理组件的参数进行插值混合，具体的插值方式由对应的参数VolumeParameter类提供。 持久化类 VolumeProfile VolumeProfile用于管理VolumeComponent列表，标准的一些Add,Remove,TryGet等函数。 VolumeComponent VolumeComponent中包含了后处理组件中的参数(VolumeParameter对象列表)。核心函数就是Override，其作用的是同一个后处理组件中的参数进行插值。 123456789101112131415161718public virtual void Override(VolumeComponent state, float interpFactor)&#123; int count = parameters.Count; for (int i = 0; i &lt; count; i++) &#123; var stateParam = state.parameters[i]; var toParam = parameters[i]; if (toParam.overrideState) &#123; // Keep track of the override state for debugging purpose stateParam.overrideState = toParam.overrideState; stateParam.Interp(stateParam, toParam, interpFactor); &#125; &#125;&#125; VolumeParameter 后处理组件的参数，有各种类型的派生类(int， float, vector等类型，也可以根据自己需求新增派生类)，核心函数就是Interp，每种类型都有自己的Interp。 12345// float的插值函数，线性插值public sealed override void Interp(float from, float to, float t)&#123; m_Value = from + (to - from) * t;&#125; 运行时类 VolumeStack VolumeStack类记录了所有的后处理组件，也是保存最终参数插值结果的。此类的组件参数值也是最终要被设置到Shader中去的参数值。 VolumeManager VolumeManager管理了所有Volume组件和当前使用的VolumeStack对象，VolumeManager的目的就是为了将所有的Volume对象中的后处理组件的参数进行插值，并保存在VolumeStack对象中，其核心函数就是Update。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293// stack ： 保存插值混合后的结果// trigger：计算混合时使用的位置（Volume对象到此位置来计算混合），默认使用相机位置，也可以在相机组件中拖拽一个对象作为计算混的位置。// layerMask: 对此相机有效的Volume对象public void Update(VolumeStack stack, Transform trigger, LayerMask layerMask)&#123; Assert.IsNotNull(stack); // 保证所有的后处理组件已创建 CheckBaseTypes(); // 检查记录最终结果的后处理栈是否创建 CheckStack(stack); // 使用后处理组件的默认值初始化后处理栈 ReplaceData(stack, m_ComponentsDefaultState); // 计算混合的触发位置，全局的Volume对象是不会跟触发位置与Volume对象之间的距离进行插值的 bool onlyGlobal = trigger == null; var triggerPos = onlyGlobal ? Vector3.zero : trigger.position; // 获取对应层级排序后的Volume对象列表 var volumes = GrabVolumes(layerMask); // 获取Trigger对应的相机，当然也可以能没有相机（直接拖拽的一个Trigger对象） Camera camera = null; if (!onlyGlobal) trigger.TryGetComponent&lt;Camera&gt;(out camera); // 遍历所有的Volume，并对他们进行插值，全局的Volume是没有距离插值的 foreach (var volume in volumes) &#123; if (volume == null) continue;#if UNITY_EDITOR // 跳过场景视图中当前显示的场景中不存在的Volume if (!IsVolumeRenderedByCamera(volume, camera)) continue;#endif // 跳过哪些被禁用，没有关连VolumeProfile或者权重为0的Volume对象 if (!volume.enabled || volume.profileRef == null || volume.weight &lt;= 0f) continue; // 全局Volume没有距离的显示，都生效，本地的则需要根据它与trigger的距离来计算插值，全局Volume只需要权重插值即可 if (volume.isGlobal) &#123; OverrideData(stack, volume.profileRef.components, Mathf.Clamp01(volume.weight)); continue; &#125; // 全局的就继续下一个Volume对象 if (onlyGlobal) continue; // 如果不是全局的，又没有Collider对象，则不能根据Trigger与Volume对象是否相交，来计算插值权重，Trigger越靠近Volume对象上Collider的范围影响越大，权重也就越大。没有Collider则无法计算，所以也排除 var colliders = m_TempColliders; volume.GetComponents(colliders); if (colliders.Count == 0) continue; // 查找最新的Collider对象 float closestDistanceSqr = float.PositiveInfinity; foreach (var collider in colliders) &#123; if (!collider.enabled) continue; var closestPoint = collider.ClosestPoint(triggerPos); var d = (closestPoint - triggerPos).sqrMagnitude; if (d &lt; closestDistanceSqr) closestDistanceSqr = d; &#125; colliders.Clear(); // 计算Volume的混合计算的平方 float blendDistSqr = volume.blendDistance * volume.blendDistance; // 如果最近的距离都大于混合距离，则说明，Trigger位置和Volume对象的Collider不相交，则此Volume对于Trigger来所不生效，所有排除 if (closestDistanceSqr &gt; blendDistSqr) continue; // 计算相交程度 float interpFactor = 1f; if (blendDistSqr &gt; 0f) interpFactor = 1f - (closestDistanceSqr / blendDistSqr); // 根据相交程度和Volume的权重值，确定最终的权重值（相交程度因子 x Volume的权重值） OverrideData(stack, volume.profileRef.components, interpFactor * Mathf.Clamp01(volume.weight)); &#125;&#125; Volume Volume代表一个后处理对象，此对象引用了VolumeProfile，VolumeProfile对象则记录了使用的后处理组件。Volume处理提供了VolumeProfile字段，还有isGlobal表示是全局还是本地Volume，weight表示权重，priority表示优先级（进行插值的先后顺序），blendDistance混合距离此字段仅用于本地Volume对象。 PostProcessPasses和PostProcessPass PostProcessPasses是PostProcessPass的一个包装类，主要包含三个Pass: ColorGradingLutPass colorGradingLutPass 在后处理之前，渲染LUT纹理。 PostProcessPass postProcessPass 对启用的后处理组件，设置参数并根据参数处理图片。 PostProcessPass finalPostProcessPass 所有后处理效果处理完后，对最后的图像再进行一次缩放，抗锯齿（FXAA）和颜色转换等。 ColorGradingLutPass ColorGradingLutPass用于生成LUT纹理。核心函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135private static void ExecutePass(ScriptableRenderContext context, PassData passData, ref RenderingData renderingData, RTHandle internalLutTarget)&#123; // 生LDR和HDR的LUT纹理的材质(LutBuilderLdr.shader和LutBuilderHdr.shader) var cmd = renderingData.commandBuffer; var lutBuilderLdr = passData.lutBuilderLdr; var lutBuilderHdr = passData.lutBuilderHdr; var allowColorGradingACESHDR = passData.allowColorGradingACESHDR; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.ColorGradingLUT))) &#123; // 获取所有调色组件参数 var stack = VolumeManager.instance.stack; var channelMixer = stack.GetComponent&lt;ChannelMixer&gt;(); var colorAdjustments = stack.GetComponent&lt;ColorAdjustments&gt;(); var curves = stack.GetComponent&lt;ColorCurves&gt;(); var liftGammaGain = stack.GetComponent&lt;LiftGammaGain&gt;(); var shadowsMidtonesHighlights = stack.GetComponent&lt;ShadowsMidtonesHighlights&gt;(); var splitToning = stack.GetComponent&lt;SplitToning&gt;(); var tonemapping = stack.GetComponent&lt;Tonemapping&gt;(); var whiteBalance = stack.GetComponent&lt;WhiteBalance&gt;(); ref var postProcessingData = ref renderingData.postProcessingData; bool hdr = postProcessingData.gradingMode == ColorGradingMode.HighDynamicRange; ref CameraData cameraData = ref renderingData.cameraData; // 选择材质 var material = hdr ? lutBuilderHdr : lutBuilderLdr; // 准备Shader中使用的参数 var lmsColorBalance = ColorUtils.ColorBalanceToLMSCoeffs(whiteBalance.temperature.value, whiteBalance.tint.value); var hueSatCon = new Vector4(colorAdjustments.hueShift.value / 360f, colorAdjustments.saturation.value / 100f + 1f, colorAdjustments.contrast.value / 100f + 1f, 0f); var channelMixerR = new Vector4(channelMixer.redOutRedIn.value / 100f, channelMixer.redOutGreenIn.value / 100f, channelMixer.redOutBlueIn.value / 100f, 0f); var channelMixerG = new Vector4(channelMixer.greenOutRedIn.value / 100f, channelMixer.greenOutGreenIn.value / 100f, channelMixer.greenOutBlueIn.value / 100f, 0f); var channelMixerB = new Vector4(channelMixer.blueOutRedIn.value / 100f, channelMixer.blueOutGreenIn.value / 100f, channelMixer.blueOutBlueIn.value / 100f, 0f); var shadowsHighlightsLimits = new Vector4( shadowsMidtonesHighlights.shadowsStart.value, shadowsMidtonesHighlights.shadowsEnd.value, shadowsMidtonesHighlights.highlightsStart.value, shadowsMidtonesHighlights.highlightsEnd.value ); var (shadows, midtones, highlights) = ColorUtils.PrepareShadowsMidtonesHighlights( shadowsMidtonesHighlights.shadows.value, shadowsMidtonesHighlights.midtones.value, shadowsMidtonesHighlights.highlights.value ); var (lift, gamma, gain) = ColorUtils.PrepareLiftGammaGain( liftGammaGain.lift.value, liftGammaGain.gamma.value, liftGammaGain.gain.value ); var (splitShadows, splitHighlights) = ColorUtils.PrepareSplitToning( splitToning.shadows.value, splitToning.highlights.value, splitToning.balance.value ); int lutHeight = postProcessingData.lutSize; int lutWidth = lutHeight * lutHeight; var lutParameters = new Vector4(lutHeight, 0.5f / lutWidth, 0.5f / lutHeight, lutHeight / (lutHeight - 1f)); // 设置Shader参数 material.SetVector(ShaderConstants._Lut_Params, lutParameters); material.SetVector(ShaderConstants._ColorBalance, lmsColorBalance); material.SetVector(ShaderConstants._ColorFilter, colorAdjustments.colorFilter.value.linear); material.SetVector(ShaderConstants._ChannelMixerRed, channelMixerR); material.SetVector(ShaderConstants._ChannelMixerGreen, channelMixerG); material.SetVector(ShaderConstants._ChannelMixerBlue, channelMixerB); material.SetVector(ShaderConstants._HueSatCon, hueSatCon); material.SetVector(ShaderConstants._Lift, lift); material.SetVector(ShaderConstants._Gamma, gamma); material.SetVector(ShaderConstants._Gain, gain); material.SetVector(ShaderConstants._Shadows, shadows); material.SetVector(ShaderConstants._Midtones, midtones); material.SetVector(ShaderConstants._Highlights, highlights); material.SetVector(ShaderConstants._ShaHiLimits, shadowsHighlightsLimits); material.SetVector(ShaderConstants._SplitShadows, splitShadows); material.SetVector(ShaderConstants._SplitHighlights, splitHighlights); // YRGB curves material.SetTexture(ShaderConstants._CurveMaster, curves.master.value.GetTexture()); material.SetTexture(ShaderConstants._CurveRed, curves.red.value.GetTexture()); material.SetTexture(ShaderConstants._CurveGreen, curves.green.value.GetTexture()); material.SetTexture(ShaderConstants._CurveBlue, curves.blue.value.GetTexture()); // Secondary curves material.SetTexture(ShaderConstants._CurveHueVsHue, curves.hueVsHue.value.GetTexture()); material.SetTexture(ShaderConstants._CurveHueVsSat, curves.hueVsSat.value.GetTexture()); material.SetTexture(ShaderConstants._CurveLumVsSat, curves.lumVsSat.value.GetTexture()); material.SetTexture(ShaderConstants._CurveSatVsSat, curves.satVsSat.value.GetTexture()); // Tonemapping (baked into the lut for HDR) if (hdr) &#123; material.shaderKeywords = null; switch (tonemapping.mode.value) &#123; case TonemappingMode.Neutral: material.EnableKeyword(ShaderKeywordStrings.TonemapNeutral); break; case TonemappingMode.ACES: material.EnableKeyword(allowColorGradingACESHDR ? ShaderKeywordStrings.TonemapACES : ShaderKeywordStrings.TonemapNeutral); break; default: break; // None &#125; // HDR output is active if (cameraData.isHDROutputActive) &#123; Vector4 hdrOutputLuminanceParams; Vector4 hdrOutputGradingParams; UniversalRenderPipeline.GetHDROutputLuminanceParameters(cameraData.hdrDisplayInformation, cameraData.hdrDisplayColorGamut, tonemapping, out hdrOutputLuminanceParams); UniversalRenderPipeline.GetHDROutputGradingParameters(tonemapping, out hdrOutputGradingParams); material.SetVector(ShaderPropertyId.hdrOutputLuminanceParams, hdrOutputLuminanceParams); material.SetVector(ShaderPropertyId.hdrOutputGradingParams, hdrOutputGradingParams); HDROutputUtils.ConfigureHDROutput(material, cameraData.hdrDisplayColorGamut, HDROutputUtils.Operation.ColorConversion); &#125; &#125; cameraData.xr.StopSinglePass(cmd); if (cameraData.xr.supportsFoveatedRendering) cmd.SetFoveatedRenderingMode(FoveatedRenderingMode.Disabled); // 渲染LUT纹理 Blitter.BlitCameraTexture(cmd, internalLutTarget, internalLutTarget, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 0); cameraData.xr.StartSinglePass(cmd); &#125;&#125; PostProcessPass 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322// Pass执行时调用的参数 public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)&#123; // 内置后处理组件，用于获取用户设置的后处理参数 var stack = VolumeManager.instance.stack; m_DepthOfField = stack.GetComponent&lt;DepthOfField&gt;(); m_MotionBlur = stack.GetComponent&lt;MotionBlur&gt;(); m_PaniniProjection = stack.GetComponent&lt;PaniniProjection&gt;(); m_Bloom = stack.GetComponent&lt;Bloom&gt;(); m_LensDistortion = stack.GetComponent&lt;LensDistortion&gt;(); m_ChromaticAberration = stack.GetComponent&lt;ChromaticAberration&gt;(); m_Vignette = stack.GetComponent&lt;Vignette&gt;(); m_ColorLookup = stack.GetComponent&lt;ColorLookup&gt;(); m_ColorAdjustments = stack.GetComponent&lt;ColorAdjustments&gt;(); m_Tonemapping = stack.GetComponent&lt;Tonemapping&gt;(); m_FilmGrain = stack.GetComponent&lt;FilmGrain&gt;(); m_UseFastSRGBLinearConversion = renderingData.postProcessingData.useFastSRGBLinearConversion; m_SupportDataDrivenLensFlare = renderingData.postProcessingData.supportDataDrivenLensFlare; // 是最后一个Pass? var cmd = renderingData.commandBuffer; if (m_IsFinalPass) &#123; using (new ProfilingScope(cmd, m_ProfilingRenderFinalPostProcessing)) &#123; // 渲染最后一个Pass RenderFinalPass(cmd, ref renderingData); &#125; &#125; else &#123; using (new ProfilingScope(cmd, m_ProfilingRenderPostProcessing)) &#123; // 渲染后处理（各种后处理组件）效果 Render(cmd, ref renderingData); &#125; &#125;&#125;// 渲染各种后处理效果void Render(CommandBuffer cmd, ref RenderingData renderingData)&#123; ref CameraData cameraData = ref renderingData.cameraData; ref ScriptableRenderer renderer = ref cameraData.renderer; bool isSceneViewCamera = cameraData.isSceneViewCamera; // 检查各种后处理是否启用 bool useStopNan = cameraData.isStopNaNEnabled &amp;&amp; m_Materials.stopNaN != null; bool useSubPixeMorpAA = cameraData.antialiasing == AntialiasingMode.SubpixelMorphologicalAntiAliasing &amp;&amp; SystemInfo.graphicsDeviceType != GraphicsDeviceType.OpenGLES2; var dofMaterial = m_DepthOfField.mode.value == DepthOfFieldMode.Gaussian ? m_Materials.gaussianDepthOfField : m_Materials.bokehDepthOfField; bool useDepthOfField = m_DepthOfField.IsActive() &amp;&amp; !isSceneViewCamera &amp;&amp; dofMaterial != null; bool useLensFlare = !LensFlareCommonSRP.Instance.IsEmpty() &amp;&amp; m_SupportDataDrivenLensFlare; bool useMotionBlur = m_MotionBlur.IsActive() &amp;&amp; !isSceneViewCamera; bool usePaniniProjection = m_PaniniProjection.IsActive() &amp;&amp; !isSceneViewCamera; // 编辑下禁用运动模糊 useMotionBlur = useMotionBlur &amp;&amp; Application.isPlaying; // 打印不能使用TAA抗锯齿的原因 bool useTemporalAA = cameraData.IsTemporalAAEnabled(); if (cameraData.antialiasing == AntialiasingMode.TemporalAntiAliasing &amp;&amp; !useTemporalAA) TemporalAA.ValidateAndWarn(ref cameraData); // 剩余的Pass数量 int amountOfPassesRemaining = (useStopNan ? 1 : 0) + (useSubPixeMorpAA ? 1 : 0) + (useDepthOfField ? 1 : 0) + (useLensFlare ? 1 : 0) + (useTemporalAA ? 1 : 0) + (useMotionBlur ? 1 : 0) + (usePaniniProjection ? 1 : 0); // 禁用SwapBufferMSAA if (m_UseSwapBuffer &amp;&amp; amountOfPassesRemaining &gt; 0) &#123; renderer.EnableSwapBufferMSAA(false); &#125; // 定义获取原和目标纹理函数 RTHandle source = m_UseSwapBuffer ? renderer.cameraColorTargetHandle : m_Source; RTHandle destination = m_UseSwapBuffer ? renderer.GetCameraColorFrontBuffer(cmd) : null; RTHandle GetSource() =&gt; source; RTHandle GetDestination() &#123; if (destination == null) &#123; RenderingUtils.ReAllocateIfNeeded(ref m_TempTarget, GetCompatibleDescriptor(), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_TempTarget\"); destination = m_TempTarget; &#125; else if (destination == m_Source &amp;&amp; m_Descriptor.msaaSamples &gt; 1) &#123; // Avoid using m_Source.id as new destination, it may come with a depth buffer that we don't want, may have MSAA that we don't want etc RenderingUtils.ReAllocateIfNeeded(ref m_TempTarget2, GetCompatibleDescriptor(), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_TempTarget2\"); destination = m_TempTarget2; &#125; return destination; &#125; // 定义交换函数 void Swap(ref ScriptableRenderer r) &#123; --amountOfPassesRemaining; if (m_UseSwapBuffer) &#123; // 交换颜色Buffer r.SwapColorBuffer(cmd); source = r.cameraColorTargetHandle; // 最后一个Pass, blit到 MSAA if (amountOfPassesRemaining == 0 &amp;&amp; !m_HasFinalPass) r.EnableSwapBufferMSAA(true); destination = r.GetCameraColorFrontBuffer(cmd); &#125; else &#123; CoreUtils.Swap(ref source, ref destination); &#125; &#125; // 设置投影矩阵 cmd.SetGlobalMatrix(ShaderConstants._FullscreenProjMat, GL.GetGPUProjectionMatrix(Matrix4x4.identity, true)); // 使用使用Stop Not-a-Number后处理，将浮点数计算结果中的，NaN值修正为正常值（0） if (useStopNan) &#123; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.StopNaNs))) &#123; Blitter.BlitCameraTexture(cmd, GetSource(), GetDestination(), RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, m_Materials.stopNaN, 0); Swap(ref renderer); &#125; &#125; // Subpixel Morphological Anti-aliasing (SMAA)抗锯齿 if (useSubPixeMorpAA) &#123; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.SMAA))) &#123; DoSubpixelMorphologicalAntialiasing(ref cameraData, cmd, GetSource(), GetDestination()); Swap(ref renderer); &#125; &#125; // 景深效果 if (useDepthOfField) &#123; var markerName = m_DepthOfField.mode.value == DepthOfFieldMode.Gaussian ? URPProfileId.GaussianDepthOfField : URPProfileId.BokehDepthOfField; using (new ProfilingScope(cmd, ProfilingSampler.Get(markerName))) &#123; DoDepthOfField(cameraData.camera, cmd, GetSource(), GetDestination(), cameraData.pixelRect); Swap(ref renderer); &#125; &#125; // TAA抗锯齿 if (useTemporalAA) &#123; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.TemporalAA))) &#123; TemporalAA.ExecutePass(cmd, m_Materials.temporalAntialiasing, ref cameraData, source, destination, m_MotionVectors.rt); Swap(ref renderer); &#125; &#125; // 运动模糊 if (useMotionBlur) &#123; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.MotionBlur))) &#123; DoMotionBlur(cmd, GetSource(), GetDestination(), ref cameraData); Swap(ref renderer); &#125; &#125; //Panini projection（帕尼尼投影）后处理是一种用于图形渲染的视觉效果，旨在模拟宽角镜头的视觉效果，同时减少边缘扭曲。 //这种方法得名于18世纪的意大利画家Giovanni Paolo Panini，他以其精湛的视角和全景画作而闻名。在现代图形渲染中，帕尼尼投影被用来改善宽屏幕或宽视角下的视觉体验，特别是在视频游戏和虚拟现实应用中，为用户提供更自然、更沉浸的视觉体验。 if (usePaniniProjection) &#123; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.PaniniProjection))) &#123; DoPaniniProjection(cameraData.camera, cmd, GetSource(), GetDestination()); Swap(ref renderer); &#125; &#125; //\"Uber\"一词通常用来形容一个综合性的或\"全包\"（all-in-one）的着色器或后处理效果，它集成了多种视觉效果和图形处理技术。 //这种\"Uber后处理效果\"可能包括，但不限于，色彩校正、HDR（高动态范围）渲染、Bloom、景深（Depth of Field）、光晕（Lens Flares）、抗锯齿、阴影、光照效果等多个组件。 using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.UberPostProcess))) &#123; // 重置Shader关键值 m_Materials.uber.shaderKeywords = null; // 设置Bloom参数 bool bloomActive = m_Bloom.IsActive(); if (bloomActive) &#123; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.Bloom))) SetupBloom(cmd, GetSource(), m_Materials.uber); &#125; // 镜头光晕 if (useLensFlare) &#123; bool usePanini; float paniniDistance; float paniniCropToFit; if (m_PaniniProjection.IsActive()) &#123; usePanini = true; paniniDistance = m_PaniniProjection.distance.value; paniniCropToFit = m_PaniniProjection.cropToFit.value; &#125; else &#123; usePanini = false; paniniDistance = 1.0f; paniniCropToFit = 1.0f; &#125; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.LensFlareDataDrivenComputeOcclusion))) &#123; LensFlareDataDrivenComputeOcclusion(cameraData.camera, cmd, GetSource(), usePanini, paniniDistance, paniniCropToFit); &#125; using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.LensFlareDataDriven))) &#123; LensFlareDataDriven(cameraData.camera, cmd, GetSource(), usePanini, paniniDistance, paniniCropToFit); &#125; &#125; // 设置其他效果参数 // Lens Distortion后处理效果模拟了真实世界相机镜头中常见的畸变现象，主要是桶形畸变（barrel distortion）和枕形畸变（pincushion distortion SetupLensDistortion(m_Materials.uber, isSceneViewCamera); //色差（Chromatic Aberration），也称为色彩像差，是一种由于镜头无法将不同颜色的光线聚焦在同一点上而产生的视觉现象。 //这种现象在图像的边缘部分尤为明显，表现为彩色的晕边，通常是紫色或绿色的边缘。色差通常出现在便宜的镜头或极宽角镜头的照片中，而高质量的镜头设计会尽量减少这种效果。 //边缘会有类似彩虹的色斑 SetupChromaticAberration(m_Materials.uber); //晕影（Vignette）效果是一种在摄影、视频和图形渲染中常用的视觉效果，其特点是图像边缘相对于中心部分显得更暗，有时也呈现出柔和的过渡。 //这种效果可以增强图像的焦点，通过在视觉上引导观众的注意力向中心集中，从而提升图像的美感和深度感。 //在传统摄影中，晕影效果可能由于镜头特性、遮光不足或特定的处理技术而自然产生。而在数字图像处理和图形渲染中，晕影效果通常是通过后处理技术故意添加的。 SetupVignette(m_Materials.uber, cameraData.xr); //色彩分级是另一种强大的图像后处理技术，用于调整图像的整体色调，包括对比度、色温、饱和度等，以达到特定的视觉风格或情绪效果。 //色彩分级在电影制作、视频游戏和摄影中广泛应用，用于增强视觉叙事或引导观众情绪。将原始颜色通过LUT(颜色查找表)，映射为对应颜色 SetupColorGrading(cmd, ref renderingData, m_Materials.uber); //Film Grain（胶片颗粒）是指在传统胶片摄影中出现的一种视觉效果，由于胶片上的银盐颗粒大小不一而产生的细微颗粒状纹理。在数码摄影和视频制作中，人们通常通过后期处理添加模拟的胶片颗粒效果，以达到增加质感、增强视觉深度或复古风格的目的。 SetupGrain(ref cameraData, m_Materials.uber); //Dithering（抖动）是一种在数字图像处理中常用的技术，用于在有限的颜色深度显示设备上模拟更广泛的颜色范围。这种技术通过在像素之间故意添加噪声或图案，来模拟中间色调或渐变效果，从而减少颜色带（色阶突变）的视觉影响。 SetupDithering(ref cameraData, m_Materials.uber); //是否需要转换回SRGB if (RequireSRGBConversionBlitToBackBuffer(ref cameraData)) m_Materials.uber.EnableKeyword(ShaderKeywordStrings.LinearToSRGBConversion); //是否需要转换HDR输出 bool requireHDROutput = RequireHDROutput(ref cameraData); if (requireHDROutput) &#123; // Color space conversion is already applied through color grading, do encoding if uber post is the last pass // Otherwise encoding will happen in the final post process pass or the final blit pass HDROutputUtils.Operation hdrOperation = !m_HasFinalPass &amp;&amp; m_EnableColorEncodingIfNeeded ? HDROutputUtils.Operation.ColorEncoding : HDROutputUtils.Operation.None; SetupHDROutput(cameraData.hdrDisplayInformation, cameraData.hdrDisplayColorGamut, m_Materials.uber, hdrOperation); &#125; if (m_UseFastSRGBLinearConversion) &#123; m_Materials.uber.EnableKeyword(ShaderKeywordStrings.UseFastSRGBLinearConversion); &#125; DebugHandler debugHandler = GetActiveDebugHandler(ref renderingData); bool resolveToDebugScreen = debugHandler != null &amp;&amp; debugHandler.WriteToDebugScreenTexture(ref cameraData); debugHandler?.UpdateShaderGlobalPropertiesForFinalValidationPass(cmd, ref cameraData, !m_HasFinalPass &amp;&amp; !resolveToDebugScreen); // 完成参数设置后，blit var colorLoadAction = RenderBufferLoadAction.DontCare; if (m_Destination == k_CameraTarget &amp;&amp; !cameraData.isDefaultViewport) colorLoadAction = RenderBufferLoadAction.Load; // Note: We rendering to \"camera target\" we need to get the cameraData.targetTexture as this will get the targetTexture of the camera stack. // Overlay cameras need to output to the target described in the base camera while doing camera stack. RenderTargetIdentifier cameraTargetID = BuiltinRenderTextureType.CameraTarget;#if ENABLE_VR &amp;&amp; ENABLE_XR_MODULE if (cameraData.xr.enabled) cameraTargetID = cameraData.xr.renderTarget;#endif if (!m_UseSwapBuffer) m_ResolveToScreen = cameraData.resolveFinalTarget || m_Destination.nameID == cameraTargetID || m_HasFinalPass == true; // 使用了SwapBuffer但又不是相机栈的最后 if (m_UseSwapBuffer &amp;&amp; !m_ResolveToScreen) &#123; if (!m_HasFinalPass) &#123; renderer.EnableSwapBufferMSAA(true); destination = renderer.GetCameraColorFrontBuffer(cmd); &#125; Blitter.BlitCameraTexture(cmd, GetSource(), destination, colorLoadAction, RenderBufferStoreAction.Store, m_Materials.uber, 0); renderer.ConfigureCameraColorTarget(destination); Swap(ref renderer); &#125; else if (!m_UseSwapBuffer) &#123; var firstSource = GetSource(); Blitter.BlitCameraTexture(cmd, firstSource, GetDestination(), colorLoadAction, RenderBufferStoreAction.Store, m_Materials.uber, 0); Blitter.BlitCameraTexture(cmd, GetDestination(), m_Destination, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, m_BlitMaterial, m_Destination.rt?.filterMode == FilterMode.Bilinear ? 1 : 0); &#125; else if (m_ResolveToScreen) &#123; // 将最终结果Blit到调试器的纹理上 if (resolveToDebugScreen) &#123; Blitter.BlitCameraTexture(cmd, GetSource(), debugHandler.DebugScreenColorHandle, RenderBufferLoadAction.Load, RenderBufferStoreAction.Store, m_Materials.uber, 0); renderer.ConfigureCameraTarget(debugHandler.DebugScreenColorHandle, debugHandler.DebugScreenDepthHandle); &#125; else &#123; // 将最终结果Blit到屏幕 RenderTargetIdentifier cameraTarget = cameraData.targetTexture != null ? new RenderTargetIdentifier(cameraData.targetTexture) : cameraTargetID; RTHandleStaticHelpers.SetRTHandleStaticWrapper(cameraTarget); var cameraTargetHandle = RTHandleStaticHelpers.s_RTHandleWrapper; RenderingUtils.FinalBlit(cmd, ref cameraData, GetSource(), cameraTargetHandle, colorLoadAction, RenderBufferStoreAction.Store, m_Materials.uber, 0); renderer.ConfigureCameraColorTarget(cameraTargetHandle); &#125; &#125; &#125;&#125; 内建后处理Shader 色彩校正(色彩调整) Unity是将色彩校正信息存在LUT纹理中，最后再查找LUT纹理中的对应颜色来替换当前颜色。Unity内建的色彩校正后处理组件有:WhiteBalance（白平衡）, ColorAdjustments（颜色调整）, SplitToning（色调分离）, ChannelMixer(通道混合), ShadowsMidtonesHighlights(阴影，中间调，高光),LiftGammaGain（提升，伽马，增强）, ColorCurves（颜色曲线）, Tonemapping（色调映射）。 Neutral LUT图，是一个“neutral”查找表（LUT）指的是一种不改变色彩的LUT，通常用于作为起点或基准。这种LUT不对图像的色彩、对比度或亮度做任何改变，使得原始图像保持不变。neutral LUT通常用于测试、校准或作为创建自定义色彩分级预设的基础。 WhiteBalance（白平衡） 白平衡是图像处理中的一个重要步骤，目的是校正图像中的色彩，使得白色对象无论在什么样的光线下都被渲染为白色，从而提高图像的色彩准确性。在进行白平衡处理时，LMS系数用于调整图像中的颜色分量，以反映光源下的真实颜色。不同环境色温下拍出来的拍色是有一定差异的，白平衡的目的就是通过后期的手段将白色还原成白色。 色温图： 色温图 在调色的过程中，如果图片本身偏高色温（蓝色），那么就要补偿低色温（橙色），来达到白色。如果想营造某种艺术效果，可以根据需求调整。 过程： 将白平衡的色温(temperature)和色调(tint)参数转换成LMS系数 将原颜色在LMS空间应用白平衡参数 最后将LMS空间中的值转回线性RGB空间 代码： 色温(temperature)和色调(tint)参数转换成LMS系数: 123456789101112131415161718192021222324252627282930313233public static Vector3 ColorBalanceToLMSCoeffs(float temperature, float tint)&#123; // 将色温和色调映射到，大约[-1.5;1.5]的范围，works best float t1 = temperature / 65f; float t2 = tint / 65f; // 获取参考白点（D65）的的xy色度值, D65指的时，色温6500K(开尔文)是纯白的色度值 // D65白点的色图值：x = 0.3127, y = 0.3290 // 具体计算公式参见维基百科的[CIE 1931 XYZ色彩空间](https://zh.wikipedia.org/wiki/CIE_1931%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4) float x = 0.31271f - t1 * (t1 &lt; 0f ? 0.1f : 0.05f); float y = StandardIlluminantY(x) + t2 * 0.05f; // 2.87f * x - 3f * x * x - 0.27509507f // 计算LMS空间中的系数。 var w1 = new Vector3(0.949237f, 1.03542f, 1.08728f); // D65白点对应的LMS系数 var w2 = CIExyToLMS(x, y); return new Vector3(w1.x / w2.x, w1.y / w2.y, w1.z / w2.z);&#125;//将CIExy转换到LMS空间下public static Vector3 CIExyToLMS(float x, float y)&#123; // 三刺值 float Y = 1f; float X = Y * x / y; float Z = Y * (1f - x - y) / y; // 三刺值到LMS的变化矩阵 float L = 0.7328f * X + 0.4296f * Y - 0.1624f * Z; float M = -0.7036f * X + 1.6975f * Y + 0.0061f * Z; float S = 0.0030f * X + 0.0136f * Y + 0.9834f * Z; return new Vector3(L, M, S);&#125; 在LMS空间中计算白平衡 123456// 当前Lut位置的Neutral值float3 colorLMS = LinearToLMS(colorLinear);// 计算白平衡colorLMS *= _ColorBalance.xyz;// 转换回线性空间，进行后续的计算colorLinear = LMSToLinear(colorLMS); ColorAdjustments（颜色调整） Color Adjustments是图像处理和图形设计中常见的一组技术，用于修改和优化图像的色彩属性，包括亮度、对比度、饱和度、色调、曝光等。这些调整能够帮助图像更好地传达预期的视觉效果或情绪，提升图像质量，或者适应特定的显示设备和环境。 对比度(Contrast)：调整图像明暗区域之间的差异，增加对比度会使图像的明暗区域更加分明，而降低对比度则会使图像看起来更加平坦。 色调(Hue)：改变图像中的色彩平衡，常用于为图像创建特定的色彩风格或修正图像的色温。 饱和度(Saturation)：调整图像中颜色的强度，饱和度高的图像色彩会更加鲜明，而饱和度低的图像则色彩更加柔和。 亮度(Brightness)：调整图像的明暗程度，使图像看起来更亮或更暗。 曝光(Exposure)：模拟调整摄影中的曝光效果，可以使图像整体变亮或变暗，用于修正过曝或欠曝的图像 对比度： 过程： 将颜色转换到对数（LogC）空间 应用对比度调节公式， 如下 将结果转换回线性(Linear)空间 \\((color - middleGray) * contrast + middleGray\\) 说明： color : 需要调节的颜色 middleGray : 中性灰色的定义（0.5或ACES定义的0.4135884） contrast ：对比度调节系数 代码： 123float3 colorLog = LinearToLogC(colorLinear);colorLog = (colorLog - ACEScc_MIDGRAY) * _HueSatCon.z + ACEScc_MIDGRAY;colorLinear = LogCToLinear(colorLog); 色调： 过程： 将RGB颜色转换到HSV色彩空间， 调整饱和度H值， 将结果转换回RGB空间 代码： 123float3 hsv = RgbToHsv(colorLinear);float hue = hsv.x + _HueSatCon.x; colorLinear = HsvToRgb(hsv); 饱和度： 过程： 将RGB颜色转换到HSV色彩空间， 调整饱和度S值， 将结果转换回RGB空间 代码： 12luma = GetLuminance(colorLinear); // real3(0.2126729, 0.7151522, 0.0721750) ACES:half3(0.272229, 0.674082, 0.0536895)colorLinear = luma.xxx + (_HueSatCon.yyy * satMult) * (colorLinear - luma.xxx); 曝光和亮度 曝光（Exposure） 和 亮度（Brightness） 是两个在摄影、视频制作和图像处理中常用的术语，它们描述图像的光照特性，但指代的概念和调整方式有所不同。 曝光（Exposure） 曝光是指在拍摄过程中，光线作用在相机传感器上的量，这个量由光圈大小（Aperture）、快门速度（Shutter Speed）和ISO感光度（ISO Sensitivity）三个基本相机设置共同决定。曝光决定了图像最初捕捉到的光量，是摄影的一个基础概念。 光圈：控制镜头进光量的大小。 快门速度：控制光线照射在传感器上的时间长度。 ISO感光度：控制传感器对光线的敏感度。 正确的曝光是指在特定的拍摄条件下，选取适当的光圈、快门速度和ISO设置，以便传感器接收到足够的光量，使得图像既不过曝（太亮，丢失细节）也不欠曝（太暗，缺乏细节）。 曝光调节模拟了在拍摄阶段改变光量的效果，它可以更加广泛地影响图像的整体明暗，包括极亮和极暗的区域。曝光调节通常对图像的动态范围有较大的影响，可能会导致亮部过曝或暗部过暗而丢失细节。在数学上，曝光调节可能被模拟为对图像每个像素值的乘法调整： \\[C^{&#39;} = C x 2^{\\delta EV} \\] \\(C^{&#39;}\\) 是调整后的像素值。 \\(C\\) 是原始像素值 \\(\\delta EV\\) 是曝光值的变化，以停（stop）为单位。正值表示增加曝光，负值表示减少曝光。每增加1个停，图像的亮度翻倍；每减少1个停，图像的亮度减半。 亮度（Brightness） 亮度是指图像看起来的光线亮度感觉，是图像处理和显示设备中的一个概念，通常用于描述图像的视觉感受。亮度调整通常在图像已经被捕捉并需要在后期处理时改变其整体明暗程度时进行。 亮度调节可以通过多种方式在图像处理中实现。最直接的方法是对图像的每个像素的亮度分量进行调整。下面是一些基本的亮度调节方法： 线性亮度调整 最简单的亮度调整方法是对图像的每个像素值直接加上一个固定的量。对于RGB图像，这意味着对R、G、B三个通道的值都进行相同的调整： \\[C^{&#39;} = C + \\delta \\] \\(C^{&#39;}\\) 是调整后的颜色值 \\(C\\) 是原始颜色值， \\(\\delta\\) 是要调整的亮度量（可以为正值或负值）。 保持色彩比例的亮度调整 为了避免在调整亮度时改变色彩的相对比例（即色调和饱和度），可以先将RGB颜色转换到一个能够分离亮度信息的色彩空间（如HSV或HSL），只对亮度分量进行调整，然后再转换回RGB空间。亮度分量的调整可以表示为： \\[V^{&#39;} = V + \\delta \\] \\(V^{&#39;}\\) 是调整后的亮度值 \\(V\\) 是原始的亮度值 \\(\\delta\\) 是要调整的亮度量 伽马校正亮度调整 伽马校正提供了一种更复杂的亮度调整方式，可以更符合人眼对亮度的非线性感知： \\[C^{&#39;}= 255x(\\frac{C}{255})^{1/ \\gamma}\\] \\(C^{&#39;}\\) 是调整后的颜色值 \\(C\\) 是原始颜色值 \\(\\gamma\\) 是伽马校正值。通过调整\\(\\gamma\\)值，可以在增亮或减暗图像的同时保持细节的自然外观。 区别 计算方式： 曝光调节通过乘法影响像素值，而亮度调节通过加法改变像素值。 影响范围： 曝光调节可以更显著地改变图像的整体亮度，包括最亮和最暗的区域，可能会导致亮部或暗部细节的丢失。亮度调节则更加温和，通常不会导致亮部或暗部细节的丢失，但对动态范围的影响较小。 应用场景： 在需要大幅度调整图像明暗，模拟不同曝光效果时，使用曝光调节；在需要微调图像亮度，保持大部分细节时，使用亮度调节。 SplitToning（色调分离） Split Toning调节可以通过多种方法实现，特别是在处理数字图像时。基础思路涉及分别向图像的阴影（Shadows）和高光（Highlights）部分应用不同的色调，而保持中间调的颜色相对不变。 过程： 为了和Adobe产品的计算方式一样，要将线性颜色转换到Gamma 计算像素的亮度值，根据亮度值,确定分离后的阴影和高光颜色 将分离出来的阴影和高光颜色与原颜色使用柔光的混合模式进行混合 最后将Gamma空间颜色转换回线性空间 公式： 基础公式 假设我们有两个目标色调：一个用于阴影部分\\(C_{shadow}\\)，另一个用于高光部分 \\(C_{highlight}\\)。图像中每个像素点的最终色调\\(C_{final}\\)可以通过以下方式计算得出： \\[C_{final} = w * C_{shadow} + (1-w)*C_{highlight}\\] 其中，\\(w\\)是一个根据像素亮度确定的权重，用于控制该像素更接近阴影色调还是高光色调。权重\\(w\\)的计算可以通过多种方式实现，一种简单的方法是使用像素亮度的线性或非线性函数。 权重计算 权重\\(w\\)可以根据像素的亮度\\(L\\)（通常是从0到1）进行计算，其中低亮度值接近阴影，高亮度值接近高光。一个简单的线性模型是： \\[w = \\frac{L-L_{min}}{L_{max}-L_{min}}\\] ​这里，​\\(L_{min}\\)和\\(L_{max}\\)分别表示图像中亮度的最小和最大值，用于确定阴影和高光的界限。为了更平滑的过渡，也可以使用基于S型函数的模型，如： \\[w = \\frac{1}{1+e^{-(L-L_{mid})*k}}\\] 这里，\\(L_{mid}\\)是阴影和高光之间的中间亮度值，\\(k\\)是控制过渡平滑程度的参数。 代码： 1234567891011121314151617// 尽管与直觉相反，但让色调分离的工作方式与Adobe中的工作方式相同// 我们必须在伽玛空间中进行所有数学运算的产品// 亮度调节参数float balance = _SplitShadows.w;// 转换到Gamma空间float3 colorGamma = PositivePow(colorLinear, 1.0 / 2.2);// 计算像素本身的亮度 + 调节亮度float luma = saturate(GetLuminance(saturate(colorGamma)) + balance);// 当luma变小时，会把大部分像素颜色往阴影颜色拉，否者像高光颜色拉float3 splitShadows = lerp((0.5).xxx, _SplitShadows.xyz, 1.0 - luma);float3 splitHighlights = lerp((0.5).xxx, _SplitHighlights.xyz, luma);// 用柔光混合模式混合原颜色和阴影色，以及高光色colorGamma = SoftLight(colorGamma, splitShadows);colorGamma = SoftLight(colorGamma, splitHighlights);// 将颜色转换回线性空间colorLinear = PositivePow(colorGamma, 2.2); 12345678// 用于色调分离的柔光混合模式。 只要 `blend` 为 [0;1] 即可在 HDR 中工作float3 SoftLight(float3 base, float3 blend)&#123; float3 r1 = 2.0 * base * blend + base * base * (1.0 - 2.0 * blend); float3 r2 = sqrt(base) * (2.0 * blend - 1.0) + 2.0 * base * (1.0 - blend); float3 t = step(0.5, blend); return r2 * t + (1.0 - t) * r1;&#125; ChannelMixer(通道混合) 过程： Channel Mixer，它允许用户调整图像中红色、绿色和蓝色通道的比例，以此来改变图像的总体色彩。通过混合这些颜色通道，用户可以实现广泛的色彩效果，包括颜色校正、黑白转换、色彩强化等。在数字图像处理中，一个像素的颜色通常由红色（R）、绿色（G）、蓝色（B）三个颜色通道的值组合而成。Channel Mixer 允许用户调整这些颜色通道的输出比例，通过修改每个通道对最终图像颜色的贡献度来改变图像的颜色平衡。 公式： \\(R_{out} = R_{in} * R_x + G_{in} * G_y + B_{in} * B_z\\) \\(G_{out} = R_{in} * R_x + G_{in} * G_y + B_{in} * B_z\\) \\(B_{out} = R_{in} * R_x + G_{in} * G_y + B_{in} * B_z\\) x,y和z分别是红，绿和蓝色通道的混合系数 代码： 12345 colorLinear &#x3D; float3( dot(colorLinear, _ChannelMixerRed.xyz), &#x2F;&#x2F; R dot(colorLinear, _ChannelMixerGreen.xyz), &#x2F;&#x2F; G dot(colorLinear, _ChannelMixerBlue.xyz) &#x2F;&#x2F; B); ShadowsMidtonesHighlights(阴影，中间调，高光) 在图像处理和色彩分级中，对阴影（Shadows）、中间调（Midtones）、高光（Highlights）的调节是一种常见的技术，用于细致调整图像的色调平衡和对比度。这种方法允许你分别调整图像中暗部、中亮部和亮部的亮度、色彩和饱和度，以达到更为精细的视觉效果。 过程： 分离阴影、中间调、高光：首先，基于像素的亮度值将图像分割成阴影、中间调和高光三个部分。这通常通过设置亮度阈值来实现，不同的亮度范围对应于阴影、中间调和高光。 应用调节：对每个部分（阴影、中间调、高光）分别应用亮度、色彩和饱和度的调节。调节可以是线性的，也可以是基于特定曲线的，如S型曲线。 公式： 虽然具体的调节公式可能根据不同的软件和实现而异，但可以用以下基础形式来近似描述： \\[C^{&#39;} = C + \\Delta C x W(L)\\] 其中: - \\(C^{&#39;}\\)是调整后的颜色值。 - \\(C\\)是原颜色。 - \\(\\Delta C\\)是想应用的调整量，可以是亮度、色彩或饱和度的变化。 - \\(W(L)\\) 是一个基于像素亮度\\(L\\)的权重函数，用于确定该像素属于阴影、中间调还是高光，以及应该如何应用调整。 代码： 1234567891011121314151617181920212223242526272829303132333435363738public static (Vector4, Vector4, Vector4) PrepareShadowsMidtonesHighlights(in Vector4 inShadows, in Vector4 inMidtones, in Vector4 inHighlights)&#123; float weight; // 根据权重设置最终应用的阴影颜色值 var shadows = inShadows; shadows.x = Mathf.GammaToLinearSpace(shadows.x); shadows.y = Mathf.GammaToLinearSpace(shadows.y); shadows.z = Mathf.GammaToLinearSpace(shadows.z); weight = shadows.w * (Mathf.Sign(shadows.w) &lt; 0f ? 1f : 4f); shadows.x = Mathf.Max(shadows.x + weight, 0f); shadows.y = Mathf.Max(shadows.y + weight, 0f); shadows.z = Mathf.Max(shadows.z + weight, 0f); shadows.w = 0f; // 根据权重设置最终应用的中间调颜色值 var midtones = inMidtones; midtones.x = Mathf.GammaToLinearSpace(midtones.x); midtones.y = Mathf.GammaToLinearSpace(midtones.y); midtones.z = Mathf.GammaToLinearSpace(midtones.z); weight = midtones.w * (Mathf.Sign(midtones.w) &lt; 0f ? 1f : 4f); midtones.x = Mathf.Max(midtones.x + weight, 0f); midtones.y = Mathf.Max(midtones.y + weight, 0f); midtones.z = Mathf.Max(midtones.z + weight, 0f); midtones.w = 0f; // 根据权重设置最终应用的高光颜色值 var highlights = inHighlights; highlights.x = Mathf.GammaToLinearSpace(highlights.x); highlights.y = Mathf.GammaToLinearSpace(highlights.y); highlights.z = Mathf.GammaToLinearSpace(highlights.z); weight = highlights.w * (Mathf.Sign(highlights.w) &lt; 0f ? 1f : 4f); highlights.x = Mathf.Max(highlights.x + weight, 0f); highlights.y = Mathf.Max(highlights.y + weight, 0f); highlights.z = Mathf.Max(highlights.z + weight, 0f); highlights.w = 0f; return (shadows, midtones, highlights);&#125; 12345678910// 计算亮度值luma = GetLuminance(colorLinear);// 计算阴影，中间调和高光的调节系数（最终只有一个部分的系数为1，其他的都为0）float shadowsFactor = 1.0 - smoothstep(_ShaHiLimits.x, _ShaHiLimits.y, luma);float highlightsFactor = smoothstep(_ShaHiLimits.z, _ShaHiLimits.w, luma);float midtonesFactor = 1.0 - shadowsFactor - highlightsFactor;// 计算最终的颜色colorLinear = colorLinear * _Shadows.xyz * shadowsFactor + colorLinear * _Midtones.xyz * midtonesFactor + colorLinear * _Highlights.xyz * highlightsFactor; LiftGammaGain（提升，伽马，增强） Lift Gamma Gain是一种常用于色彩分级和图像调整中的技术，特别是在视频和电影后期制作中。它允许用户独立调整图像的阴影（Lift）、中间调（Gamma）和高光（Gain）部分，提供了对图像色彩和对比度的精细控制。这三个参数共同作用，可以实现复杂的视觉效果和色彩调整。 原理： Lift（提升）：主要影响图像的暗部（阴影）。调整Lift会向上或向下移动色彩的黑点，从而改变图像的整体明暗度，而不大幅影响高光。 Gamma（伽马）：影响图像的中间调。调整Gamma值是对图像中间亮度级别的调整，可以在不显著改变高光和阴影的情况下，增加或减少图像的对比度。 Gain（增益）：主要影响图像的亮部（高光）。调整Gain会增加或减少图像高光部分的亮度，而对暗部的影响较小。 代码： 123456789101112131415161718192021222324252627282930313233343536373839public static (Vector4, Vector4, Vector4) PrepareLiftGammaGain(in Vector4 inLift, in Vector4 inGamma, in Vector4 inGain)&#123; var lift = inLift; lift.x = Mathf.GammaToLinearSpace(lift.x) * 0.15f; lift.y = Mathf.GammaToLinearSpace(lift.y) * 0.15f; lift.z = Mathf.GammaToLinearSpace(lift.z) * 0.15f; float lumLift = Luminance(lift); lift.x = lift.x - lumLift + lift.w; lift.y = lift.y - lumLift + lift.w; lift.z = lift.z - lumLift + lift.w; lift.w = 0f; var gamma = inGamma; gamma.x = Mathf.GammaToLinearSpace(gamma.x) * 0.8f; gamma.y = Mathf.GammaToLinearSpace(gamma.y) * 0.8f; gamma.z = Mathf.GammaToLinearSpace(gamma.z) * 0.8f; float lumGamma = Luminance(gamma); gamma.w += 1f; gamma.x = 1f / Mathf.Max(gamma.x - lumGamma + gamma.w, 1e-03f); gamma.y = 1f / Mathf.Max(gamma.y - lumGamma + gamma.w, 1e-03f); gamma.z = 1f / Mathf.Max(gamma.z - lumGamma + gamma.w, 1e-03f); gamma.w = 0f; var gain = inGain; gain.x = Mathf.GammaToLinearSpace(gain.x) * 0.8f; gain.y = Mathf.GammaToLinearSpace(gain.y) * 0.8f; gain.z = Mathf.GammaToLinearSpace(gain.z) * 0.8f; float lumGain = Luminance(gain); gain.w += 1f; gain.x = gain.x - lumGain + gain.w; gain.y = gain.y - lumGain + gain.w; gain.z = gain.z - lumGain + gain.w; gain.w = 0f; return (lift, gamma, gain);&#125; 12colorLinear = colorLinear * _Gain.xyz + _Lift.xyz;colorLinear = sign(colorLinear) * pow(abs(colorLinear), _Gamma.xyz); ColorCurves（颜色曲线） 颜色曲线（Color Curves）是一种强大的工具，它允许用户通过调整色彩通道的输入和输出值之间的曲线关系来精细控制图像的亮度、对比度、色彩平衡和色调。色彩曲线提供了比基本亮度和对比度调节更细致和灵活的控制方式。 Unity的ColorCurves提供了两种类型的曲线：色彩映射曲线和YRGB曲线 色彩映射曲线 色彩映射曲线是在图像和视频后期处理中使用的常见方法，用于细粒度地调整色彩属性。尽管每种调节关注的色彩属性不同，它们都旨在通过不同的映射关系来改变图像的色彩表现。其中包括：HueVsHue、HueVsSat、SatVsSat和LumVsSat HueVsHue：这是色相对色相的调整，允许你根据原始色相改变色相。例如，可以将所有的绿色调整为更偏蓝的色相。 HueVsSat：这是色相对饱和度的调整，允许你基于图像中特定色相的存在来增加或减少饱和度。例如，可以仅增加红色的饱和度而不影响其他色彩。 SatVsSat：这是饱和度对饱和度的调整，允许你根据原始饱和度改变饱和度级别。这可以用来增强色彩鲜艳度或者使图像看起来更自然。 LumVsSat：这是亮度对饱和度的调整，允许你基于像素的亮度值来增加或减少饱和度。例如，可以减少亮度最高区域的饱和度来防止色彩过饱和。 YRGB曲线 YRGB曲线是一种用于图像处理和颜色校正中的工具，它允许用户分别调整图像中的亮度（Y）和红色（R）、绿色（G）、蓝色（B）三个颜色通道。通过调整这些曲线，可以影响图像的整体色调、对比度和颜色平衡。 Y曲线：Y通常代表亮度（Luminance）或亮度信息，通过调整Y曲线，可以不影响色彩的情况下调整图像的亮暗程度。这是因为Y曲线单独控制亮度信息，不直接改变色彩。 RGB曲线：RGB分别代表红色、绿色和蓝色三个颜色通道。通过独立调整每个颜色通道的曲线，可以控制图像中特定颜色的饱和度和色调。例如，提高红色曲线可以使图像看起来更暖，而调低蓝色曲线则会使图像看起来更凉。 过程： 将曲线值映射到大小128*1的纹理中，曲线值离散到128个值存在纹理。 在Shader中计算Lut中的值。 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 色彩映射曲线float satMult;float3 hsv = RgbToHsv(colorLinear);&#123; // Hue Vs Sat satMult = EvaluateCurve(_CurveHueVsSat, hsv.x) * 2.0; // Sat Vs Sat satMult *= EvaluateCurve(_CurveSatVsSat, hsv.y) * 2.0; // Lum Vs Sat satMult *= EvaluateCurve(_CurveLumVsSat, Luminance(colorLinear)) * 2.0; // Hue Vs Hue float hue = hsv.x + _HueSatCon.x; float offset = EvaluateCurve(_CurveHueVsHue, hue) - 0.5; hue += offset; hsv.x = RotateHue(hue, 0.0, 1.0);&#125;colorLinear = HsvToRgb(hsv);// 计算饱和度luma = GetLuminance(colorLinear);colorLinear = luma.xxx + (_HueSatCon.yyy * satMult) * (colorLinear - luma.xxx);// YRGB curves&#123; const float kHalfPixel = (1.0 / 128.0) / 2.0; float3 c = colorLinear; // Y (master) c += kHalfPixel.xxx; float mr = EvaluateCurve(_CurveMaster, c.r); float mg = EvaluateCurve(_CurveMaster, c.g); float mb = EvaluateCurve(_CurveMaster, c.b); c = float3(mr, mg, mb); // RGB c += kHalfPixel.xxx; float r = EvaluateCurve(_CurveRed, c.r); float g = EvaluateCurve(_CurveGreen, c.g); float b = EvaluateCurve(_CurveBlue, c.b); colorLinear = float3(r, g, b);&#125; Tonemapping（色调映射） 色调映射（Tonemapping）算法 Neutral Tonemapping和ACES Tonemapping是两种不同的色调映射（Tonemapping）算法，它们在处理图像时有一些区别： Neutral Tonemapping（中性色调映射）： 中性色调映射是一种简单的色调映射算法，旨在保持图像的整体对比度和亮度，并尽可能地保留原始图像的色彩和细节。 中性色调映射通常采用简单的灰度映射函数，将图像的亮度值进行线性或对数调整，以使得整个图像的亮度范围适应于显示设备的动态范围。 中性色调映射不太复杂，易于实现，并且通常用于一般的图像处理任务中。 ACES Tonemapping（Academy Color Encoding System色调映射）： ACES是一种广泛使用的颜色管理系统，旨在实现在各种不同设备和平台上的一致色彩表现。ACES Tonemapping是基于这一系统的色调映射算法。 ACES Tonemapping考虑了更多的颜色科学原理和视觉感知模型，以更好地模拟人眼对真实世界场景的感知。 ACES Tonemapping通常包括对色彩、对比度和亮度的调整，以实现更加自然和逼真的图像呈现效果。 ACES Tonemapping算法更复杂，需要更多的计算和参数调整，但可以产生更高质量的图像处理结果，特别是对于视觉特效和电影制作等专业领域。 代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 片原函数float4 FragLutBuilderHdr(Varyings input) : SV_Target&#123; //Lut 空间 我们使用 Alexa LogC (El 1000) 来存储 LUT，因为它提供了足够好的范围 (~58.85666)，并且足以存储在 fp16 中，而不会在暗部失去精度 float3 colorLutSpace = GetLutStripValue(input.texcoord, _Lut_Params); // 上面那些后处理步骤，只是会在不同的空间中计算 float3 gradedColor = ColorGrade(colorLutSpace); #ifdef HDR_COLORSPACE_CONVERSION //处理输出到HDR显示器的转换 gradedColor = ProcessColorForHDR(gradedColor); #else gradedColor = Tonemap(gradedColor); #endif return float4(gradedColor, 1.0);&#125;// 处理输入到HDR显示器时的转换float3 ProcessColorForHDR(float3 colorLinear)&#123; #ifdef HDR_COLORSPACE_CONVERSION #ifdef _TONEMAP_ACES float3 aces = ACEScg_to_ACES(colorLinear); return HDRMappingACES(aces.rgb, PaperWhite, MinNits, MaxNits, RangeReductionMode, true); #elif _TONEMAP_NEUTRAL return HDRMappingFromRec2020(colorLinear.rgb, PaperWhite, MinNits, MaxNits, RangeReductionMode, HueShift, true); #else // 在 Rec2020 中完成分级，转换为预期的色彩空间和 [0, 10k] 尼特范围 return RotateRec2020ToOutputSpace(colorLinear) * PaperWhite; #endif #endif return colorLinear;&#125;// 色调映射float3 Tonemap(float3 colorLinear)&#123; #if _TONEMAP_NEUTRAL &#123; colorLinear = NeutralTonemap(colorLinear); &#125; #elif _TONEMAP_ACES &#123; // Note: input is actually ACEScg (AP1 w/ linear encoding) float3 aces = ACEScg_to_ACES(colorLinear); colorLinear = AcesTonemap(aces); &#125; #endif return colorLinear;&#125; 视觉效果（VFX） 景深（Depth of Field，DOF） 景深后处理是一种用于模拟相机镜头产生的景深效果的图像处理技术。它可以在后期处理阶段模拟出景深的效果，使得图像中的某些部分变得模糊，而其他部分保持清晰，从而增强了图像的艺术感和视觉效果。 实现原理： 景深后处理的实现原理主要基于两个关键概念：景深和高斯模糊。 景深（Depth of Field，DOF）：景深是指相机镜头焦点前后一定范围内的物体都能保持清晰的范围。景深受到相机参数（如焦距、光圈大小）和拍摄场景的影响。在景深后处理中，通过模拟景深范围内的物体变得模糊来实现景深效果。 高斯模糊：高斯模糊是一种常用的图像模糊技术，它通过对图像中每个像素的周围像素应用高斯函数来降低像素的清晰度。模糊半径决定了模糊的程度，通常用于模拟景深中物体的模糊效果。 合成：将模糊处理后的图像与原始图像进行混合，以获得最终的景深效果。通常使用混合模式（如线性混合或增加混合）来调整模糊图像与原始图像之间的比例。 URP提供了两种模糊算法：高斯模糊和散景模糊，代码如下： C#部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119void DoDepthOfField(Camera camera, CommandBuffer cmd, RTHandle source, RTHandle destination, Rect pixelRect)&#123; // 高斯模糊方案 if (m_DepthOfField.mode.value == DepthOfFieldMode.Gaussian) DoGaussianDepthOfField(camera, cmd, source, destination, pixelRect); // 散景模糊方案 else if (m_DepthOfField.mode.value == DepthOfFieldMode.Bokeh) DoBokehDepthOfField(cmd, source, destination, pixelRect);&#125;// 设置高斯模糊参数void DoGaussianDepthOfField(Camera camera, CommandBuffer cmd, RTHandle source, RTHandle destination, Rect pixelRect)&#123; int downSample = 2; var material = m_Materials.gaussianDepthOfField; int wh = m_Descriptor.width / downSample; int hh = m_Descriptor.height / downSample; float farStart = m_DepthOfField.gaussianStart.value; float farEnd = Mathf.Max(farStart, m_DepthOfField.gaussianEnd.value); // 假设半径 1 在 1080p 下为 1。超过一定半径后，我们的高斯核看起来会非常糟糕，因此在非常高的分辨率 (4K+)时，我们将其限制它 float maxRadius = m_DepthOfField.gaussianMaxRadius.value * (wh / 1080f); maxRadius = Mathf.Min(maxRadius, 2f); CoreUtils.SetKeyword(material, ShaderKeywordStrings.HighQualitySampling, m_DepthOfField.highQualitySampling.value); material.SetVector(ShaderConstants._CoCParams, new Vector3(farStart, farEnd, maxRadius)); RenderingUtils.ReAllocateIfNeeded(ref m_FullCoCTexture, GetCompatibleDescriptor(m_Descriptor.width, m_Descriptor.height, m_GaussianCoCFormat), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_FullCoCTexture\"); RenderingUtils.ReAllocateIfNeeded(ref m_HalfCoCTexture, GetCompatibleDescriptor(wh, hh, m_GaussianCoCFormat), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_HalfCoCTexture\"); RenderingUtils.ReAllocateIfNeeded(ref m_PingTexture, GetCompatibleDescriptor(wh, hh, GraphicsFormat.R16G16B16A16_SFloat), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_PingTexture\"); RenderingUtils.ReAllocateIfNeeded(ref m_PongTexture, GetCompatibleDescriptor(wh, hh, GraphicsFormat.R16G16B16A16_SFloat), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_PongTexture\"); PostProcessUtils.SetSourceSize(cmd, m_Descriptor); cmd.SetGlobalVector(ShaderConstants._DownSampleScaleFactor, new Vector4(1.0f / downSample, 1.0f / downSample, downSample, downSample)); //Compute CoC 计算焦外圈，CoC是指焦平面上物体的像散焦在焦平面上的像上所形成的圆形面积，通常被称为\"焦外圆\"。Compute CoC是计算这个圆的大小的过程，它是景深模拟算法中的重要部分。景深模拟是一种技术，通过模拟相机对焦的效果，以便在数字图像中产生与真实世界中相似的景深效果。Compute CoC用于确定在图像中哪些部分是焦外的，以便进行适当的模糊处理，从而模拟真实的景深效果。 Blitter.BlitCameraTexture(cmd, source, m_FullCoCTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 0); // 降采样和预处理颜色和coc， m_MRT2[0] = m_HalfCoCTexture.nameID; m_MRT2[1] = m_PingTexture.nameID; cmd.SetGlobalTexture(ShaderConstants._FullCoCTexture, m_FullCoCTexture.nameID); CoreUtils.SetRenderTarget(cmd, m_MRT2, m_HalfCoCTexture); Vector2 viewportScale = source.useScaling ? new Vector2(source.rtHandleProperties.rtHandleScale.x, source.rtHandleProperties.rtHandleScale.y) : Vector2.one; Blitter.BlitTexture(cmd, source, viewportScale, material, 1); // 模糊 cmd.SetGlobalTexture(ShaderConstants._HalfCoCTexture, m_HalfCoCTexture.nameID); cmd.SetGlobalTexture(ShaderConstants._ColorTexture, source); Blitter.BlitCameraTexture(cmd, m_PingTexture, m_PongTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 2); Blitter.BlitCameraTexture(cmd, m_PongTexture, m_PingTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 3); // 合并 cmd.SetGlobalTexture(ShaderConstants._ColorTexture, m_PingTexture.nameID); cmd.SetGlobalTexture(ShaderConstants._FullCoCTexture, m_FullCoCTexture.nameID); Blitter.BlitCameraTexture(cmd, source, destination, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 4);&#125;// 设置散景模糊参数void DoBokehDepthOfField(CommandBuffer cmd, RTHandle source, RTHandle destination, Rect pixelRect)&#123; int downSample = 2; var material = m_Materials.bokehDepthOfField; int wh = m_Descriptor.width / downSample; int hh = m_Descriptor.height / downSample; // 生成合成图像的镜头和光圈相机模型[Potmesil81] //Potmesil（1981）的论文,提出了一种用于合成图像生成的镜头和光圈相机模型。该论文介绍了一种基于物理原理的相机模型，旨在模拟真实相机的成像过程，以生成高质量的合成图像。 //这个模型考虑了光线从场景中的对象经过透镜和光圈到达成像平面的过程。透镜模型通常用于描述光线的折射和聚焦效应，而光圈模型则用于控制进入相机的光线的量和分布。 //Potmesil 的相机模型还考虑了各种参数，例如透镜的焦距、光圈的直径以及相机与场景之间的距离等，以更准确地模拟真实相机的行为。这样的模型对于计算机图形学和合成图像生成非常有用，因为它们可以产生更逼真的图像，更好地模拟真实世界的光学效应。 float F = m_DepthOfField.focalLength.value / 1000f; float A = m_DepthOfField.focalLength.value / m_DepthOfField.aperture.value; float P = m_DepthOfField.focusDistance.value; float maxCoC = (A * F) / (P - F); float maxRadius = GetMaxBokehRadiusInPixels(m_Descriptor.height); float rcpAspect = 1f / (wh / (float)hh); CoreUtils.SetKeyword(material, ShaderKeywordStrings.UseFastSRGBLinearConversion, m_UseFastSRGBLinearConversion); cmd.SetGlobalVector(ShaderConstants._CoCParams, new Vector4(P, maxCoC, maxRadius, rcpAspect)); // 准备散景内核参数 int hash = m_DepthOfField.GetHashCode(); if (hash != m_BokehHash || maxRadius != m_BokehMaxRadius || rcpAspect != m_BokehRCPAspect) &#123; m_BokehHash = hash; m_BokehMaxRadius = maxRadius; m_BokehRCPAspect = rcpAspect; PrepareBokehKernel(maxRadius, rcpAspect); &#125; cmd.SetGlobalVectorArray(ShaderConstants._BokehKernel, m_BokehKernel); RenderingUtils.ReAllocateIfNeeded(ref m_FullCoCTexture, GetCompatibleDescriptor(m_Descriptor.width, m_Descriptor.height, GraphicsFormat.R8_UNorm), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_FullCoCTexture\"); RenderingUtils.ReAllocateIfNeeded(ref m_PingTexture, GetCompatibleDescriptor(wh, hh, GraphicsFormat.R16G16B16A16_SFloat), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_PingTexture\"); RenderingUtils.ReAllocateIfNeeded(ref m_PongTexture, GetCompatibleDescriptor(wh, hh, GraphicsFormat.R16G16B16A16_SFloat), FilterMode.Bilinear, TextureWrapMode.Clamp, name: \"_PongTexture\"); PostProcessUtils.SetSourceSize(cmd, m_Descriptor); cmd.SetGlobalVector(ShaderConstants._DownSampleScaleFactor, new Vector4(1.0f / downSample, 1.0f / downSample, downSample, downSample)); float uvMargin = (1.0f / m_Descriptor.height) * downSample; cmd.SetGlobalVector(ShaderConstants._BokehConstants, new Vector4(uvMargin, uvMargin * 2.0f)); // Compute CoC 计算焦外圈 Blitter.BlitCameraTexture(cmd, source, m_FullCoCTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 0); cmd.SetGlobalTexture(ShaderConstants._FullCoCTexture, m_FullCoCTexture.nameID); //降采样和预处理颜色和coc Blitter.BlitCameraTexture(cmd, source, m_PingTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 1); // 散景模糊 Blitter.BlitCameraTexture(cmd, m_PingTexture, m_PongTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 2); // 散景模糊后再使用9-tap tent滤波核进行模糊，“9-tap tent filter” 意味着您想要应用一个 9个采样点的滤波器，滤波器的形状类似于一个帐篷（tent）。然而我们使用4个双线性（bilinear）采样点线性插值的方式来计算这9个采样点的值。 Blitter.BlitCameraTexture(cmd, m_PongTexture, m_PingTexture, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 3); // 合并 cmd.SetGlobalTexture(ShaderConstants._DofTexture, m_PingTexture.nameID); Blitter.BlitCameraTexture(cmd, source, destination, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, material, 4);&#125; Shader部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557// 高斯模糊的景深效果Shader \"Hidden/Universal Render Pipeline/GaussianDepthOfField\"&#123; HLSLINCLUDE #pragma target 3.5 #pragma exclude_renderers gles #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Filtering.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl\" #include \"Packages/com.unity.render-pipelines.core/Runtime/Utilities/Blit.hlsl\" TEXTURE2D_X(_ColorTexture); TEXTURE2D_X(_FullCoCTexture); TEXTURE2D_X(_HalfCoCTexture); float4 _SourceSize; float4 _DownSampleScaleFactor; float3 _CoCParams; #define FarStart _CoCParams.x #define FarEnd _CoCParams.y #define MaxRadius _CoCParams.z #define BLUR_KERNEL 0 #if BLUR_KERNEL == 0 // 可分离双线性 3-tap 高斯滤波器，相当于5-tap滤波 const static int kTapCount = 3; const static float kOffsets[] = &#123; -1.33333333, 0.00000000, 1.33333333 &#125;; const static half kCoeffs[] = &#123; 0.35294118, 0.29411765, 0.35294118 &#125;; #elif BLUR_KERNEL == 1 // // 可分离双线性 5-tap 高斯滤波器，相当于9-tap滤波 const static int kTapCount = 5; const static float kOffsets[] = &#123; -3.23076923, -1.38461538, 0.00000000, 1.38461538, 3.23076923 &#125;; const static half kCoeffs[] = &#123; 0.07027027, 0.31621622, 0.22702703, 0.31621622, 0.07027027 &#125;; #endif // 计算Coc half FragCoC(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); float depth = LOAD_TEXTURE2D_X(_CameraDepthTexture, _SourceSize.xy * uv).x; depth = LinearEyeDepth(depth, _ZBufferParams); half coc = (depth - FarStart) / (FarEnd - FarStart); return saturate(coc); &#125; struct PrefilterOutput &#123; half coc : SV_Target0; half3 color : SV_Target1; &#125;; // 进行滤波处理 PrefilterOutput FragPrefilter(Varyings input) &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); #if _HIGH_QUALITY_SAMPLING //《High Quality Antialiasing》（Lorach07）提出了使用旋转网格来最小化水平和垂直边界带来的伪影。这种技术旨在改善图形渲染中的抗锯齿效果，特别是在处理水平和垂直边界时。 //这种方法的核心思想是，通过旋转采样网格，使得锯齿边缘的能量在整个图像上更加均匀地分布。通常情况下，水平和垂直边界会导致锯齿效应更加显著，因为它们的方向与像素阵列的方向相对应。通过旋转网格，可以打破这种规律，减少锯齿效应。 //具体实现时，可以在像素级别上应用旋转网格。这意味着在像素着色器中，对于每个像素，都可以使用不同的旋转网格来进行采样。这样，即使在处理水平和垂直边界时，也可以获得更加均匀的采样，从而减少锯齿效应。 //通过这种方法，可以提高图形渲染的质量，特别是对于需要处理锯齿效应的场景，例如渲染具有许多直线或边缘的场景，如建筑物或栅格图形。 const int kCount = 5; const float2 kTaps[] = &#123; float2( 0.0, 0.0), float2( 0.9, -0.4), float2(-0.9, 0.4), float2( 0.4, 0.9), float2(-0.4, -0.9) &#125;; half3 colorAcc = 0.0; half farCoCAcc = 0.0; UNITY_UNROLL for (int i = 0; i &lt; kCount; i++) &#123; float2 tapCoord = _SourceSize.zw * kTaps[i] + uv; half3 tapColor = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, tapCoord).xyz; half coc = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, tapCoord).x; // 预先乘以焦平面模糊（CoC）可以帮助减少背景模糊向聚焦区域的渗透。这个技术常用于渲染引擎中，以改善焦外区域的逼真度，同时保持焦点区域的清晰度。 colorAcc += tapColor * coc; farCoCAcc += coc; &#125; half3 color = colorAcc * rcp(kCount); half farCoC = farCoCAcc * rcp(kCount); #else //理论上，对 CoC 进行双线性采样可能不太准确，因为 CoC 的大小通常不是线性变化的。然而，在某些情况下，为了提高速度和简化实现，可以选择使用双线性采样来近似 CoC。 //双线性采样是一种简单且快速的插值方法，适用于许多图形渲染情景。它通过对四个最近的像素进行加权平均来估计一个给定位置的值。虽然这种方法在处理 CoC 时可能会引入一些误差，但在实践中，它通常可以提供足够的准确性，特别是在需要快速渲染的情况下。 half farCoC = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv).x; //快速的双线性下采样源目标，并预先将 CoC 与颜色值相乘以减少背景模糊渗透到焦点区域 half3 color = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv).xyz; color *= farCoC; #endif PrefilterOutput o; o.coc = farCoC; o.color = color; return o; &#125; half4 Blur(Varyings input, float2 dir, float premultiply) &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); // Use the center CoC as radius int2 positionSS = int2(_SourceSize.xy * _DownSampleScaleFactor.xy * uv); half samp0CoC = LOAD_TEXTURE2D_X(_HalfCoCTexture, positionSS).x; float2 offset = _SourceSize.zw * _DownSampleScaleFactor.zw * dir * samp0CoC * MaxRadius; half4 acc = 0.0; UNITY_UNROLL for (int i = 0; i &lt; kTapCount; i++) &#123; float2 sampCoord = uv + kOffsets[i] * offset; half sampCoC = SAMPLE_TEXTURE2D_X(_HalfCoCTexture, sampler_LinearClamp, sampCoord).x; half3 sampColor = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, sampCoord).xyz; // Weight &amp; pre-multiply to limit bleeding on the focused area half weight = saturate(1.0 - (samp0CoC - sampCoC)); acc += half4(sampColor, premultiply ? sampCoC : 1.0) * kCoeffs[i] * weight; &#125; acc.xyz /= acc.w + 1e-4; // Zero-div guard return half4(acc.xyz, 1.0); &#125; half4 FragBlurH(Varyings input) : SV_Target &#123; return Blur(input, float2(1.0, 0.0), 1.0); &#125; half4 FragBlurV(Varyings input) : SV_Target &#123; return Blur(input, float2(0.0, 1.0), 0.0); &#125; half4 FragComposite(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); half3 baseColor = LOAD_TEXTURE2D_X(_BlitTexture, _SourceSize.xy * uv).xyz; half coc = LOAD_TEXTURE2D_X(_FullCoCTexture, _SourceSize.xy * uv).x; #if _HIGH_QUALITY_SAMPLING &amp;&amp; !defined(SHADER_API_GLES) half3 farColor = SampleTexture2DBicubic(TEXTURE2D_X_ARGS(_ColorTexture, sampler_LinearClamp), uv, _SourceSize * _DownSampleScaleFactor, 1.0, unity_StereoEyeIndex).xyz; #else half3 farColor = SAMPLE_TEXTURE2D_X(_ColorTexture, sampler_LinearClamp, uv).xyz; #endif half3 dstColor = 0.0; half dstAlpha = 1.0; UNITY_BRANCH if (coc &gt; 0.0) &#123; // Non-linear blend // \"CryEngine 3 Graphics Gems\" [Sousa13] half blend = sqrt(coc * TWO_PI); dstColor = farColor * saturate(blend); dstAlpha = saturate(1.0 - blend); &#125; return half4(baseColor * dstAlpha + dstColor, 1.0); &#125; ENDHLSL SubShader &#123; Tags &#123; \"RenderPipeline\" = \"UniversalPipeline\" &#125; LOD 100 ZTest Always ZWrite Off Cull Off Pass &#123; Name \"Gaussian Depth Of Field CoC\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragCoC ENDHLSL &#125; Pass &#123; Name \"Gaussian Depth Of Field Prefilter\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragPrefilter #pragma multi_compile_local _ _HIGH_QUALITY_SAMPLING ENDHLSL &#125; Pass &#123; Name \"Gaussian Depth Of Field Blur Horizontal\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragBlurH ENDHLSL &#125; Pass &#123; Name \"Gaussian Depth Of Field Blur Vertical\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragBlurV ENDHLSL &#125; Pass &#123; Name \"Gaussian Depth Of Field Composite\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragComposite #pragma multi_compile_local _ _HIGH_QUALITY_SAMPLING ENDHLSL &#125; &#125;&#125;// 散景模糊的景深效果Shader \"Hidden/Universal Render Pipeline/BokehDepthOfField\"&#123; HLSLINCLUDE #pragma exclude_renderers gles #pragma multi_compile_local_fragment _ _USE_FAST_SRGB_LINEAR_CONVERSION #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/DeclareDepthTexture.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/Shaders/PostProcessing/Common.hlsl\" // Do not change this without changing PostProcessPass.PrepareBokehKernel() #define SAMPLE_COUNT 42 // Toggle this to reduce flickering - note that it will reduce overall bokeh energy and add // a small cost to the pre-filtering pass #define COC_LUMA_WEIGHTING 0 TEXTURE2D_X(_DofTexture); TEXTURE2D_X(_FullCoCTexture); half4 _SourceSize; half4 _HalfSourceSize; half4 _DownSampleScaleFactor; half4 _CoCParams; half4 _BokehKernel[SAMPLE_COUNT]; half4 _BokehConstants; #define FocusDist _CoCParams.x #define MaxCoC _CoCParams.y #define MaxRadius _CoCParams.z #define RcpAspect _CoCParams.w half FragCoC(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); float depth = LOAD_TEXTURE2D_X(_CameraDepthTexture, _SourceSize.xy * uv).x; float linearEyeDepth = LinearEyeDepth(depth, _ZBufferParams); half coc = (1.0 - FocusDist / linearEyeDepth) * MaxCoC; half nearCoC = clamp(coc, -1.0, 0.0); half farCoC = saturate(coc); return saturate((farCoC + nearCoC + 1.0) * 0.5); &#125; half4 FragPrefilter(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); #if SHADER_TARGET &gt;= 45 &amp;&amp; defined(PLATFORM_SUPPORT_GATHER) // Sample source colors half4 cr = GATHER_RED_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv); half4 cg = GATHER_GREEN_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv); half4 cb = GATHER_BLUE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv); half3 c0 = half3(cr.x, cg.x, cb.x); half3 c1 = half3(cr.y, cg.y, cb.y); half3 c2 = half3(cr.z, cg.z, cb.z); half3 c3 = half3(cr.w, cg.w, cb.w); // Sample CoCs half4 cocs = GATHER_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv) * 2.0 - 1.0; half coc0 = cocs.x; half coc1 = cocs.y; half coc2 = cocs.z; half coc3 = cocs.w; #else float3 duv = _SourceSize.zwz * float3(0.5, 0.5, -0.5); float2 uv0 = uv - duv.xy; float2 uv1 = uv - duv.zy; float2 uv2 = uv + duv.zy; float2 uv3 = uv + duv.xy; // Sample source colors half3 c0 = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv0).xyz; half3 c1 = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv1).xyz; half3 c2 = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv2).xyz; half3 c3 = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv3).xyz; // Sample CoCs half coc0 = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv0).x * 2.0 - 1.0; half coc1 = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv1).x * 2.0 - 1.0; half coc2 = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv2).x * 2.0 - 1.0; half coc3 = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv3).x * 2.0 - 1.0; #endif #if COC_LUMA_WEIGHTING // Apply CoC and luma weights to reduce bleeding and flickering half w0 = abs(coc0) / (Max3(c0.x, c0.y, c0.z) + 1.0); half w1 = abs(coc1) / (Max3(c1.x, c1.y, c1.z) + 1.0); half w2 = abs(coc2) / (Max3(c2.x, c2.y, c2.z) + 1.0); half w3 = abs(coc3) / (Max3(c3.x, c3.y, c3.z) + 1.0); // Weighted average of the color samples half3 avg = c0 * w0 + c1 * w1 + c2 * w2 + c3 * w3; avg /= max(w0 + w1 + w2 + w3, 1e-5); #else half3 avg = (c0 + c1 + c2 + c3) / 4.0; #endif // Select the largest CoC value half cocMin = min(coc0, Min3(coc1, coc2, coc3)); half cocMax = max(coc0, Max3(coc1, coc2, coc3)); half coc = (-cocMin &gt; cocMax ? cocMin : cocMax) * MaxRadius; // Premultiply CoC avg *= smoothstep(0, _SourceSize.w * 2.0, abs(coc)); #if defined(UNITY_COLORSPACE_GAMMA) avg = GetSRGBToLinear(avg); #endif return half4(avg, coc); &#125; void Accumulate(half4 samp0, float2 uv, half4 disp, inout half4 farAcc, inout half4 nearAcc) &#123; half4 samp = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + disp.wy); // Compare CoC of the current sample and the center sample and select smaller one half farCoC = max(min(samp0.a, samp.a), 0.0); // Compare the CoC to the sample distance &amp; add a small margin to smooth out half farWeight = saturate((farCoC - disp.z + _BokehConstants.y) / _BokehConstants.y); half nearWeight = saturate((-samp.a - disp.z + _BokehConstants.y) / _BokehConstants.y); // Cut influence from focused areas because they're darkened by CoC premultiplying. This is only // needed for near field nearWeight *= step(_BokehConstants.x, -samp.a); // Accumulation farAcc += half4(samp.rgb, 1.0h) * farWeight; nearAcc += half4(samp.rgb, 1.0h) * nearWeight; &#125; half4 FragBlur(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); half4 samp0 = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv); half4 farAcc = 0.0; // Background: far field bokeh half4 nearAcc = 0.0; // Foreground: near field bokeh // Center sample isn't in the kernel array, accumulate it separately Accumulate(samp0, uv, 0.0, farAcc, nearAcc); UNITY_LOOP for (int si = 0; si &lt; SAMPLE_COUNT; si++) &#123; Accumulate(samp0, uv, _BokehKernel[si], farAcc, nearAcc); &#125; // Get the weighted average farAcc.rgb /= farAcc.a + (farAcc.a == 0.0); // Zero-div guard nearAcc.rgb /= nearAcc.a + (nearAcc.a == 0.0); // Normalize the total of the weights for the near field nearAcc.a *= PI / (SAMPLE_COUNT + 1); // Alpha premultiplying half alpha = saturate(nearAcc.a); half3 rgb = lerp(farAcc.rgb, nearAcc.rgb, alpha); return half4(rgb, alpha); &#125; half4 FragPostBlur(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); // 9-tap tent filter with 4 bilinear samples float4 duv = _SourceSize.zwzw * _DownSampleScaleFactor.zwzw * float4(0.5, 0.5, -0.5, 0); half4 acc; acc = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - duv.xy); acc += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - duv.zy); acc += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + duv.zy); acc += SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + duv.xy); return acc * 0.25; &#125; half4 FragComposite(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); half4 dof = SAMPLE_TEXTURE2D_X(_DofTexture, sampler_LinearClamp, uv); half coc = SAMPLE_TEXTURE2D_X(_FullCoCTexture, sampler_LinearClamp, uv).r; coc = (coc - 0.5) * 2.0 * MaxRadius; // Convert CoC to far field alpha value float ffa = smoothstep(_SourceSize.w * 2.0, _SourceSize.w * 4.0, coc); half4 color = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv); #if defined(UNITY_COLORSPACE_GAMMA) color = GetSRGBToLinear(color); #endif half alpha = Max3(dof.r, dof.g, dof.b); color = lerp(color, half4(dof.rgb, alpha), ffa + dof.a - ffa * dof.a); #if defined(UNITY_COLORSPACE_GAMMA) color = GetLinearToSRGB(color); #endif return color; &#125; ENDHLSL SubShader &#123; Tags &#123; \"RenderPipeline\" = \"UniversalPipeline\" &#125; LOD 100 ZTest Always ZWrite Off Cull Off Pass &#123; Name \"Bokeh Depth Of Field CoC\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragCoC #pragma target 4.5 ENDHLSL &#125; Pass &#123; Name \"Bokeh Depth Of Field Prefilter\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragPrefilter #pragma target 4.5 ENDHLSL &#125; Pass &#123; Name \"Bokeh Depth Of Field Blur\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragBlur #pragma target 4.5 ENDHLSL &#125; Pass &#123; Name \"Bokeh Depth Of Field Post Blur\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragPostBlur #pragma target 4.5 ENDHLSL &#125; Pass &#123; Name \"Bokeh Depth Of Field Composite\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragComposite #pragma target 4.5 ENDHLSL &#125; &#125;&#125; 运动模糊（Motion Blur） Motion Blur 的后处理实现原理是在图像已经捕获或生成后，在整个图像上应用模糊效果，以模拟相机或物体运动时的模糊效果。 以下是 Motion Blur 后处理的基本原理： 1. 获取速度信息： 在图像处理中，速度信息通常是由每个像素的运动向量表示的。这些向量可以通过两个连续帧之间的像素位移或动作估计算法（如光流法）来获取。 2. 计算模糊效果： 一旦获得了速度信息，就可以根据每个像素的运动向量来计算应用于该像素的模糊程度。通常情况下，速度越高的像素，应用的模糊效果就越强烈。 3. 模糊处理： 一种常见的模糊方法是使用卷积核。对于每个像素，可以根据其周围像素的权重来计算模糊效果。这可以通过高斯模糊、运动模糊或其他模糊核来实现。 4. 混合： 最后，将计算得到的模糊效果应用到原始图像上。通常采用像素值的混合或者合成技术来实现。混合的方式可以是简单的加权平均，也可以是更复杂的技术，如逐像素运动模糊等。 核心函数代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 计算每个像素的移动速度half2 GetCameraVelocity(float4 uv)&#123; #if UNITY_REVERSED_Z half depth = SampleSceneDepth(uv.xy).x; #else half depth = lerp(UNITY_NEAR_CLIP_VALUE, 1, SampleSceneDepth(uv.xy).x); #endif float4 worldPos = float4(ComputeWorldSpacePosition(uv.xy, depth, UNITY_MATRIX_I_VP), 1.0); float4 prevClipPos = mul(_PrevViewProjM, worldPos); float4 curClipPos = mul(_ViewProjM, worldPos); half2 prevPosCS = prevClipPos.xy / prevClipPos.w; half2 curPosCS = curClipPos.xy / curClipPos.w; // Backwards motion vectors half2 velocity = (prevPosCS - curPosCS); #if UNITY_UV_STARTS_AT_TOP velocity.y = -velocity.y; #endif return ClampVelocity(velocity, _Clamp);&#125;half3 GatherSample(half sampleNumber, half2 velocity, half invSampleCount, float2 centerUV, half randomVal, half velocitySign)&#123; half offsetLength = (sampleNumber + 0.5h) + (velocitySign * (randomVal - 0.5h)); float2 sampleUV = centerUV + (offsetLength * invSampleCount) * velocity * velocitySign; return SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_PointClamp, sampleUV).xyz;&#125;half4 DoMotionBlur(VaryingsCMB input, int iterations)&#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord.xy); half2 velocity = GetCameraVelocity(float4(uv, input.texcoord.zw)) * _Intensity; half randomVal = InterleavedGradientNoise(uv * _SourceSize.xy, 0); half invSampleCount = rcp(iterations * 2.0); half3 color = 0.0; UNITY_UNROLL for (int i = 0; i &lt; iterations; i++) &#123; color += GatherSample(i, velocity, invSampleCount, uv, randomVal, -1.0); color += GatherSample(i, velocity, invSampleCount, uv, randomVal, 1.0); &#125; return half4(color * invSampleCount, 1.0);&#125; Panini投影（Panini Projection） Panini Projection 是一种透视投影技术，它可以在保持图像的透视感的同时，扭曲图像以适应更宽的视角，从而产生一种类似鱼眼镜头的效果。实现 Panini Projection 的后处理通常涉及以下步骤： 1. 确定投影参数： 首先，需要确定用于 Panini 投影的参数，包括视角（FOV）、压缩因子等。这些参数将影响最终投影效果的弯曲程度和透视感。 2. 图像扭曲： 将原始图像应用于 Panini 投影的算法，对图像进行扭曲。这通常涉及到对图像中的每个像素进行重新定位，以适应所选的投影参数。在这一步中，图像中的像素位置会根据所选的参数进行重新映射，以产生弯曲的效果。 3. 插值和填充： 由于进行投影扭曲可能会导致某些像素位置在新图像中没有对应的值，因此需要进行插值和填充处理。通常采用的插值方法包括双线性插值、双三次插值等，以确保图像的平滑过渡和连续性。 4. 边缘处理： 在扭曲后的图像边缘可能会出现拉伸或压缩的情况，因此需要进行边缘处理以消除这种失真。常见的方法包括使用遮罩或者边界像素的加权平均来平滑边缘过渡。 总的来说，Panini Projection 的后处理实现原理涉及将原始图像应用于 Panini 投影算法，对图像进行扭曲和重新映射，然后进行插值、填充和边缘处理，最终得到扭曲的图像以产生所需的透视效果。 核心函数代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798float2 Panini_UnitDistance(float2 view_pos)&#123; // Given // S----------- E--X------- // | ` . /,´ // |-- --- Q // 1 | ,´/ ` // | ,´ / ´ // | ,´ / ` // | ,´ / . // O` / . // | / ` // | / ´ // 1 | / ´ // | / ´ // |/_ . ´ // P // // Have E // Want to find X // // First apply tangent-secant theorem to find Q // PE*QE = SE*SE // QE = PE-PQ // PQ = PE-(SE*SE)/PE // Q = E*(PQ/PE) // Then project Q to find X const float d = 1.0; const float view_dist = 2.0; const float view_dist_sq = 4.0; float view_hyp = sqrt(view_pos.x * view_pos.x + view_dist_sq); float cyl_hyp = view_hyp - (view_pos.x * view_pos.x) / view_hyp; float cyl_hyp_frac = cyl_hyp / view_hyp; float cyl_dist = view_dist * cyl_hyp_frac; float2 cyl_pos = view_pos * cyl_hyp_frac; return cyl_pos / (cyl_dist - d);&#125;float2 Panini_Generic(float2 view_pos, float d)&#123; // Given // S----------- E--X------- // | ` ~. /,´ // |-- --- Q // | ,/ ` // 1 | ,´/ ` // | ,´ / ´ // | ,´ / ´ // |,` / , // O / // | / , // d | / // | / , // |/ . // P // | ´ // | , ´ // +- ´ // // Have E // Want to find X // // First compute line-circle intersection to find Q // Then project Q to find X float view_dist = 1.0 + d; float view_hyp_sq = view_pos.x * view_pos.x + view_dist * view_dist; float isect_D = view_pos.x * d; float isect_discrim = view_hyp_sq - isect_D * isect_D; float cyl_dist_minus_d = (-isect_D * view_pos.x + view_dist * sqrt(isect_discrim)) / view_hyp_sq; float cyl_dist = cyl_dist_minus_d + d; float2 cyl_pos = view_pos * (cyl_dist / view_dist); return cyl_pos / (cyl_dist - d);&#125;half4 FragPaniniProjection(Varyings input) : SV_Target&#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 view_pos = (2.0 * input.texcoord - 1.0) * _Params.xy * _Params.w; #if _GENERIC float2 proj_pos = Panini_Generic(view_pos, _Params.z); #else // _UNIT_DISTANCE float2 proj_pos = Panini_UnitDistance(view_pos); #endif float2 proj_ndc = proj_pos / _Params.xy; float2 coords = proj_ndc * 0.5 + 0.5; return SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, coords);&#125; 泛光（Bloom） 泛光（Bloom）后处理是一种常用的图像处理技术，用于增强图像中亮度较高区域的效果，使其产生发光的感觉。其原理如下： 1. 提取高亮区域： 首先，对原始图像进行处理，提取出高亮区域。这些高亮区域通常是亮度值较高的像素，可以通过阈值处理或者高通滤波器等方法来提取。 2. 生成泛光图： 将提取出的高亮区域进行模糊处理，生成泛光图。这一步可以使用高斯模糊、径向模糊等模糊算法，使高亮区域周围产生较大的光晕效果。 3. 叠加到原始图像： 将生成的泛光图与原始图像进行叠加。通常情况下，叠加时会根据泛光图中的亮度值进行加权叠加，使高亮区域的发光效果更加明显。 C#代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111void SetupBloom(CommandBuffer cmd, RTHandle source, Material uberMaterial)&#123; // 降分辨率 int downres = 1; switch (m_Bloom.downscale.value) &#123; case BloomDownscaleMode.Half: downres = 1; break; case BloomDownscaleMode.Quarter: downres = 2; break; default: throw new System.ArgumentOutOfRangeException(); &#125; int tw = m_Descriptor.width &gt;&gt; downres; int th = m_Descriptor.height &gt;&gt; downres; // 确定迭代次数 int maxSize = Mathf.Max(tw, th); int iterations = Mathf.FloorToInt(Mathf.Log(maxSize, 2f) - 1); int mipCount = Mathf.Clamp(iterations, 1, m_Bloom.maxIterations.value); // 过滤器参数 float clamp = m_Bloom.clamp.value; float threshold = Mathf.GammaToLinearSpace(m_Bloom.threshold.value); float thresholdKnee = threshold * 0.5f; // Hardcoded soft knee // 设置材质参数 float scatter = Mathf.Lerp(0.05f, 0.95f, m_Bloom.scatter.value); var bloomMaterial = m_Materials.bloom; bloomMaterial.SetVector(ShaderConstants._Params, new Vector4(scatter, clamp, threshold, thresholdKnee)); CoreUtils.SetKeyword(bloomMaterial, ShaderKeywordStrings.BloomHQ, m_Bloom.highQualityFiltering.value); CoreUtils.SetKeyword(bloomMaterial, ShaderKeywordStrings.UseRGBM, m_UseRGBM); // 创建RT var desc = GetCompatibleDescriptor(tw, th, m_DefaultHDRFormat); for (int i = 0; i &lt; mipCount; i++) &#123; RenderingUtils.ReAllocateIfNeeded(ref m_BloomMipUp[i], desc, FilterMode.Bilinear, TextureWrapMode.Clamp, name: m_BloomMipUp[i].name); RenderingUtils.ReAllocateIfNeeded(ref m_BloomMipDown[i], desc, FilterMode.Bilinear, TextureWrapMode.Clamp, name: m_BloomMipDown[i].name); desc.width = Mathf.Max(1, desc.width &gt;&gt; 1); desc.height = Mathf.Max(1, desc.height &gt;&gt; 1); &#125; // 将原图Blit到m_BloomMipDown[0] Blitter.BlitCameraTexture(cmd, source, m_BloomMipDown[0], RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, bloomMaterial, 0); //Gaussian Pyramid是一种图像金字塔构建方法，通常用于图像处理和计算机视觉中的多尺度分析。它的基本思想是通过逐级下采样来生成一系列分辨率逐渐降低的图像。具体而言，它的操作步骤如下： //高斯模糊（Gaussian Blur）：首先，对原始图像进行高斯模糊操作，这有助于去除图像中的高频细节，使图像更加平滑。 //下采样（Downsampling）：然后，对模糊后的图像进行下采样操作，即将图像尺寸减小为原始图像的一半（或其他比例），这样就得到了一个分辨率降低的图像。 //重复操作：这两个步骤可以迭代多次，每次都在前一级图像上进行高斯模糊和下采样操作，生成更低分辨率的图像。 var lastDown = m_BloomMipDown[0]; for (int i = 1; i &lt; mipCount; i++) &#123; Blitter.BlitCameraTexture(cmd, lastDown, m_BloomMipUp[i], RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, bloomMaterial, 1); Blitter.BlitCameraTexture(cmd, m_BloomMipUp[i], m_BloomMipDown[i], RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, bloomMaterial, 2); lastDown = m_BloomMipDown[i]; &#125; //上采样（默认情况下为双线性，HQ 过滤采用双三次) for (int i = mipCount - 2; i &gt;= 0; i--) &#123; var lowMip = (i == mipCount - 2) ? m_BloomMipDown[i + 1] : m_BloomMipUp[i + 1]; var highMip = m_BloomMipDown[i]; var dst = m_BloomMipUp[i]; cmd.SetGlobalTexture(ShaderConstants._SourceTexLowMip, lowMip); Blitter.BlitCameraTexture(cmd, highMip, dst, RenderBufferLoadAction.DontCare, RenderBufferStoreAction.Store, bloomMaterial, 3); &#125; //在 uber 上设置 Bloom var tint = m_Bloom.tint.value.linear; var luma = ColorUtils.Luminance(tint); tint = luma &gt; 0f ? tint * (1f / luma) : Color.white; var bloomParams = new Vector4(m_Bloom.intensity.value, tint.r, tint.g, tint.b); uberMaterial.SetVector(ShaderConstants._Bloom_Params, bloomParams); uberMaterial.SetFloat(ShaderConstants._Bloom_RGBM, m_UseRGBM ? 1f : 0f); cmd.SetGlobalTexture(ShaderConstants._Bloom_Texture, m_BloomMipUp[0]); // 在uber上设置镜头污垢参数 // 保持纵横比正确并将污垢纹理居中，不被拉伸或挤压 var dirtTexture = m_Bloom.dirtTexture.value == null ? Texture2D.blackTexture : m_Bloom.dirtTexture.value; float dirtRatio = dirtTexture.width / (float)dirtTexture.height; float screenRatio = m_Descriptor.width / (float)m_Descriptor.height; var dirtScaleOffset = new Vector4(1f, 1f, 0f, 0f); float dirtIntensity = m_Bloom.dirtIntensity.value; if (dirtRatio &gt; screenRatio) &#123; dirtScaleOffset.x = screenRatio / dirtRatio; dirtScaleOffset.z = (1f - dirtScaleOffset.x) * 0.5f; &#125; else if (screenRatio &gt; dirtRatio) &#123; dirtScaleOffset.y = dirtRatio / screenRatio; dirtScaleOffset.w = (1f - dirtScaleOffset.y) * 0.5f; &#125; uberMaterial.SetVector(ShaderConstants._LensDirt_Params, dirtScaleOffset); uberMaterial.SetFloat(ShaderConstants._LensDirt_Intensity, dirtIntensity); uberMaterial.SetTexture(ShaderConstants._LensDirt_Texture, dirtTexture); if (m_Bloom.highQualityFiltering.value) uberMaterial.EnableKeyword(dirtIntensity &gt; 0f ? ShaderKeywordStrings.BloomHQDirt : ShaderKeywordStrings.BloomHQ); else uberMaterial.EnableKeyword(dirtIntensity &gt; 0f ? ShaderKeywordStrings.BloomLQDirt : ShaderKeywordStrings.BloomLQ);&#125; Shader代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221Shader \"Hidden/Universal Render Pipeline/Bloom\"&#123; HLSLINCLUDE #pragma exclude_renderers gles #pragma multi_compile_local _ _USE_RGBM #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Filtering.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" #include \"Packages/com.unity.render-pipelines.core/Runtime/Utilities/Blit.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/FoveatedRendering.hlsl\" float4 _BlitTexture_TexelSize; TEXTURE2D_X(_SourceTexLowMip); float4 _SourceTexLowMip_TexelSize; float4 _Params; // x: scatter, y: clamp, z: threshold (linear), w: threshold knee #define Scatter _Params.x #define ClampMax _Params.y #define Threshold _Params.z #define ThresholdKnee _Params.w half4 EncodeHDR(half3 color) &#123; #if _USE_RGBM half4 outColor = EncodeRGBM(color); #else half4 outColor = half4(color, 1.0); #endif #if UNITY_COLORSPACE_GAMMA return half4(sqrt(outColor.xyz), outColor.w); // linear to γ #else return outColor; #endif &#125; half3 DecodeHDR(half4 color) &#123; #if UNITY_COLORSPACE_GAMMA color.xyz *= color.xyz; // γ to linear #endif #if _USE_RGBM return DecodeRGBM(color); #else return color.xyz; #endif &#125; half4 FragPrefilter(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord);#if defined(_FOVEATED_RENDERING_NON_UNIFORM_RASTER) uv = RemapFoveatedRenderingResolve(uv);#endif #if _BLOOM_HQ float texelSize = _BlitTexture_TexelSize.x; half4 A = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(-1.0, -1.0)); half4 B = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(0.0, -1.0)); half4 C = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(1.0, -1.0)); half4 D = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(-0.5, -0.5)); half4 E = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(0.5, -0.5)); half4 F = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(-1.0, 0.0)); half4 G = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv); half4 H = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(1.0, 0.0)); half4 I = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(-0.5, 0.5)); half4 J = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(0.5, 0.5)); half4 K = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(-1.0, 1.0)); half4 L = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(0.0, 1.0)); half4 M = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + texelSize * float2(1.0, 1.0)); half2 div = (1.0 / 4.0) * half2(0.5, 0.125); half4 o = (D + E + I + J) * div.x; o += (A + B + G + F) * div.y; o += (B + C + H + G) * div.y; o += (F + G + L + K) * div.y; o += (G + H + M + L) * div.y; half3 color = o.xyz; #else half3 color = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv).xyz; #endif color = min(ClampMax, color); // 阈值化 half brightness = Max3(color.r, color.g, color.b); half softness = clamp(brightness - Threshold + ThresholdKnee, 0.0, 2.0 * ThresholdKnee); softness = (softness * softness) / (4.0 * ThresholdKnee + 1e-4); half multiplier = max(brightness - Threshold, softness) / max(brightness, 1e-4); color *= multiplier; color = max(color, 0); return EncodeHDR(color); &#125; half4 FragBlurH(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float texelSize = _BlitTexture_TexelSize.x * 2.0; float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); // 9-tap高斯模糊 half3 c0 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - float2(texelSize * 4.0, 0.0))); half3 c1 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - float2(texelSize * 3.0, 0.0))); half3 c2 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - float2(texelSize * 2.0, 0.0))); half3 c3 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - float2(texelSize * 1.0, 0.0))); half3 c4 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv )); half3 c5 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + float2(texelSize * 1.0, 0.0))); half3 c6 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + float2(texelSize * 2.0, 0.0))); half3 c7 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + float2(texelSize * 3.0, 0.0))); half3 c8 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + float2(texelSize * 4.0, 0.0))); half3 color = c0 * 0.01621622 + c1 * 0.05405405 + c2 * 0.12162162 + c3 * 0.19459459 + c4 * 0.22702703 + c5 * 0.19459459 + c6 * 0.12162162 + c7 * 0.05405405 + c8 * 0.01621622; return EncodeHDR(color); &#125; half4 FragBlurV(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float texelSize = _BlitTexture_TexelSize.y; float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); // 双线性插值方式用5个采样点模拟9-tap高斯核 half3 c0 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - float2(0.0, texelSize * 3.23076923))); half3 c1 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv - float2(0.0, texelSize * 1.38461538))); half3 c2 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv )); half3 c3 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + float2(0.0, texelSize * 1.38461538))); half3 c4 = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv + float2(0.0, texelSize * 3.23076923))); half3 color = c0 * 0.07027027 + c1 * 0.31621622 + c2 * 0.22702703 + c3 * 0.31621622 + c4 * 0.07027027; return EncodeHDR(color); &#125; half3 Upsample(float2 uv) &#123; half3 highMip = DecodeHDR(SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv)); #if _BLOOM_HQ &amp;&amp; !defined(SHADER_API_GLES) half3 lowMip = DecodeHDR(SampleTexture2DBicubic(TEXTURE2D_X_ARGS(_SourceTexLowMip, sampler_LinearClamp), uv, _SourceTexLowMip_TexelSize.zwxy, (1.0).xx, unity_StereoEyeIndex)); #else half3 lowMip = DecodeHDR(SAMPLE_TEXTURE2D_X(_SourceTexLowMip, sampler_LinearClamp, uv)); #endif return lerp(highMip, lowMip, Scatter); &#125; half4 FragUpsample(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); half3 color = Upsample(UnityStereoTransformScreenSpaceTex(input.texcoord)); return EncodeHDR(color); &#125; ENDHLSL SubShader &#123; Tags &#123; \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\"&#125; LOD 100 ZTest Always ZWrite Off Cull Off Pass &#123; Name \"Bloom Prefilter\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragPrefilter #pragma multi_compile_local _ _BLOOM_HQ #pragma multi_compile_fragment _ _FOVEATED_RENDERING_NON_UNIFORM_RASTER // Foveated rendering currently not supported in dxc on metal #pragma never_use_dxc metal ENDHLSL &#125; Pass &#123; Name \"Bloom Blur Horizontal\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragBlurH ENDHLSL &#125; Pass &#123; Name \"Bloom Blur Vertical\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragBlurV ENDHLSL &#125; Pass &#123; Name \"Bloom Upsample\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragUpsample #pragma multi_compile_local _ _BLOOM_HQ ENDHLSL &#125; &#125;&#125; 晕影效果(Vignette) 晕影效果（Vignette）是一种在图像边缘逐渐减弱亮度或增加饱和度的效果，通常用于突出图像中心或增加视觉焦点。实现晕影效果的后处理方法通常基于图像处理技术，其实现原理可以概括如下： 基于距离的衰减： 一种常见的实现方法是根据像素与图像中心的距离来计算衰减系数，距离中心越远的像素，衰减系数越大。这样，在后处理过程中，可以根据像素与中心的距离来对像素的亮度或饱和度进行加权，使得边缘逐渐变暗或变饱和。 径向函数： 晕影效果的实现也可以利用径向函数来描述衰减的形式，例如高斯函数或多项式函数。通过选择合适的径向函数参数，可以实现不同形式的晕影效果，例如较平滑的渐变或较陡峭的渐变。 遮罩技术： 另一种实现晕影效果的方法是使用遮罩技术，即在图像上叠加一个透明的黑色或彩色遮罩，使得边缘透明度逐渐增加。通过调整遮罩的形状和透明度，可以实现不同形式的晕影效果。 C#代码 1234567891011121314151617181920// 设置晕影参数void SetupVignette(Material material, XRPass xrPass)&#123; var color = m_Vignette.color.value; var center = m_Vignette.center.value; var aspectRatio = m_Descriptor.width / (float)m_Descriptor.height; var v1 = new Vector4( color.r, color.g, color.b, m_Vignette.rounded.value ? aspectRatio : 1f ); var v2 = new Vector4( center.x, center.y, m_Vignette.intensity.value * 3f, m_Vignette.smoothness.value * 5f ); material.SetVector(ShaderConstants._Vignette_Params1, v1); material.SetVector(ShaderConstants._Vignette_Params2, v2);&#125; 12345678910111213half3 ApplyVignette(half3 input, float2 uv, float2 center, float intensity, float roundness, float smoothness, half3 color)&#123; center = UnityStereoTransformScreenSpaceTex(center); float2 dist = abs(uv - center) * intensity;#if defined(UNITY_SINGLE_PASS_STEREO) dist.x /= unity_StereoScaleOffset[unity_StereoEyeIndex].x;#endif dist.x *= roundness; float vfactor = pow(saturate(1.0 - dot(dist, dist)), smoothness); return input * lerp(color, (1.0).xxx, vfactor);&#125; 胶片颗粒（Film Grain） 胶片颗粒效果（Film Grain）是一种模拟传统胶片摄影中出现的颗粒状噪点的效果，可以增加图像的质感和艺术感。实现胶片颗粒效果的后处理方法通常基于图像处理技术，其实现原理可以概括如下： 随机噪声生成： 胶片颗粒效果的实现通常涉及生成随机的颗粒噪声。可以使用伪随机数生成器来生成服从特定分布（如高斯分布）的随机数序列，然后将这些随机数映射到图像像素上，以模拟胶片颗粒的分布。 混合叠加： 生成的颗粒噪声可以与原始图像进行混合叠加。可以通过调整混合的透明度或混合模式（如叠加、乘法等）来控制颗粒效果的强度和影响范围。 空间滤波： 在一些情况下，可以使用空间滤波技术来模拟胶片颗粒的空间分布特征。例如，可以使用卷积滤波器（如高斯滤波器）来对图像进行模糊处理，然后通过减去模糊图像和原始图像之间的差异来生成颗粒噪声。 颗粒参数调整： 胶片颗粒效果的外观可以通过调整参数来控制，例如颗粒的大小、密度、形状和颜色等。通过调整这些参数，可以实现不同类型和风格的胶片颗粒效果。 C#代码 123456789101112131415161718192021222324public static void ConfigureFilmGrain(PostProcessData data, FilmGrain settings, int cameraPixelWidth, int cameraPixelHeight, Material material)&#123; var texture = settings.texture.value; if (settings.type.value != FilmGrainLookup.Custom) texture = data.textures.filmGrainTex[(int)settings.type.value];#if LWRP_DEBUG_STATIC_POSTFX float offsetX = 0f; float offsetY = 0f;#else Random.InitState(Time.frameCount); float offsetX = Random.value; float offsetY = Random.value;#endif var tilingParams = texture == null ? Vector4.zero : new Vector4(cameraPixelWidth / (float)texture.width, cameraPixelHeight / (float)texture.height, offsetX, offsetY); material.SetTexture(ShaderConstants._Grain_Texture, texture); material.SetVector(ShaderConstants._Grain_Params, new Vector2(settings.intensity.value * 4f, settings.response.value)); material.SetVector(ShaderConstants._Grain_TilingParams, tilingParams);&#125; Shader代码 123456789101112131415161718half3 ApplyGrain(half3 input, float2 uv, TEXTURE2D_PARAM(GrainTexture, GrainSampler), float intensity, float response, float2 scale, float2 offset, float oneOverPaperWhite)&#123; // 颗粒范围为 [0;1]，中性值为 0.5 half grain = SAMPLE_TEXTURE2D(GrainTexture, GrainSampler, uv * scale + offset).w; // 重映射范围 [-1;1] grain = (grain - 0.5) * 2.0; // 基于场景亮度的噪声响应曲线 float lum = Luminance(input); #ifdef HDR_INPUT lum *= oneOverPaperWhite; #endif lum = 1.0 - sqrt(lum); lum = lerp(1.0, lum, response); return input + input * grain * intensity * lum;&#125; Uber \"Uber\"一词通常用来形容一个综合性的或\"全包\"（all-in-one）的着色器或后处理效果，它集成了多种视觉效果和图形处理技术。 这种\"Uber后处理效果\"可能包括，但不限于，色彩校正、HDR（高动态范围）渲染、Bloom、晕影效果(Vignette)、光晕（Lens Flares）、胶片颗粒（Film Grain）等多个组件。 Shader代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298Shader \"Hidden/Universal Render Pipeline/UberPost\"&#123; HLSLINCLUDE #pragma exclude_renderers gles #pragma multi_compile_local_fragment _ _DISTORTION #pragma multi_compile_local_fragment _ _CHROMATIC_ABERRATION #pragma multi_compile_local_fragment _ _BLOOM_LQ _BLOOM_HQ _BLOOM_LQ_DIRT _BLOOM_HQ_DIRT #pragma multi_compile_local_fragment _ _HDR_GRADING _TONEMAP_ACES _TONEMAP_NEUTRAL #pragma multi_compile_local_fragment _ _FILM_GRAIN #pragma multi_compile_local_fragment _ _DITHERING #pragma multi_compile_local_fragment _ _GAMMA_20 _LINEAR_TO_SRGB_CONVERSION #pragma multi_compile_local_fragment _ _USE_FAST_SRGB_LINEAR_CONVERSION #pragma multi_compile_fragment _ _FOVEATED_RENDERING_NON_UNIFORM_RASTER // Foveated rendering currently not supported in dxc on metal #pragma never_use_dxc metal #pragma multi_compile_fragment _ DEBUG_DISPLAY #pragma multi_compile_fragment _ SCREEN_COORD_OVERRIDE #pragma multi_compile_local_fragment _ HDR_INPUT HDR_ENCODING #ifdef HDR_ENCODING #define HDR_INPUT 1 // this should be defined when HDR_ENCODING is defined #endif #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Filtering.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/ScreenCoordOverride.hlsl\"#if defined(HDR_ENCODING) #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/HDROutput.hlsl\"#endif #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/Shaders/PostProcessing/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Debug/DebuggingFullscreen.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/FoveatedRendering.hlsl\" // Hardcoded dependencies to reduce the number of variants #if _BLOOM_LQ || _BLOOM_HQ || _BLOOM_LQ_DIRT || _BLOOM_HQ_DIRT #define BLOOM #if _BLOOM_LQ_DIRT || _BLOOM_HQ_DIRT #define BLOOM_DIRT #endif #endif TEXTURE2D_X(_Bloom_Texture); TEXTURE2D(_LensDirt_Texture); TEXTURE2D(_Grain_Texture); TEXTURE2D(_InternalLut); TEXTURE2D(_UserLut); TEXTURE2D(_BlueNoise_Texture); TEXTURE2D_X(_OverlayUITexture); float4 _Lut_Params; float4 _UserLut_Params; float4 _Bloom_Params; float _Bloom_RGBM; float4 _LensDirt_Params; float _LensDirt_Intensity; float4 _Distortion_Params1; float4 _Distortion_Params2; float _Chroma_Params; half4 _Vignette_Params1; float4 _Vignette_Params2; #ifdef USING_STEREO_MATRICES float4 _Vignette_ParamsXR; #endif float2 _Grain_Params; float4 _Grain_TilingParams; float4 _Bloom_Texture_TexelSize; float4 _Dithering_Params; float4 _HDROutputLuminanceParams; #define DistCenter _Distortion_Params1.xy #define DistAxis _Distortion_Params1.zw #define DistTheta _Distortion_Params2.x #define DistSigma _Distortion_Params2.y #define DistScale _Distortion_Params2.z #define DistIntensity _Distortion_Params2.w #define ChromaAmount _Chroma_Params.x #define BloomIntensity _Bloom_Params.x #define BloomTint _Bloom_Params.yzw #define BloomRGBM _Bloom_RGBM.x #define LensDirtScale _LensDirt_Params.xy #define LensDirtOffset _LensDirt_Params.zw #define LensDirtIntensity _LensDirt_Intensity.x #define VignetteColor _Vignette_Params1.xyz #ifdef USING_STEREO_MATRICES #define VignetteCenterEye0 _Vignette_ParamsXR.xy #define VignetteCenterEye1 _Vignette_ParamsXR.zw #else #define VignetteCenter _Vignette_Params2.xy #endif #define VignetteIntensity _Vignette_Params2.z #define VignetteSmoothness _Vignette_Params2.w #define VignetteRoundness _Vignette_Params1.w #define LutParams _Lut_Params.xyz #define PostExposure _Lut_Params.w #define UserLutParams _UserLut_Params.xyz #define UserLutContribution _UserLut_Params.w #define GrainIntensity _Grain_Params.x #define GrainResponse _Grain_Params.y #define GrainScale _Grain_TilingParams.xy #define GrainOffset _Grain_TilingParams.zw #define DitheringScale _Dithering_Params.xy #define DitheringOffset _Dithering_Params.zw #define MinNits _HDROutputLuminanceParams.x #define MaxNits _HDROutputLuminanceParams.y #define PaperWhite _HDROutputLuminanceParams.z #define OneOverPaperWhite _HDROutputLuminanceParams.w // UV扭曲，部分后处理需要扭曲UV float2 DistortUV(float2 uv) &#123; #if _DISTORTION &#123; uv = (uv - 0.5) * DistScale + 0.5; float2 ruv = DistAxis * (uv - 0.5 - DistCenter); float ru = length(float2(ruv)); UNITY_BRANCH if (DistIntensity &gt; 0.0) &#123; float wu = ru * DistTheta; ru = tan(wu) * (rcp(ru * DistSigma)); uv = uv + ruv * (ru - 1.0); &#125; else &#123; ru = rcp(ru) * DistTheta * atan(ru * DistSigma); uv = uv + ruv * (ru - 1.0); &#125; &#125; #endif return uv; &#125; half4 FragUberPost(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = SCREEN_COORD_APPLY_SCALEBIAS(UnityStereoTransformScreenSpaceTex(input.texcoord)); float2 uvDistorted = DistortUV(uv); half3 color = (0.0).xxx; //色差（Chromatic Aberration），也称为色彩像差，是一种由于镜头无法将不同颜色的光线聚焦在同一点上而产生的视觉现象。 //这种现象在图像的边缘部分尤为明显，表现为彩色的晕边，通常是紫色或绿色的边缘。色差通常出现在便宜的镜头或极宽角镜头的照片中，而高质量的镜头设计会尽量减少这种效果。 //边缘会有类似彩虹的色斑 #if _CHROMATIC_ABERRATION &#123; //高清渲染管线（HDRP）中的色差的超快速版本，使用3个样本和硬编码的光谱LUT。在低端GPU上性能显著提升。 float2 coords = 2.0 * uv - 1.0; float2 end = uv - coords * dot(coords, coords) * ChromaAmount; float2 delta = (end - uv) / 3.0; half r = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, SCREEN_COORD_REMOVE_SCALEBIAS(uvDistorted) ).x; half g = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, SCREEN_COORD_REMOVE_SCALEBIAS(DistortUV(delta + uv) )).y; half b = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, SCREEN_COORD_REMOVE_SCALEBIAS(DistortUV(delta * 2.0 + uv))).z; color = half3(r, g, b); &#125; #else &#123; color = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, SCREEN_COORD_REMOVE_SCALEBIAS(uvDistorted)).xyz; &#125; #endif //Gamma 空间...只需将 Uber 的其余部分以线性方式进行，最后再转换回 sRGB #if UNITY_COLORSPACE_GAMMA &#123; color = GetSRGBToLinear(color); &#125; #endif // 泛光（Bloom） #if defined(BLOOM) &#123; float2 uvBloom = uvDistorted; #if defined(_FOVEATED_RENDERING_NON_UNIFORM_RASTER) uvBloom = RemapFoveatedRenderingDistort(uvBloom); #endif #if _BLOOM_HQ &amp;&amp; !defined(SHADER_API_GLES) half4 bloom = SampleTexture2DBicubic(TEXTURE2D_X_ARGS(_Bloom_Texture, sampler_LinearClamp), SCREEN_COORD_REMOVE_SCALEBIAS(uvBloom), _Bloom_Texture_TexelSize.zwxy, (1.0).xx, unity_StereoEyeIndex); #else half4 bloom = SAMPLE_TEXTURE2D_X(_Bloom_Texture, sampler_LinearClamp, SCREEN_COORD_REMOVE_SCALEBIAS(uvBloom)); #endif #if UNITY_COLORSPACE_GAMMA bloom.xyz *= bloom.xyz; // γ to linear #endif UNITY_BRANCH if (BloomRGBM &gt; 0) &#123; bloom.xyz = DecodeRGBM(bloom); &#125; bloom.xyz *= BloomIntensity; color += bloom.xyz * BloomTint; #if defined(BLOOM_DIRT) &#123; //污垢纹理的 UV 应该是 DistortUV(uv * DirtScale + DirtOffset)，但考虑到我们在污垢纹理上使用了覆盖式比例，差异并不大，因此我们选择在这里保存一些 ALU，以防镜头畸变处于活动状态 。 half3 dirt = SAMPLE_TEXTURE2D(_LensDirt_Texture, sampler_LinearClamp, uvDistorted * LensDirtScale + LensDirtOffset).xyz; dirt *= LensDirtIntensity; color += dirt * bloom.xyz; &#125; #endif &#125; #endif // To save on variants we'll use an uniform branch for vignette. Lower end platforms // don't like these but if we're running Uber it means we're running more expensive // effects anyway. Lower-end devices would limit themselves to on-tile compatible effect // and thus this shouldn't too much of a problem (famous last words). //为了节省变体，我们将使用统一的小插图分支。 低端平台不喜欢这些，但如果我们运行 Uber，这意味着我们无论如何都会运行更昂贵的效果。 低端设备会将自己限制为瓷砖兼容效果，因此这应该不是什么太大的问题（著名的遗言）。 UNITY_BRANCH if (VignetteIntensity &gt; 0) &#123; color = ApplyVignette(color, uvDistorted, VignetteCenter, VignetteIntensity, VignetteRoundness, VignetteSmoothness, VignetteColor); &#125; // 颜色修正 &#123; color = ApplyColorGrading(color, PostExposure, TEXTURE2D_ARGS(_InternalLut, sampler_LinearClamp), LutParams, TEXTURE2D_ARGS(_UserLut, sampler_LinearClamp), UserLutParams, UserLutContribution); &#125; // 胶片颗粒（Film Grain） #if _FILM_GRAIN &#123; color = ApplyGrain(color, uv, TEXTURE2D_ARGS(_Grain_Texture, sampler_LinearRepeat), GrainIntensity, GrainResponse, GrainScale, GrainOffset, OneOverPaperWhite); &#125; #endif // 当 Unity 配置为使用 gamma 颜色编码时，我们忽略转换为 gamma 2.0 的请求，而是回退到 sRGB 编码 #if _GAMMA_20 &amp;&amp; !UNITY_COLORSPACE_GAMMA &#123; color = LinearToGamma20(color); &#125; // Back to sRGB #elif UNITY_COLORSPACE_GAMMA || _LINEAR_TO_SRGB_CONVERSION &#123; color = GetLinearToSRGB(color); &#125; #endif //Dithering（抖动）是一种在数字图像处理中常用的技术，用于在有限的颜色深度显示设备上模拟更广泛的颜色范围。这种技术通过在像素之间故意添加噪声或图案，来模拟中间色调或渐变效果，从而减少颜色带（色阶突变）的视觉影响。 #if _DITHERING &#123; color = ApplyDithering(color, uv, TEXTURE2D_ARGS(_BlueNoise_Texture, sampler_PointRepeat), DitheringScale, DitheringOffset, PaperWhite, OneOverPaperWhite); //假设颜色 &gt; 0 并防止 0 - ditherNoise。 //如果通过渲染到 FP16 纹理反馈到后处理，负色可能会导致问题。 color = max(color, 0); &#125; #endif // HDR编码颜色 #ifdef HDR_ENCODING &#123; float4 uiSample = SAMPLE_TEXTURE2D_X(_OverlayUITexture, sampler_PointClamp, input.texcoord); color.rgb = SceneUIComposition(uiSample, color.rgb, PaperWhite, MaxNits); color.rgb = OETF(color.rgb, MaxNits); &#125; #endif return half4(color, 1.0); &#125; ENDHLSL SubShader &#123; Tags &#123; \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\" &#125; LOD 100 ZTest Always ZWrite Off Cull Off Pass &#123; Name \"UberPost\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragUberPost ENDHLSL &#125; &#125;&#125; FinalPass FinalPass是在后处理Pass处理完后，对其进行最后一次修改，主要包括：HDR输出，升/降分辨率和应用各种抗锯齿算法(FXAA和TAA等) C#代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205// 渲染最终Passvoid RenderFinalPass(CommandBuffer cmd, ref RenderingData renderingData)&#123; ref var cameraData = ref renderingData.cameraData; var material = m_Materials.finalPass; material.shaderKeywords = null; //设置目标纹理大小（计算动态分辨率） PostProcessUtils.SetSourceSize(cmd, cameraData.cameraTargetDescriptor); //设置Film Grain（胶片颗粒） SetupGrain(ref cameraData, material); //Dithering（抖动） SetupDithering(ref cameraData, material); //是否需要转换回SRGB if (RequireSRGBConversionBlitToBackBuffer(ref cameraData)) material.EnableKeyword(ShaderKeywordStrings.LinearToSRGBConversion); //是否输出HDR HDROutputUtils.Operation hdrOperations = HDROutputUtils.Operation.None; bool requireHDROutput = RequireHDROutput(ref cameraData); if (requireHDROutput) &#123; // If there is a final post process pass, it's always the final pass so do color encoding hdrOperations = m_EnableColorEncodingIfNeeded ? HDROutputUtils.Operation.ColorEncoding : HDROutputUtils.Operation.None; // If the color space conversion wasn't applied by the uber pass, do it here if (!cameraData.postProcessEnabled) hdrOperations |= HDROutputUtils.Operation.ColorConversion; SetupHDROutput(cameraData.hdrDisplayInformation, cameraData.hdrDisplayColorGamut, material, hdrOperations); &#125; //配置调式处理器 DebugHandler debugHandler = GetActiveDebugHandler(ref renderingData); bool resolveToDebugScreen = debugHandler != null &amp;&amp; debugHandler.WriteToDebugScreenTexture(ref cameraData); debugHandler?.UpdateShaderGlobalPropertiesForFinalValidationPass(cmd, ref cameraData, m_IsFinalPass &amp;&amp; !resolveToDebugScreen); if (m_UseSwapBuffer) m_Source = cameraData.renderer.GetCameraColorBackBuffer(cmd); RTHandle sourceTex = m_Source; var colorLoadAction = cameraData.isDefaultViewport ? RenderBufferLoadAction.DontCare : RenderBufferLoadAction.Load; bool isFxaaEnabled = (cameraData.antialiasing == AntialiasingMode.FastApproximateAntialiasing); //是否FSR重采样是否开启 bool isFsrEnabled = ((cameraData.imageScalingMode == ImageScalingMode.Upscaling) &amp;&amp; (cameraData.upscalingFilter == ImageUpscalingFilter.FSR)); //重复使用 RCAS 通道作为 TAA 的可选独立后锐化通道。 这避免了 EASU 的成本，并且可用于其他升级选项。如果启用 FSR，则 FSR 设置将覆盖 TAA 设置，并且我们仅执行一次 RCAS。 bool isTaaSharpeningEnabled = (cameraData.IsTemporalAAEnabled() &amp;&amp; cameraData.taaSettings.contrastAdaptiveSharpening &gt; 0.0f) &amp;&amp; !isFsrEnabled; //最终渲染目标需要缩放 if (cameraData.imageScalingMode != ImageScalingMode.None) &#123; // 当在缩放渲染中启用 FXAA 时，我们在单独的 blit 中执行它，因为它不是设计用于在 // 输入和输出分辨率不匹配的情况。 // 当 FSR 处于活动状态时，我们总是需要额外的通道，因为它有非常特殊的颜色编码要求。 // 注意：理想的实现可以将此颜色转换逻辑内联到 UberPost 通道中，但当前的代码结构会使 // 这个过程非常复杂。 具体来说，我们需要保证 uber post 输出始终写入 UNORM 格式渲染 // 目标是为了保留特殊编码的颜色数据的精度。 bool isSetupRequired = (isFxaaEnabled || isFsrEnabled); // 确保从临时渲染目标中删除任何 MSAA 和附加的深度缓冲区 var tempRtDesc = cameraData.cameraTargetDescriptor; tempRtDesc.msaaSamples = 1; tempRtDesc.depthBufferBits = 0; // 选择 UNORM 格式，因为我们已经执行了色调映射。 （值在0-1范围内） // 这可以提高精度，如果我们想在使用 FSR 时避免过度条带，这是必需的。 if (!requireHDROutput) tempRtDesc.graphicsFormat = UniversalRenderPipeline.MakeUnormRenderTextureGraphicsFormat(); m_Materials.scalingSetup.shaderKeywords = null; //是否启用了FXAA抗锯齿或是否启用了FSR重采样 if (isSetupRequired) &#123; //需要HDR输出 if (requireHDROutput) &#123; SetupHDROutput(cameraData.hdrDisplayInformation, cameraData.hdrDisplayColorGamut, m_Materials.scalingSetup, hdrOperations); &#125; //启用了FXAA if (isFxaaEnabled) &#123; m_Materials.scalingSetup.EnableKeyword(ShaderKeywordStrings.Fxaa); &#125; //启用了FRS if (isFsrEnabled) &#123; m_Materials.scalingSetup.EnableKeyword(hdrOperations.HasFlag(HDROutputUtils.Operation.ColorEncoding) ? ShaderKeywordStrings.Gamma20AndHDRInput : ShaderKeywordStrings.Gamma20); &#125; //Blit RenderingUtils.ReAllocateIfNeeded(ref m_ScalingSetupTarget, tempRtDesc, FilterMode.Point, TextureWrapMode.Clamp, name: \"_ScalingSetupTexture\"); Blitter.BlitCameraTexture(cmd, m_Source, m_ScalingSetupTarget, colorLoadAction, RenderBufferStoreAction.Store, m_Materials.scalingSetup, 0); sourceTex = m_ScalingSetupTarget; &#125; //根据缩放模式（放大或缩小） switch (cameraData.imageScalingMode) &#123; case ImageScalingMode.Upscaling: &#123; // 在放大情况下，根据所选的放大过滤器设置材质关键字 // 注意：如果启用了 FSR，无论当前渲染比例如何，我们都会沿着这条路径走下去。 我们这样做是因为 // FSR 在 100% 比例下仍然提供视觉效果。 这也将实现 99% 和 100% 之间的过渡 // 对于 FSR 与动态分辨率缩放一起使用的情况，缩放不太明显。 switch (cameraData.upscalingFilter) &#123; case ImageUpscalingFilter.Point: &#123; //TAA 后锐化是 RCAS 通道，避免用点采样覆盖它。 if(!isTaaSharpeningEnabled) material.EnableKeyword(ShaderKeywordStrings.PointSampling); break; &#125; case ImageUpscalingFilter.Linear: &#123; //Shader的模式重采样算法，不做任何设置 break; &#125; case ImageUpscalingFilter.FSR: &#123; //FSR重采样 m_Materials.easu.shaderKeywords = null; var upscaleRtDesc = tempRtDesc; upscaleRtDesc.width = cameraData.pixelWidth; upscaleRtDesc.height = cameraData.pixelHeight; // EASU RenderingUtils.ReAllocateIfNeeded(ref m_UpscaledTarget, upscaleRtDesc, FilterMode.Point, TextureWrapMode.Clamp, name: \"_UpscaledTexture\"); var fsrInputSize = new Vector2(cameraData.cameraTargetDescriptor.width, cameraData.cameraTargetDescriptor.height); var fsrOutputSize = new Vector2(cameraData.pixelWidth, cameraData.pixelHeight); FSRUtils.SetEasuConstants(cmd, fsrInputSize, fsrInputSize, fsrOutputSize); Blitter.BlitCameraTexture(cmd, sourceTex, m_UpscaledTarget, colorLoadAction, RenderBufferStoreAction.Store, m_Materials.easu, 0); // RCAS // 如果可用，则使用覆盖值，否则使用默认值。 float sharpness = cameraData.fsrOverrideSharpness ? cameraData.fsrSharpness : FSRUtils.kDefaultSharpnessLinear; // 设置 RCAS 通道的参数，除非锐度值表明它不会产生任何影响。 if (cameraData.fsrSharpness &gt; 0.0f) &#123; // RCAS 在最后的 post blit 期间执行，但我们在这里设置参数是为了更好的逻辑分组。 material.EnableKeyword(requireHDROutput ? ShaderKeywordStrings.EasuRcasAndHDRInput : ShaderKeywordStrings.Rcas); FSRUtils.SetRcasConstantsLinear(cmd, sharpness); &#125; // 更新源纹理以进行下一步操作 sourceTex = m_UpscaledTarget; PostProcessUtils.SetSourceSize(cmd, upscaleRtDesc); break; &#125; &#125; break; &#125; case ImageScalingMode.Downscaling: &#123; // 在缩小的情况下，我们不执行任何类型的过滤器覆盖逻辑，因为我们总是想要线性过滤 // 它已经是着色器中的默认选项。 // 缩小尺寸时还禁用 TAA 后锐化通道。 isTaaSharpeningEnabled = false; break; &#125; &#125; &#125; else if (isFxaaEnabled) &#123; // 在未缩放的渲染中，FXAA 可以在 FinalPost 着色器中安全地执行 material.EnableKeyword(ShaderKeywordStrings.Fxaa); &#125; // 重用 RCAS 作为 TAA 的独立锐化过滤器。 // 如果启用了 FSR，那么它会覆盖 TAA 设置，我们会跳过它。 if(isTaaSharpeningEnabled) &#123; material.EnableKeyword(ShaderKeywordStrings.Rcas); FSRUtils.SetRcasConstantsLinear(cmd, cameraData.taaSettings.contrastAdaptiveSharpening); &#125; var cameraTarget = RenderingUtils.GetCameraTargetIdentifier(ref renderingData); if (resolveToDebugScreen) &#123; // Blit 到调试器纹理而不是相机目标 Blitter.BlitCameraTexture(cmd, sourceTex, debugHandler.DebugScreenColorHandle, RenderBufferLoadAction.Load, RenderBufferStoreAction.Store, material, 0); cameraData.renderer.ConfigureCameraTarget(debugHandler.DebugScreenColorHandle, debugHandler.DebugScreenDepthHandle); &#125; else &#123; // Blit到最终屏幕 RTHandleStaticHelpers.SetRTHandleStaticWrapper(cameraTarget); var cameraTargetHandle = RTHandleStaticHelpers.s_RTHandleWrapper; RenderingUtils.FinalBlit(cmd, ref cameraData, sourceTex, cameraTargetHandle, colorLoadAction, RenderBufferStoreAction.Store, material, 0); &#125;&#125; Shader代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173Shader \"Hidden/Universal Render Pipeline/FinalPost\"&#123; HLSLINCLUDE #pragma multi_compile_local_fragment _ _POINT_SAMPLING _RCAS _EASU_RCAS_AND_HDR_INPUT #pragma multi_compile_local_fragment _ _FXAA #pragma multi_compile_local_fragment _ _FILM_GRAIN #pragma multi_compile_local_fragment _ _DITHERING #pragma multi_compile_local_fragment _ _LINEAR_TO_SRGB_CONVERSION #pragma multi_compile_fragment _ DEBUG_DISPLAY #pragma multi_compile_fragment _ SCREEN_COORD_OVERRIDE #pragma multi_compile_local_fragment _ HDR_INPUT HDR_COLORSPACE_CONVERSION HDR_ENCODING HDR_COLORSPACE_CONVERSION_AND_ENCODING #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/Color.hlsl\" #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/ScreenCoordOverride.hlsl\"#if defined(HDR_COLORSPACE_CONVERSION) || defined(HDR_ENCODING) || defined(HDR_COLORSPACE_CONVERSION_AND_ENCODING) #define HDR_INPUT 1 // this should be defined when HDR_COLORSPACE_CONVERSION or HDR_ENCODING are defined #include \"Packages/com.unity.render-pipelines.core/ShaderLibrary/HDROutput.hlsl\"#endif #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/ShaderLibrary/Debug/DebuggingFullscreen.hlsl\" #include \"Packages/com.unity.render-pipelines.universal/Shaders/PostProcessing/Common.hlsl\" TEXTURE2D(_Grain_Texture); TEXTURE2D(_BlueNoise_Texture); TEXTURE2D_X(_OverlayUITexture); float4 _SourceSize; float2 _Grain_Params; float4 _Grain_TilingParams; float4 _Dithering_Params; float4 _HDROutputLuminanceParams; #define GrainIntensity _Grain_Params.x #define GrainResponse _Grain_Params.y #define GrainScale _Grain_TilingParams.xy #define GrainOffset _Grain_TilingParams.zw #define DitheringScale _Dithering_Params.xy #define DitheringOffset _Dithering_Params.zw #define MinNits _HDROutputLuminanceParams.x #define MaxNits _HDROutputLuminanceParams.y #define PaperWhite _HDROutputLuminanceParams.z #define OneOverPaperWhite _HDROutputLuminanceParams.w #if SHADER_TARGET &gt;= 45 #define FSR_INPUT_TEXTURE _BlitTexture #define FSR_INPUT_SAMPLER sampler_LinearClamp // 如果定义了 HDR_INPUT，我们还必须在包含 FSR 公共标头之前定义 FSR_EASU_ONE_OVER_PAPER_WHITE。 // URP 实际上并不使用 FinalPost 着色器中的 EASU，仅使用 RCAS。 #define FSR_EASU_ONE_OVER_PAPER_WHITE OneOverPaperWhite #include \"Packages/com.unity.render-pipelines.core/Runtime/PostProcessing/Shaders/FSRCommon.hlsl\" #endif half4 FragFinalPost(Varyings input) : SV_Target &#123; UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input); float2 uv = UnityStereoTransformScreenSpaceTex(input.texcoord); float2 positionNDC = uv; int2 positionSS = uv * _SourceSize.xy; #if _POINT_SAMPLING half3 color = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_PointClamp, uv).xyz; #elif (_RCAS || _EASU_RCAS_AND_HDR_INPUT) &amp;&amp; SHADER_TARGET &gt;= 45 half3 color = ApplyRCAS(positionSS); // 当Unity配置为使用gamma颜色编码时，我们必须在执行RCAS后从线性转换回来。 //（此着色器变体的输入颜色数据始终是线性编码的，因为 RCAS 需要它） #if _EASU_RCAS_AND_HDR_INPUT // Revert operation from ScalingSetup.shader color.rgb = FastTonemapInvert(color.rgb) * PaperWhite; #endif #if UNITY_COLORSPACE_GAMMA color = GetLinearToSRGB(color); #endif #else half3 color = SAMPLE_TEXTURE2D_X(_BlitTexture, sampler_LinearClamp, uv).xyz; #endif #if _FXAA &#123; color = ApplyFXAA(color, positionNDC, positionSS, _SourceSize, _BlitTexture, PaperWhite, OneOverPaperWhite); &#125; #endif #if _FILM_GRAIN &#123; color = ApplyGrain(color, SCREEN_COORD_APPLY_SCALEBIAS(positionNDC), TEXTURE2D_ARGS(_Grain_Texture, sampler_LinearRepeat), GrainIntensity, GrainResponse, GrainScale, GrainOffset, OneOverPaperWhite); &#125; #endif #if _LINEAR_TO_SRGB_CONVERSION &#123; color = LinearToSRGB(color); &#125; #endif #if _DITHERING &#123; color = ApplyDithering(color, SCREEN_COORD_APPLY_SCALEBIAS(positionNDC), TEXTURE2D_ARGS(_BlueNoise_Texture, sampler_PointRepeat), DitheringScale, DitheringOffset, PaperWhite, OneOverPaperWhite); &#125; #endif #ifdef HDR_COLORSPACE_CONVERSION &#123; color.rgb = RotateRec709ToOutputSpace(color.rgb) * PaperWhite; &#125; #endif #ifdef HDR_ENCODING &#123; float4 uiSample = SAMPLE_TEXTURE2D_X(_OverlayUITexture, sampler_PointClamp, input.texcoord); color.rgb = SceneUIComposition(uiSample, color.rgb, PaperWhite, MaxNits); color.rgb = OETF(color.rgb, MaxNits); &#125; #endif half4 finalColor = half4(color, 1); #if defined(DEBUG_DISPLAY) half4 debugColor = 0; if(CanDebugOverrideOutputColor(finalColor, uv, debugColor)) &#123; return debugColor; &#125; #endif return finalColor; &#125; ENDHLSL /// Standard FinalPost shader variant with support for FSR /// Note: FSR requires shader target 4.5 because it relies on texture gather instructions SubShader &#123; Tags &#123; \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\"&#125; LOD 100 ZTest Always ZWrite Off Cull Off Pass &#123; Name \"FinalPost\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragFinalPost #pragma target 4.5 ENDHLSL &#125; &#125; /// Fallback version of FinalPost shader which lacks support for FSR SubShader &#123; Tags &#123; \"RenderType\" = \"Opaque\" \"RenderPipeline\" = \"UniversalPipeline\"&#125; LOD 100 ZTest Always ZWrite Off Cull Off Pass &#123; Name \"FinalPost\" HLSLPROGRAM #pragma vertex Vert #pragma fragment FragFinalPost ENDHLSL &#125; &#125;&#125; 总结 Unity的后处理系统主要包含三个部分： Volume系统，主要负责后处理数据的编辑，包括不同Volume对象中的相交部分的插值计算。 URP的后处理Pass, 主要有三个Pass：生成LUT的Pass, 后处理Pass，FinalPass。Pass主要是计算和设置Shader中要用到的参数以及调用Blit 各种后处理Shader，处理图像。 参考 你真的了解眼里所见的色彩吗？(一文总结RGB/HSV/Lab) 数字图像处理之6大颜色空间 关于后处理效果的一些总结 HSL和HSV色彩空间 【数字图像处理】8.6:彩色图像-色彩空间 HSI(HSL)、HSV(HSB) Image enhancement algorithm based on hue, saturation, lightness (HSL) space and self-adaptation inverse hyperbolic tangent function Post-processing and full-screen effects Post Process Effects 后期误区——白平衡，深藏不露的调色高手 LMS color space CIE 1931 XYZ色彩空间 CIE1931色彩空间介绍 Standard illuminant","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"后处理","slug":"后处理","permalink":"http://yoursite.com/tags/%E5%90%8E%E5%A4%84%E7%90%86/"}]},{"title":"Photoshop图层混合","slug":"Unity/Graphics/Photoshop图层混合","date":"2023-04-02T08:06:02.000Z","updated":"2025-04-26T11:06:24.129Z","comments":true,"path":"2023/04/02/Unity/Graphics/Photoshop图层混合/","link":"","permalink":"http://yoursite.com/2023/04/02/Unity/Graphics/Photoshop%E5%9B%BE%E5%B1%82%E6%B7%B7%E5%90%88/","excerpt":"本文的目的是通过Shader代码实现Photoshop的图层混合模式，图层混合模式是将上图层中的颜色与下层图层中的颜色，通过一个公式计算出最总的颜色。 颜色混合公式 具体的公式如下： 混合模式 公式 说明 透明度 alpha * foregroundCol + bgCol * (1.0 - alpha) 实现Alpha混合 变暗 min(bgCol, foregroundCol) 上下图层中，取最小的颜色值 正片叠底 bgCol * foregroundCol 混合后颜色整体压暗，最常用的混合模式 颜色加深 1 - ((1-bgCol) / foregroundCol) - 线性加深 bgCol + foregroundCol - 1 - 变亮 max(bgCol, foregroundCol) 上下图层中，取最大的颜色值 滤色 1 - (1-bgCol) * (1-foregroundCol) 和正片叠底真好向反，两图层取反后再乘，再取反，也是比较常用的混合模式 颜色减淡 bgCol / (1-foregroundCol) - 线性减淡 bgCol + foregroundCol - 叠加 当bgCol&lt;0.5时, 2(bgColforegroundCol); 当bgCol &gt;= 0.5时, 1-2(1-bgCol)(1-foregroundCol) 当bgCol小于0.5时，使用2倍的正片叠底；当bgColr大于等于0.5时，使用2倍的滤色，使用2倍是为了当bgCol=0.5时不断层. 柔光 当foregroundCol&lt;0.5时, 2bgColforegroundCol+pow(bgCol,2)(1-2foregroundCol); 当foregroundCol &gt;= 0.5时, (2bgCol(1-foregroundCol)+sqrt(bgCol)(2foregroundCol - 1)) - 强光 当foregroundCol&lt;0.5时, bgCol * (2foregroundCol); 当foregroundCol &gt;= 0.5时, 1-((1-bgCol) (1-2*(foregroundCol - 0.5))) - 亮光 当foregroundCol&lt;0.5时, 1-(1-bgCol)/(2foregroundCol); 当foregroundCol &gt;= 0.5时, bgCol/(1-2(foregroundCol-0.5)) - 线性光 当foregroundCol&lt;0.5时, bgCol+2foregroundCol-1; 当foregroundCol &gt;= 0.5时, bgCol+2(foregroundCol-0.5) - 点光 当foregroundCol&lt;0.5时, min(bgCol, 2foregroundCol); 当foregroundCol &gt;= 0.5时, max(bgCol, 2(foregroundCol-0.5)) - 差值 abs(bgCol-foregroundCol) - 排除 0.5-2(bgCol-0.5)(foregroundCol-0.5) - 减去 bgCol-foregroundCol -","text":"本文的目的是通过Shader代码实现Photoshop的图层混合模式，图层混合模式是将上图层中的颜色与下层图层中的颜色，通过一个公式计算出最总的颜色。 颜色混合公式 具体的公式如下： 混合模式 公式 说明 透明度 alpha * foregroundCol + bgCol * (1.0 - alpha) 实现Alpha混合 变暗 min(bgCol, foregroundCol) 上下图层中，取最小的颜色值 正片叠底 bgCol * foregroundCol 混合后颜色整体压暗，最常用的混合模式 颜色加深 1 - ((1-bgCol) / foregroundCol) - 线性加深 bgCol + foregroundCol - 1 - 变亮 max(bgCol, foregroundCol) 上下图层中，取最大的颜色值 滤色 1 - (1-bgCol) * (1-foregroundCol) 和正片叠底真好向反，两图层取反后再乘，再取反，也是比较常用的混合模式 颜色减淡 bgCol / (1-foregroundCol) - 线性减淡 bgCol + foregroundCol - 叠加 当bgCol&lt;0.5时, 2(bgColforegroundCol); 当bgCol &gt;= 0.5时, 1-2(1-bgCol)(1-foregroundCol) 当bgCol小于0.5时，使用2倍的正片叠底；当bgColr大于等于0.5时，使用2倍的滤色，使用2倍是为了当bgCol=0.5时不断层. 柔光 当foregroundCol&lt;0.5时, 2bgColforegroundCol+pow(bgCol,2)(1-2foregroundCol); 当foregroundCol &gt;= 0.5时, (2bgCol(1-foregroundCol)+sqrt(bgCol)(2foregroundCol - 1)) - 强光 当foregroundCol&lt;0.5时, bgCol * (2foregroundCol); 当foregroundCol &gt;= 0.5时, 1-((1-bgCol) (1-2*(foregroundCol - 0.5))) - 亮光 当foregroundCol&lt;0.5时, 1-(1-bgCol)/(2foregroundCol); 当foregroundCol &gt;= 0.5时, bgCol/(1-2(foregroundCol-0.5)) - 线性光 当foregroundCol&lt;0.5时, bgCol+2foregroundCol-1; 当foregroundCol &gt;= 0.5时, bgCol+2(foregroundCol-0.5) - 点光 当foregroundCol&lt;0.5时, min(bgCol, 2foregroundCol); 当foregroundCol &gt;= 0.5时, max(bgCol, 2(foregroundCol-0.5)) - 差值 abs(bgCol-foregroundCol) - 排除 0.5-2(bgCol-0.5)(foregroundCol-0.5) - 减去 bgCol-foregroundCol - Shader代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135&#x2F;&#x2F; https:&#x2F;&#x2F;www.deepskycolors.com&#x2F;archive&#x2F;2010&#x2F;04&#x2F;21&#x2F;formulas-for-Photoshop-blending-modes.html&#x2F;&#x2F; https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Blend_modes#Overlay&#x2F;&#x2F; 注意：渲染管线必须是gamma空间&#x2F;&#x2F; 注意：渲染管线必须是gamma空间&#x2F;&#x2F; 注意：渲染管线必须是gamma空间Shader &quot;PhotoShopLayerBlend&#x2F;LayerBlend&quot;&#123; Properties &#123; _FrontTex (&quot;Top&quot;, 2D) &#x3D; &quot;white&quot; &#123;&#125; _BackgroudTex (&quot;Bottom&quot;, 2D) &#x3D; &quot;white&quot; &#123;&#125; _Alpha (&quot;Alpha&quot;, Range(0, 1)) &#x3D; 1 [KeywordEnum(OPACITY, DARKEN,MULTIPLY,COLOR_BURN,LINEAR_BURN, LIGHTEN,SCREEN,COLOR_DODGE,LINEAR_DODGE, OVERLAY,SOFT_LIGHT,HARD_LIGHT,VIVID_LIGHT,LINEAR_LIGHT,PIN_LIGHT, DIFFERENCE,EXCLUSION, SUB)] _BLEND (&quot;Blend&quot;, Float) &#x3D; 0 &#125; SubShader &#123; Tags &#123; &quot;RenderType&quot;&#x3D;&quot;Opaque&quot; &quot;RenderPipeline&quot;&#x3D;&quot;UniversalPipeline&quot;&#125; LOD 100 Pass &#123; HLSLPROGRAM #include &quot;Packages&#x2F;com.unity.render-pipelines.universal@12.1.7&#x2F;ShaderLibrary&#x2F;Core.hlsl&quot; #include &quot;Packages&#x2F;com.unity.render-pipelines.core@12.1.7&#x2F;ShaderLibrary&#x2F;Color.hlsl&quot; #pragma vertex vert #pragma fragment frag #pragma multi_compile _ _BLEND_OPACITY _BLEND_DARKEN _BLEND_MULTIPLY _BLEND_COLOR_BURN _BLEND_LINEAR_BURN _BLEND_LIGHTEN _BLEND_SCREEN _BLEND_COLOR_DODGE _BLEND_LINEAR_DODGE _BLEND_OVERLAY _BLEND_SOFT_LIGHT _BLEND_HARD_LIGHT _BLEND_VIVID_LIGHT _BLEND_LINEAR_LIGHT _BLEND_PIN_LIGHT _BLEND_DIFFERENCE _BLEND_EXCLUSION _BLEND_SUB sampler2D _BackgroudTex; float4 _BackgroudTex_ST; sampler2D _FrontTex; float4 _FrontTex_ST; float _Alpha; struct Attributes &#123; float4 positionOS : POSITION; float2 texcoord : TEXCOORD0; &#125;; struct Varyings &#123; float4 positionCS : SV_POSITION; float4 uv : TEXCOORD0; &#125;; Varyings vert(Attributes input) &#123; Varyings o; o.positionCS &#x3D; TransformObjectToHClip(input.positionOS.xyz); o.uv.xy &#x3D; TRANSFORM_TEX(input.texcoord, _BackgroudTex); o.uv.zw &#x3D; TRANSFORM_TEX(input.texcoord, _FrontTex); return o; &#125; half4 frag(Varyings input) : SV_TARGET &#123; float3 bgCol &#x3D; tex2D(_BackgroudTex, input.uv.xy).rgb; float3 foregroundCol &#x3D; tex2D(_FrontTex, input.uv.zw).rgb; float3 rstCol &#x3D; float3(0, 0, 0); &#x2F;&#x2F; 透明度 #if _BLEND_OPACITY float alpha &#x3D; (_Alpha); rstCol &#x3D; alpha * foregroundCol + bgCol * (1.0 - alpha); &#x2F;&#x2F; 变暗组 #elif _BLEND_DARKEN &#x2F;&#x2F;变暗 rstCol &#x3D; min(bgCol, foregroundCol); #elif _BLEND_MULTIPLY &#x2F;&#x2F;正片叠底 rstCol &#x3D; bgCol * foregroundCol; #elif _BLEND_COLOR_BURN &#x2F;&#x2F;颜色加深 rstCol &#x3D; 1 - ((1-bgCol) &#x2F; foregroundCol); #elif _BLEND_LINEAR_BURN &#x2F;&#x2F;线性加深 rstCol &#x3D; bgCol + foregroundCol - 1; &#x2F;&#x2F; 变亮组 #elif _BLEND_LIGHTEN &#x2F;&#x2F;变亮 rstCol &#x3D; max(bgCol, foregroundCol); #elif _BLEND_SCREEN &#x2F;&#x2F;滤色 rstCol &#x3D; 1 - (1-bgCol) * (1-foregroundCol); #elif _BLEND_COLOR_DODGE &#x2F;&#x2F;颜色减淡 rstCol &#x3D; bgCol &#x2F; (1-foregroundCol); #elif _BLEND_LINEAR_DODGE &#x2F;&#x2F;线性减淡 rstCol &#x3D; bgCol + foregroundCol; &#x2F;&#x2F; 叠加 #elif _BLEND_OVERLAY float3 s &#x3D; step(bgCol, float3(0.5, 0.5, 0.5)); rstCol &#x3D; s * (2*bgCol*foregroundCol) + (1-s) * (1-2*(1-bgCol)*(1-foregroundCol)); &#x2F;&#x2F; 柔光 #elif _BLEND_SOFT_LIGHT float3 s &#x3D; step(foregroundCol, float3(0.5, 0.5, 0.5)); &#x2F;&#x2F; rstCol &#x3D; s * (bgCol*(foregroundCol+0.5)) + (1-s) * (1-(1-bgCol)*(1-(foregroundCol-0.5))); rstCol &#x3D; s * (2*bgCol*foregroundCol+bgCol*bgCol*(1-2*foregroundCol)) + (1-s) * ((2*bgCol*(1-foregroundCol)+sqrt(bgCol)*(2*foregroundCol - 1))); &#x2F;&#x2F; 强光 #elif _BLEND_HARD_LIGHT float3 s &#x3D; step(foregroundCol, float3(0.5, 0.5, 0.5)); &#x2F;&#x2F; rstCol &#x3D; s * (2*bgCol*foregroundCol) + (1-s) * (1-2*(1-bgCol)*(1-foregroundCol)); rstCol &#x3D; s * (bgCol * (2*foregroundCol)) + (1-s) * (1-((1-bgCol) * (1-2*(foregroundCol - 0.5)))); &#x2F;&#x2F; 亮光 #elif _BLEND_VIVID_LIGHT float3 s &#x3D; step(foregroundCol, float3(0.5, 0.5, 0.5)); rstCol &#x3D; s * (1-(1-bgCol)&#x2F;(2*foregroundCol)) + (1-s) * (bgCol&#x2F;(1-2*(foregroundCol-0.5))); &#x2F;&#x2F; 线性光 #elif _BLEND_LINEAR_LIGHT float3 s &#x3D; step(foregroundCol, float3(0.5, 0.5, 0.5)); rstCol &#x3D; s * (bgCol+2*foregroundCol-1) + (1-s) * (bgCol+2*(foregroundCol-0.5)); &#x2F;&#x2F; 点光 #elif _BLEND_PIN_LIGHT float3 s &#x3D; step(foregroundCol, float3(0.5, 0.5, 0.5)); rstCol &#x3D; s * (min(bgCol, 2*foregroundCol)) + (1-s) * (max(bgCol, 2*(foregroundCol-0.5))); &#x2F;&#x2F; 差值 #elif _BLEND_DIFFERENCE rstCol &#x3D; abs(bgCol-foregroundCol); &#x2F;&#x2F; 排除 #elif _BLEND_EXCLUSION rstCol &#x3D; 0.5-2*(bgCol-0.5)*(foregroundCol-0.5); &#x2F;&#x2F; 减去 #elif _BLEND_SUB rstCol &#x3D; bgCol-foregroundCol; #else rstCol &#x3D; float3(1, 1, 1); #endif return half4(rstCol.rgb, 1); &#125; ENDHLSL &#125; &#125;&#125; 注意:要想和Photoshop的效果一样,必须使用gamma空间, Photoshop是在gamma空间下计算的颜色值 参考 https://en.wikipedia.org/wiki/Blend_modes https://www.deepskycolors.com/archive/2010/04/21/formulas-for-Photoshop-blending-modes.html","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"Rendering","slug":"Rendering","permalink":"http://yoursite.com/tags/Rendering/"}]},{"title":"Static Batching, Dynamic Batching, GUP Instancing和SRP Batcher","slug":"Unity/Graphics/Batching","date":"2023-02-14T00:45:18.000Z","updated":"2025-04-26T11:06:24.129Z","comments":true,"path":"2023/02/14/Unity/Graphics/Batching/","link":"","permalink":"http://yoursite.com/2023/02/14/Unity/Graphics/Batching/","excerpt":"要在屏幕上绘制几何体时，Unity会调用底层的图形API的Draw命令进行绘制。一个Draw命令高数图形API绘制什么以及如何绘制。每个Draw命令都包含图形API所需的所有信息，其中包括texture,shader和buffers数据。绘图调用可能是资源密集型的，但绘图调用的准备工作通常比绘图调用本身更耗费资源。 准备绘制调用时，CPU去构建资源并通过图形API改变GPU的内部设置。这些设置统称为渲染状态。改变这些渲染状态通常时资源密集型的操作，比如切换不同的材质时。 因为渲染状态的改变是资源密集型的，所有减少渲染状态改变的次数是主要的优化方法，这有两种方法能到达此目的： 减少总的绘制调用 有效组织绘制调用，以减少渲染状态的切换。 优化绘制调用和渲染状态改变数量，主要减少每帧的时间，它也能： 减少应用程序的电池的消耗。 提高应用程序未来开发的可维护性。当你更早的优化绘制调用和渲染状态改变，那么它将长期维持在一个相对优化的级别。 Unity提供了如下几种优化绘制调用和渲染状态的方法，一些方法只适用于特定的场景。 Static Batching : 静态合并Mesh来减少绘制调用和渲染状态的改变。需要将对象标记为static。Unity将组合数据发送到GPU，但单独渲染组合中的每个网格。Unity仍然可以单独剔除网格，但每次绘制调用占用的资源较少，因为数据状态永远不会改变。（减少DrawCall） Dynamic Batching : 在CPU动态上转换网格顶点，将相同配置的顶点分组，并在一次绘制调用中渲染它们。 比如顶点存储相同数量和类型的属性，则它们共享相同的配置。例如，位置和法线。 （减少DrawCall） Manually combining meshes : 通过调用Mesh.CombineMeshes函数将多个Mesh合并为一个。 （减少DrawCall） GPU Instancing : 渲染相同的mesh多次。GPU 实例化对于绘制在场景中多次出现的几何图形非常有用，例如树木或灌木丛。（减少DrawCall） SRP Batcher : 在SRP项目中，可以使用SRP Batcher减少相同着色器变体的材质准备和绘制调用所需的CPU时间。 （不减少DrawCall，减少状态改变次数） 您可以在同一场景中使用多个绘制调用优化方法，但请注意，Unity会按特定顺序对绘制调用优化方法进行优先排序。如果您将一个游戏对象标记为使用多种绘制调用优化方法，Unity将使用优先级最高的方法。唯一的例外是SRP Batcher。当您使用SRP Batcher时，Unity还支持对与SRP Batcher兼容的游戏对象进行静态批处理。 Unity 按以下顺序对绘制调用优化进行优先排序： SRP Batcher and Static batching GPU Instancing Dynamic Batching 如果您将GameObject标记为静态批处理并且Unity成功对其进行了批处理，Unity会禁用该GameObject的GPU实例化，即使渲染器使用实例化着色器也是如此。发生这种情况时，Inspector 窗口显示一条警告消息，建议您禁用静态批处理。 同样，如果Unity可以对网格使用GPU实例化，Unity会禁用该网格的动态批处理。","text":"要在屏幕上绘制几何体时，Unity会调用底层的图形API的Draw命令进行绘制。一个Draw命令高数图形API绘制什么以及如何绘制。每个Draw命令都包含图形API所需的所有信息，其中包括texture,shader和buffers数据。绘图调用可能是资源密集型的，但绘图调用的准备工作通常比绘图调用本身更耗费资源。 准备绘制调用时，CPU去构建资源并通过图形API改变GPU的内部设置。这些设置统称为渲染状态。改变这些渲染状态通常时资源密集型的操作，比如切换不同的材质时。 因为渲染状态的改变是资源密集型的，所有减少渲染状态改变的次数是主要的优化方法，这有两种方法能到达此目的： 减少总的绘制调用 有效组织绘制调用，以减少渲染状态的切换。 优化绘制调用和渲染状态改变数量，主要减少每帧的时间，它也能： 减少应用程序的电池的消耗。 提高应用程序未来开发的可维护性。当你更早的优化绘制调用和渲染状态改变，那么它将长期维持在一个相对优化的级别。 Unity提供了如下几种优化绘制调用和渲染状态的方法，一些方法只适用于特定的场景。 Static Batching : 静态合并Mesh来减少绘制调用和渲染状态的改变。需要将对象标记为static。Unity将组合数据发送到GPU，但单独渲染组合中的每个网格。Unity仍然可以单独剔除网格，但每次绘制调用占用的资源较少，因为数据状态永远不会改变。（减少DrawCall） Dynamic Batching : 在CPU动态上转换网格顶点，将相同配置的顶点分组，并在一次绘制调用中渲染它们。 比如顶点存储相同数量和类型的属性，则它们共享相同的配置。例如，位置和法线。 （减少DrawCall） Manually combining meshes : 通过调用Mesh.CombineMeshes函数将多个Mesh合并为一个。 （减少DrawCall） GPU Instancing : 渲染相同的mesh多次。GPU 实例化对于绘制在场景中多次出现的几何图形非常有用，例如树木或灌木丛。（减少DrawCall） SRP Batcher : 在SRP项目中，可以使用SRP Batcher减少相同着色器变体的材质准备和绘制调用所需的CPU时间。 （不减少DrawCall，减少状态改变次数） 您可以在同一场景中使用多个绘制调用优化方法，但请注意，Unity会按特定顺序对绘制调用优化方法进行优先排序。如果您将一个游戏对象标记为使用多种绘制调用优化方法，Unity将使用优先级最高的方法。唯一的例外是SRP Batcher。当您使用SRP Batcher时，Unity还支持对与SRP Batcher兼容的游戏对象进行静态批处理。 Unity 按以下顺序对绘制调用优化进行优先排序： SRP Batcher and Static batching GPU Instancing Dynamic Batching 如果您将GameObject标记为静态批处理并且Unity成功对其进行了批处理，Unity会禁用该GameObject的GPU实例化，即使渲染器使用实例化着色器也是如此。发生这种情况时，Inspector 窗口显示一条警告消息，建议您禁用静态批处理。 同样，如果Unity可以对网格使用GPU实例化，Unity会禁用该网格的动态批处理。 合并Mesh减少DrawCall 将多个Mesh合并成一个提交给GPU,以减少DrawCall的调用，Unity提供了三种方式：Static Batching（静态批处理），Dynamic Batching（动态批处理）和Manually combining meshes(手动合并Mesh)API。与手动合并Mesh相比，Unity的内置绘图调用批处理有几个优点； 最值得注意的是，Unity仍然可以单独剔除网格。 但是，它也有一些缺点； 静态批处理会产生内存和存储开销，而动态批处理会产生一些CPU开销。 在内置渲染管线中，您可以使用MaterialPropertyBlock更改材质属性，不会中断绘制调用批处理。CPU仍需要进行一些渲染状态更改，但使用MaterialPropertyBlock比使用多种材质更快。如果您的项目使用SRP，请不要使用，MaterialPropertyBlock因为它们删除了材质的SRP Batcher兼容性。 透明着色器通常需要Unity以从后到前的顺序渲染网格。为了批量处理透明网格，Unity首先将它们从后向前排序，然后尝试对它们进行批量处理。由于Unity必须从后到前渲染网格，因此它通常无法批量处理与不透明网格一样多的透明网格。 Unity无法将动态批处理应用于包含镜像的游戏对象. 例如，如果一个GameObject的Scale为1而另一个GameObject的Scale为–1，Unity无法将它们批处理在一起。 如果您不能使用绘图调用批处理，手动组合彼此靠近的网格可能是一个不错的选择。 警告：当您从C#脚本访问共享材质属性时，请确保使用Renderer.sharedMaterial而不是Renderer.material。Renderer.material创建材质的副本并将副本分配回渲染器。这会阻止Unity为该渲染器批处理绘制调用。 Static Batching 静态批处理是将不移动对象的Mesh合并以达到减少DrawCall的目的，它将合并的Mesh转换为世界空间，并为它们构建一个共享的顶点和索引缓冲区。 然后，对于可见网格，Unity执行一系列简单的绘制调用，每个调用之间几乎没有状态变化。 静态批处理不会减少绘制调用的次数（实测是能减少绘制调用的，这取决于摄像机观察到的对象顶点，在共享的顶点缓冲区中是否连续），而是减少它们之间渲染状态更改的次数。 不连续时的DrawCall情况 不连续时的绘制情况 从上图中，可以看出在顶点不连续的情况下是通过两个DrawCall来绘制的，第一个从Index 108开是绘制了一个Cube，第二个从0开始绘制了一个Cube。 连续时的DrawCall情况 连续时的DrawCall情况 从上图中，可以看出在顶点连续的情况下只需要一个DrawCall就可以把所有的Cube绘制完成。 如何使用静态批处理 Unity可以在构建时和运行时执行静态批处理。 作为一般规则，如果游戏对象存在于场景中在构建应用程序之前，使用编辑器在构建时批处理游戏对象。 如果您在运行时创建游戏对象及其网格，则需要使用用运行时API。使用运行时 API时，您可以更改静态批处理根的对象的Transform属性。 这意味着您可以移动、旋转或缩放构成静态批次的整个网格组合。 您不能更改单个网格的Transform属性。 要对一组游戏对象使用静态批处理，游戏对象必须符合静态批处理的条件。 除了常见使用信息中描述的标准外，请确保： 游戏对象处于活动状态。 GameObject 有一个 Mesh Filter组件，并且该组件已启用。 Mesh Filter 组件引用了 Mesh。 网格已启用读/写。 网格的顶点数大于 0。 该网格尚未与另一个网格组合。 游戏对象有一个MeshRender组件，并且该组件已启用。 MeshRenderer组件不使用带有DisableBatching标记设置为true的着色器材质 要一起批处理的网格必须使用相同的顶点属性。 例如，可以将使用顶点位置、顶点法线和一个UV的网格相互批处理，但不能与使用顶点位置、顶点法线、UV0、UV1和顶点切线的网格进行批处理。 在构建时使用静态批处理，需要在Edit &gt; Project Settings &gt; Player-&gt;Other Settings中启用Static Batching, 并在场景中，将需要批处理的对象勾上Batching Static。 在运行时，可以使用StaticBatchingUtility.Combine函数进行静态批处理，跟直接使用建模工具合并不同，静态批处理的对象能够执行单独的相机剔除。 注意： - 使用静态批处理需要额外的CPU内存来存储组合的几何图形。 如果多个游戏对象使用相同的网格，Unity会为每个游戏对象创建一个网格副本，并将每个副本插入到组合网格中。这意味着相同的几何体多次出现在组合网格中。无论您是使用编辑器还是运行时API为静态批处理准备游戏对象，Unity都会这样做。如果您想保持较小的内存占用，您可能不得不牺牲渲染性能并避免对某些游戏对象进行静态批处理。 例如，在茂密的森林环境中将树标记为静态会对内存产生严重影响。 - 静态批次可以包含的顶点数量是有限制的。每个静态批次最多可包含64000个顶点。 如果有更多，Unity会创建另一个批次。 Dynamic Batching 动态批处理是一种绘制调用批处理方法，可对移动的GameObjects进行批处理减少绘制调用。Unity在运行时动态生成的网格和几何体之间的工作方式不同，例如粒子系统。 动态批处理网格和动态几何之体间的内部差异如下： 动态批处理网格 网格的动态批处理通过将所有顶点转换为世界空间来工作。在CPU上，而不是在GPU上。这意味着动态批处理只是一种优化，前提是转换工作比绘制调用占用更少的资源。绘制调用的资源需求取决于许多因素，主要是图形API。 例如，在控制台或Apple Metal等现代API上，绘制调用开销通常要低得多，而且动态批处理通常不会产生性能提升。要确定在您的应用程序中使用动态批处理是否有益，需要分析使用和不使用动态批处理的应用程序。Unity可以对阴影投射者使用动态批处理，即使它们的材质不同，只要Unity需要的阴影传递的材质值相同即可。 例如，多个板条箱可以使用具有不同纹理的材质。 尽管材质资产不同，但这些差异与阴影投射Pass无关，Unity可以在阴影渲染步骤为GameObjects批处理阴影。 动态批处理的一些限制： Unity无法将动态批处理应用于包含超过900个顶点属性和225个顶点的网格。 这是因为网格的动态批处理具有每个顶点的开销。例如，如果您的着色器使用顶点位置、顶点法线和单个UV，那么Unity最多可以批处理225个顶点。但是，如果您的着色器使用顶点位置、顶点法线、UV0、UV1 和顶点切线，那么Unity只能批处理180个顶点。(说明：在Unity2021.3.6f中限制实际要大很多，内建管线：三个属性的情况4000多个顶点为一个批次，URP中三个属性的情况7000多个顶点为一个批次) 如果GameObjects使用不同的材质实例，Unity无法将它们批处理在一起，即使它们本质上相同。唯一的例外是阴影投射渲染。 具有光照贴图的游戏对象具有额外的渲染器参数。这意味着，如果您想批量处理光照贴图游戏对象，它们必须指向相同的光照贴图位置。 Unity无法将动态批处理完全应用于使用多Pass着色器的游戏对象。 动态批处理动态产生的几何体 以下渲染器动态生成几何图形，例如粒子和线条，您可以使用动态批处理对其进行优化： - Built-in Particle Systems - Line Renderers - Trail Renderers 动态生成的几何图形的动态批处理与网格的工作方式不同： 1. 对于每个渲染器，Unity将所有可动态批处理的内容构建到一个大型顶点缓冲区中。 2. 渲染器设置批次的材质状态。 3. 然后Unity将顶点缓冲区绑定到GPU。 4. 对于批处理中的每个渲染器，Unity更新顶点缓冲区中的偏移量并提交新的绘制调用。 Manually combining meshes(手动合并Mesh) 使用建模工具或其他的工具在外部进行Mesh的合并，但就不能像静态批处理一样具有单独相机剔除了，如果Mesh有一个角在屏幕内都会将全部的Mesh丢给渲染器渲染。 GPU Instancing GPU实例化是一种绘制调用优化方法，可在一次绘制调用中使用相同的材质渲染网格的多个副本。网格的每个副本都称为一个实例。这对于绘制在场景中多次出现的事物很有用，例如，树木或灌木。GPU实例化在相同的绘制调用中渲染相同的网格。为了增加变化并减少重复的出现，每个实例都可以具有不同的属性，例如颜色或缩放。 渲染多个实例的绘制调用在Frame Debugger中显示为Draw Mesh (instanced)。GPU Intancing与SRP Batcher不兼容。 SRP Batcher优先于GPU Intancing。 如果是游戏对象与SRP Batcher兼容，则优先使用SRP批处理，而不是GPUIntancing。 如果您的项目使用SRP批处理，并且您想将GPU Intancing用于GameObject，则可以执行以下操作之一： 使用Graphics.DrawMeshinstance。 该API绕过了GameObject的使用，并使用指定的参数直接在屏幕上绘制网格。 手动删除SRP批处理兼容性。不把材质属性放在CBuffer中。 如何使用GPU实例化 Unity使用GPU实例来共享相同网格和材质的游戏对象。 实例网格和材质： 材质球的着色器必须支持GPU实例。 Unity的标准着色器和所有表面着色器都支持GPU Instancing。要向任何其他着色器中添加GPU启动支持，请参阅创建支持GPU Intancing的着色器。 网格必须来自以下来这几种方式： Meshrenderer组件或Graphics.Rendermesh调用。Unity将这些网格添加到列表中，然后检查以查看可以实例的网格。Unity不支持SkinnedMeshrenderers和兼容SRP Batcher的Meshrenderer的组件。 Graphics.RenderMeshInstanced或Graphics.RenderMeshIndirect调用。 这些方法使用相同的着色器多次渲染相同的网格。 这些方法的每次调用都会触发单独的绘制调用。Unity不会合并这些调用。 GPU Instancing支持Unity的烘焙全球照明系统。 Unity标准着色器和表面着色器支持GPU Intancingb以及Unity的烘焙全球照明。 每个GPU实例都支持以下来源之一的全局照明： 任何数量的光照探针。 一张光照图。注意：一个实例可以使用光照图中的多个区域。 一个光照探针代理（LPPV）组件。 注意：您必须烘培包含所有实例的LPPV。 使用GPU Intancing无法有效地处理具有较少顶点的网格，因为GPU无法以完全使用GPU资源的方式分发工作。 这种处理效率低下可能对性能产生不利影响。 效率低下的阈值取决于GPU，但作为一般规则，请不要将GPU Intancing用于少于256个顶点的网格。 如果您想多次渲染具有较少顶点的网格，最好的做法是创建一个包含所有网格信息的单个缓冲区，并使用它来绘制网格。 创建支持GPU Intancing的着色器 此节包含有关如何添加GPU Instancing到自定义Unity着色器中。首先说明自定义着色器支持GPU Instancing所需的着色器的关键字（keywords），变量(variables)和函数(function)。 着色器修改 本节包含有关着色器添加的信息，这些信息与GPU实例有关。 #pragma multi_compile_instancing 生成实例变体。这是片段和顶点着色器所需的。对于表面着色器是可选的。 #pragma instancing_options 指定Unity用于实例的选项。 有关可用选项开关的信息，请参见#Pragma Intancing_options。 UNITY_VERTEX_INPUT_INSTANCE_ID 在顶点着色器输入/输出结构中定义实例ID。 要使用此宏，请启用INSTANCING_ON着色器关键字。 否则，Unity无法设置实例ID。要访问实例ID，请在#IFDEF INSTANCING_ON块中使用vertexInput.instanceID。 如果您不使用此块，则变体无法编译。 UNITY_INSTANCING_BUFFER_START(bufferName) 声明名为bufferName的每种固定常数缓冲区的开始。 将此宏与UNITY_INSTANCING_BUFFER_END一起使用，以包装您想要在每个实例中唯一的属性的声明。 使用UNITY_DEFINE_INSTANCED_PROP在缓冲区内声明属性。 UNITY_INSTANCING_BUFFER_END(bufferName) 声明名为bufferName的每种固定常数缓冲区的结束。 将此宏与UNITY_INSTANCING_BUFFER_START一起使用，以包装您想要在每个实例中唯一的属性声明。 使用UNITY_DEFINE_INSTANCED_PROP在缓冲区内声明属性。 UNITY_DEFINE_INSTANCED_PROP(type, propertyName) 用指定的类型和名称定义着色器属性。 UNITY_SETUP_INSTANCE_ID(v); 允许着色器函数访问实例ID。 对于顶点着色器，开始时需要此宏。 对于碎片着色器，此添加是可选的。 UNITY_TRANSFER_INSTANCE_ID(v, o); 将实例ID从输入结构复制到顶点着色器中的输出结构。 如果您需要在片段着色器中访问每种实体数据，请使用此宏。 UNITY_ACCESS_INSTANCED_PROP(bufferName, propertyName) 在实例常量缓冲区中访问每个着色器属性。 Unity使用实例ID去索引实例数据数组中的数据。 bufferName必须匹配包含指定属性的常量缓冲区的名称。 当您使用多个属性时，您无需在MaterialPropertyBlock对象中填充所有这些属性。 另外，如果一个实例缺乏属性，Unity将从引用材质中的默认值。 如果材质没有该属性的默认值，则Unity将值设置为0。不要放置一个非实例属性在MaterialPropertyBlock中，因为此操纵会禁用GPU实例化。 相反的，为它们创建不同的材质。 Instancing_options 开关 #pragma instancing_options 指令能使用下列的开关： forcemaxcount:batchSize和maxcount:batchSize 在大多数平台上，Unity会自动计算实例化数据数组的大小。它将目标设备上的最大常量缓冲区大小除以包含所有实例属性的结构的大小。 通常，您无需担心批量大小。 但是，某些平台需要固定的数组大小。要为这些平台指定批次大小，请使用maxcount选项。 其他平台忽略此选项。 如果你想为所有平台强制一个批次大小，使用forcemaxcount。 例如，当您的项目使用DrawMeshInstanced绘制带有256个实例化对象的绘制调用时，这很有用. 这两个选项的默认值为500。 assumeuniformscaling 指示Unity假定所有实例都具有统一的缩放（所有 X、Y 和 Z 轴的缩放相同）。 nolodfade 使Unity不将GPU实例化应用于LOD fade值。 nolightprobe 防止Unity将GPU实例化应用到Light Probe值及其遮挡数据。 如果您的项目不包含使用了GPU实例化和光照探针的对象，将此选项设置为ON可以提高性能。 nolightmap 防止Unity将GPU实例化应用到光照图中值。 如果您的项目不包含同时使用GPU实例化和光照贴图的游戏对象，则将此选项设置为ON可以提高性能。 procedural:FunctionName 生成用于Graphics.DrawMeshInstancedIndirect的附加变体。在顶点着色器阶段开始时，Unity调用冒号后指定的函数。 手动设置实例数据，请将每个实例数据在此函数中添加。 如果任何获取的实例属性包含在片段着色器中，Unity也会在片段着色器的开头调用此函数。 将实例属性添加到GPU实例化着色器 默认情况下，Unity GPU实例化在每个绘制调用中具有不同Transform信息。 要为实例添加更多变化，请修改着色器以添加每个实例的属性，例如颜色。 您可以在表面着色器和顶点/片段着色器中执行此操作。 自定义着色器不需要每个实例数据，但它们确实需要一个实例ID，因为世界矩阵需要一个实例ID才能正常运行。 表面着色器会自动设置实例ID，但自定义顶点和片段着色器不会。 要为自定义顶点和片段着色器设置ID，请在着色器的开头使用UNITY_SETUP_INSTANCE_ID。 有关如何执行此操作的示例，请参阅下方的“顶点和片段着色器”。 当您声明一个实例化属性时，Unity会将GameObjects上设置的MaterialPropertyBlock对象的所有属性值收集到单个绘制调用中。 有关如何使用MaterialPropertyBlock设置对象在运行时每个实例数据，请参阅下方的“在运行时改变每个实例数据”。 顶点和片段着色器： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768Shader &quot;Unlit&#x2F;GPUInstancing&quot;&#123; Properties &#123; _MainTex (&quot;Texture&quot;, 2D) &#x3D; &quot;white&quot; &#123;&#125; _Color (&quot;Color&quot;, Color) &#x3D; (1, 1, 1, 1) _ExtSize (&quot;ExtSize&quot;, float) &#x3D; 0 &#125; SubShader &#123; Tags &#123; &quot;RenderType&quot;&#x3D;&quot;Opaque&quot; &quot;RenderPipeline&quot;&#x3D;&quot;UniversalPipeline&quot;&#125; Pass &#123; Tags &#123;&quot;LightMode&quot; &#x3D; &quot;UniversalForward&quot;&#125; HLSLPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_instancing #include &quot;Packages&#x2F;com.unity.render-pipelines.universal&#x2F;ShaderLibrary&#x2F;Core.hlsl&quot; struct Attributes &#123; float4 positionOS : POSITION; float3 normalOS : NORMAL; float2 uv : TEXCOORD0; UNITY_VERTEX_INPUT_INSTANCE_ID &#125;; struct Varyings &#123; float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; UNITY_VERTEX_INPUT_INSTANCE_ID &#125;; sampler2D _MainTex; float4 _MainTex_ST; UNITY_INSTANCING_BUFFER_START(Test) UNITY_DEFINE_INSTANCED_PROP(float4, _Color) UNITY_DEFINE_INSTANCED_PROP(float, _ExtSize) UNITY_INSTANCING_BUFFER_END(Test) Varyings vert (Attributes input) &#123; Varyings o; UNITY_SETUP_INSTANCE_ID(input); UNITY_TRANSFER_INSTANCE_ID(input, o); float3 positionOS &#x3D; input.positionOS.xyz + input.normalOS * UNITY_ACCESS_INSTANCED_PROP(Test, _ExtSize); o.positionCS &#x3D; TransformObjectToHClip(positionOS); o.uv &#x3D; TRANSFORM_TEX(input.uv, _MainTex); return o; &#125; half4 frag (Varyings input) : SV_Target &#123; UNITY_SETUP_INSTANCE_ID(input); float4 col &#x3D; UNITY_ACCESS_INSTANCED_PROP(Test, _Color); float4 mainCol &#x3D; tex2D(_MainTex, input.uv); return mainCol * col; &#125; ENDHLSL &#125; &#125;&#125; 在运行时改变每个实例数据 1234567891011121314151617using UnityEngine;public class GPUInstanceTest : MonoBehaviour&#123; void Start() &#123; MaterialPropertyBlock props = new MaterialPropertyBlock(); for (int i = 0; i &lt; transform.childCount; i++) &#123; var mr = transform.GetChild(i).GetComponent&lt;MeshRenderer&gt;(); props.SetColor(\"_Color\", new Color(Random.Range(0.0f, 1.0f), Random.Range(0.0f, 1.0f), Random.Range(0.0f, 1.0f), 1.0f)); props.SetFloat(\"_ExtSize\", Random.Range(0.0001f, 0.0001f)); mr.SetPropertyBlock(props); // 必须要每次设置一下 &#125; &#125;&#125; SRP Batcher 可编程渲染管线(SRP) Batcher是一种绘制调用优化，可显着提高使用SRP的应用程序的性能。 SRP Batcher减少了Unity为使用相同着色器变体的材质准备和调度绘制调用所需的CPU时间。 需求和兼容性 GameObject兼容性 在任何给定的场景中, 一些游戏对象与SRP Batcher兼容，有些则不兼容。兼容的游戏对象使用SRP Batcher代码路径，不兼容的游戏对象使用标准的SRP代码路径。 有关详细信息，请参阅后续章节的“SRP Batcher的工作原理”。 GameObject必须满足以下要求才能与SRP Batcher代码路径兼容： GameObject必须包含网格或蒙皮网格。 它不能是一个粒子。 GameObject不得使用 MaterialPropertyBlocks。 GameObject使用的着色器必须与SRP Batcher兼容。 有关详细信息，请参阅后续章节的“着色器兼容性”。 着色器兼容性 高清渲染管线 (HDRP) 和通用渲染管线(URP)中的所有带光照和不带光照的着色器都符合此要求（除粒子版本）。 对于与SRP Batcher兼容的自定义着色器，它必须满足以下要求： 着色器必须在名为UnityPerDraw的单个常量缓冲区中声明所有内置引擎属性。 例如，unity_ObjectToWorld或unity_SHAr。 着色器必须在名为UnityPerMaterial的单个常量缓冲区中声明所有材质属性。 SRP Batcher的工作原理 优化绘制调用的传统方法是减少它们的数量。 相反，SRP Batcher减少了绘制调用之间的渲染状态更改。 为此，SRP Batcher结合了一系列绑定和绘制GPU命令。 每个命令序列称为一个SRP批处理。 SetPass区别 为实现渲染的最佳性能，每个SRP Batcher应包含尽可能多的绑定和绘制命令。 为此，请使用尽可能少的着色器变体。 您仍然可以根据需要使用具有相同着色器的任意多种不同材质。 当Unity在渲染循环中检测到新材质时，CPU会收集所有属性并将它们绑定到GPU的常量缓冲区中。 GPU缓冲区的数量取决于着色器如何声明其常量缓冲区。 SRP Batcher是一个底层的渲染循环，可使材质数据持久保存在GPU内存中。 如果材质内容没有改变，SRP Batcher不会改变任何渲染状态。 相反，SRP Batcher使用专用代码路径来更新大型GPU缓冲区中的Unity引擎属性，如下所示： SRP Batcher如何更新Buffer 在这里，CPU只处理Unity引擎属性，在上图中标记为 Per Object large buffer。 所有材质都有位于GPU内存中的持久常量缓冲区，随时可以使用。这会加快渲染速度，因为： 所有材质内容现在都保存在GPU内存中。 专用代码为每个对象的所有属性管理一个大型的GPU常量缓冲区。 故意删除GameObjects的SRP Batcher兼容性 在极少数情况下，您可能希望故意使特定游戏对象与SRP Batcher不兼容。 例如，如果您想使用与SRP Batcher不兼容的GPU实例化。 如果您想使用完全相同的材质渲染许多相同的网格，GPU实例化可能比SRP Batcher更高效。 要使用GPU实例化，您必须： 使用 Graphics.DrawMeshInstanced。 手动删除SRP Batcher兼容性并为材质启用GPU实例化。 有两种方法可以从GameObject中删除与SRP Batcher的兼容性： 使着色器不兼容。 使渲染器不兼容。 提示：如果您使用GPU实例化而不是SRP Batcher，请使用 Profiler以确保GPU实例化对于您的应用程序比SRP Batcher更有效。 删除着色器兼容性 您可以使手写着色器和Shader Graph着色器与SRP Batcher不兼容。 但是，对于Shader Graph着色器，如果您经常更改和重新编译Shader Graph，则使渲染器不兼容会更简单。 要使Unity着色器与SRP Batcher不兼容，您需要更改着色器源文件： 对于自定义着色器，打开着色器源文件。 对于Shader Graph着色器，将Shader Graph的已编译着色器源代码复制到新的着色器源文件中。 在您的应用程序中使用新的着色器源文件而不是着色器图。 将新的材质属性声明添加到着色器的属性块中。 不要在UnityPerMaterial常量缓冲区中声明新的材质属性。 材质属性不需要做任何事情；只需要将其放在UnityPerMaterial常量缓冲区外部，则可以使着色器与SRP Batcher不兼容。 警告：如果使用 Shader Graph，请注意每次编辑和重新编译 Shader Graph 时，都必须重复此过程。 删除渲染器兼容性 可以将MaterialPropertyBlock添加到渲染器，一旦渲染器中添加了MaterialPropertyBlock则不能SRP Batcher兼容。 SRP Batcher着色器示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061Shader &quot;Unlit&#x2F;SRPBatcher&quot;&#123; Properties &#123; _MainTex (&quot;Texture&quot;, 2D) &#x3D; &quot;white&quot; &#123;&#125; _Color (&quot;Color&quot;, Color) &#x3D; (1, 1, 1, 1) _ExtSize (&quot;ExtSize&quot;, float) &#x3D; 0 &#125; SubShader &#123; Tags &#123; &quot;RenderType&quot;&#x3D;&quot;Opaque&quot; &quot;RenderPipeline&quot;&#x3D;&quot;UniversalPipeline&quot;&#125; Pass &#123; Tags &#123;&quot;LightMode&quot; &#x3D; &quot;UniversalForward&quot;&#125; HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include &quot;Packages&#x2F;com.unity.render-pipelines.universal&#x2F;ShaderLibrary&#x2F;Core.hlsl&quot; struct Attributes &#123; float4 positionOS : POSITION; float3 normalOS : NORMAL; float2 uv : TEXCOORD0; &#125;; struct Varyings &#123; float4 positionCS : SV_POSITION; float2 uv : TEXCOORD0; &#125;; sampler2D _MainTex; CBUFFER_START(UnityPerMaterial) float4 _Color; float _ExtSize; float4 _MainTex_ST; CBUFFER_END Varyings vert (Attributes input) &#123; Varyings o; float3 positionOS &#x3D; input.positionOS.xyz + input.normalOS * _ExtSize; o.positionCS &#x3D; TransformObjectToHClip(positionOS); o.uv &#x3D; TRANSFORM_TEX(input.uv, _MainTex); return o; &#125; half4 frag (Varyings input) : SV_Target &#123; float4 mainCol &#x3D; tex2D(_MainTex, input.uv); return mainCol * _Color; &#125; ENDHLSL &#125; &#125;&#125; 说明：以上的测试结果是基于Unity 2021.3.6f1，每个版本的测试的结果可能会不一样。 官方文档","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"Batching","slug":"Batching","permalink":"http://yoursite.com/tags/Batching/"}]},{"title":"URP源码阅读-应用阶段","slug":"Unity/Graphics/URP源码阅读-应用阶段","date":"2022-10-06T06:44:58.000Z","updated":"2025-04-26T11:06:24.130Z","comments":true,"path":"2022/10/06/Unity/Graphics/URP源码阅读-应用阶段/","link":"","permalink":"http://yoursite.com/2022/10/06/Unity/Graphics/URP%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E5%BA%94%E7%94%A8%E9%98%B6%E6%AE%B5/","excerpt":"通用渲染管线（URP）是Unity官方创建的可编程渲染管线（SRP）。URP提供了艺术家友好的工作流程，能够快速且简单创建跨平台的图形，从移动端到高端控制台和PC。SRP的内容可以参见:Unity渲染管线介绍。 Unity将渲染管线的代码分为了几个包，分别如下： com.unity.render-pipelines.core 此包包含了一些公共的可重用代码，URP和HDRP都会使用此包。 com.unity.render-pipelines.high-definition-config HDRP配置包。 com.unity.render-pipelines.high-definition HDRP的核心包。 com.unity.render-pipelines.universal URP的核心包。 应用阶段 从名字我们可以看出，这个阶段是由我们的应用主导的，因此通常由CPU负责实现。换句话说，我们这些开发者具有这个阶段的绝对控制权。 在这阶段中，开发者有3个主要任务： 我们要准备好场景数据，例如摄像机的位置、视椎体、场景中包含了哪些模型、使用了哪些光源等等； 为了渲染性能，我们往往需要做一个粗粒度剔除（culling）工作,以把哪些不可见的物体踢出去，这样就不需要再移交给几何阶段进行处理； 最后我们要设置好每个模型的渲染状态，这些渲染状态包括但不限于它们使用的材质（漫反射颜色、高光反色颜色）、使用的纹理、使用的Shader等。 这一阶段最重要的输出是渲染所需的几何信息，即渲染图元（rendering primitives）。通俗来讲，渲染图元可以是点、线、三角面等。这些渲染图元将会被传递给下一个阶段——几何阶段。","text":"通用渲染管线（URP）是Unity官方创建的可编程渲染管线（SRP）。URP提供了艺术家友好的工作流程，能够快速且简单创建跨平台的图形，从移动端到高端控制台和PC。SRP的内容可以参见:Unity渲染管线介绍。 Unity将渲染管线的代码分为了几个包，分别如下： com.unity.render-pipelines.core 此包包含了一些公共的可重用代码，URP和HDRP都会使用此包。 com.unity.render-pipelines.high-definition-config HDRP配置包。 com.unity.render-pipelines.high-definition HDRP的核心包。 com.unity.render-pipelines.universal URP的核心包。 应用阶段 从名字我们可以看出，这个阶段是由我们的应用主导的，因此通常由CPU负责实现。换句话说，我们这些开发者具有这个阶段的绝对控制权。 在这阶段中，开发者有3个主要任务： 我们要准备好场景数据，例如摄像机的位置、视椎体、场景中包含了哪些模型、使用了哪些光源等等； 为了渲染性能，我们往往需要做一个粗粒度剔除（culling）工作,以把哪些不可见的物体踢出去，这样就不需要再移交给几何阶段进行处理； 最后我们要设置好每个模型的渲染状态，这些渲染状态包括但不限于它们使用的材质（漫反射颜色、高光反色颜色）、使用的纹理、使用的Shader等。 这一阶段最重要的输出是渲染所需的几何信息，即渲染图元（rendering primitives）。通俗来讲，渲染图元可以是点、线、三角面等。这些渲染图元将会被传递给下一个阶段——几何阶段。 URP渲染管线 RenderPipelineManager RenderPipelineManager主要管理管线的创建和切换，以及一些渲染循环中的事件。 渲染入口RenderPipelineManager.DoRenderLoop_Internal, 此后函数是Unity Native Code调用的 123456789101112131415161718192021222324252627// Unity Native Code每帧调用private static void DoRenderLoop_Internal( RenderPipelineAsset pipe, IntPtr loopPtr, List&lt;Camera.RenderRequest&gt; renderRequests, AtomicSafetyHandle safety)&#123; // 检查并通过渲染管线资源创建创建渲染管线对象（RenderPipeline） RenderPipelineManager.PrepareRenderPipeline(pipe); // 没有创建成功则返回 if (RenderPipelineManager.currentPipeline == null) return; // 创建与底层渲染接口交互的对象 ScriptableRenderContext context = new ScriptableRenderContext(loopPtr, safety); // 清除相机列表（重复利用此List,避免GC） RenderPipelineManager.s_Cameras.Clear(); // 通过底层渲染接口获取场景中的相机 context.GetCameras(RenderPipelineManager.s_Cameras); // 是否是单独的渲染请求，不是则按标准流程，所有相机进行渲染，否则就用第一相机执行单独的渲染请求 if (renderRequests == null) RenderPipelineManager.currentPipeline.InternalRender(context, RenderPipelineManager.s_Cameras); else RenderPipelineManager.currentPipeline.InternalRenderWithRequests(context, RenderPipelineManager.s_Cameras, renderRequests); // 渲染完后把相机从列表中清除 RenderPipelineManager.s_Cameras.Clear();&#125; 渲染循环事件： 123456789101112131415// 开始渲染public static event Action&lt;ScriptableRenderContext, List&lt;Camera&gt;&gt; beginContextRendering; //2021.1及以后的版本用此，public static event Action&lt;ScriptableRenderContext, Camera[]&gt; beginFrameRendering; // 之前的版本，监听其中一个即可// 开始一个相机的渲染public static event Action&lt;ScriptableRenderContext, Camera&gt; beginCameraRendering;// 一个相机的渲染结束public static event Action&lt;ScriptableRenderContext, Camera&gt; endCameraRendering;// 渲染结束public static event Action&lt;ScriptableRenderContext, List&lt;Camera&gt;&gt; endContextRendering; //2021.1及以后的版本用此public static event Action&lt;ScriptableRenderContext, Camera[]&gt; endFrameRendering; // 之前的版本，监听其中一个即可// 管线类型改变public static event Action activeRenderPipelineTypeChanged; RenderPipeline RenderPipeline的作用主要是提供了Render抽象函数（渲染的入口）以及提供渲染循环事件函数并调用到RenderPipelineManager中的对应事件。渲染循环事件函数提供给子类调用，避免每个子类写重复的代码。 核心函数： 12345678910// 此函数由RenderPipelineManager.DoRenderLoop_Internal调用而来internal void InternalRender(ScriptableRenderContext context, List&lt;Camera&gt; cameras)&#123; // 销毁了，不应该调用 if (this.disposed) throw new ObjectDisposedException(string.Format(\"&#123;0&#125; has been disposed. Do not call Render on disposed a RenderPipeline.\", (object) this)); // 调用抽象的Render函数，派生的渲染管线必须实现此函数 this.Render(context, cameras);&#125; UniversalRenderPipeline UniversalRenderPipeline继承至RenderPipeline，并实现了Render函数，当然此类的核心函数就是Render。 核心函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162protected override void Render(ScriptableRenderContext renderContext, List&lt;Camera&gt; cameras)&#123; // 记录一次渲染循环的消耗 using var profScope = new ProfilingScope(null, ProfilingSampler.Get(URPProfileId.UniversalRenderTotal)); // 记录开始上下文渲染的消耗 using (new ProfilingScope(null, Profiling.Pipeline.beginContextRendering)) &#123; BeginContextRendering(renderContext, cameras); &#125; // 相关配置 GraphicsSettings.lightsUseLinearIntensity = (QualitySettings.activeColorSpace == ColorSpace.Linear); GraphicsSettings.lightsUseColorTemperature = true; GraphicsSettings.useScriptableRenderPipelineBatching = asset.useSRPBatcher; GraphicsSettings.defaultRenderingLayerMask = k_DefaultRenderingLayerMask; // 设置环境光和天空盒子颜色到shader属性（Lighting窗口下的Environment页签下的参数） SetupPerFrameShaderConstants(); // 根据相机的depth排序 SortCameras(cameras); // 遍历排序后的相机，并渲染 for (int i = 0; i &lt; cameras.Count; ++i) &#123; // 是Game窗口的相机？ var camera = cameras[i]; if (IsGameCamera(camera)) &#123; // 渲染相机栈 RenderCameraStack(renderContext, camera); &#125; else &#123; // 捕获BeginCameraRendering回调函数的性能消耗 using (new ProfilingScope(null, Profiling.Pipeline.beginCameraRendering)) &#123; BeginCameraRendering(renderContext, camera); &#125;#if VISUAL_EFFECT_GRAPH_0_0_1_OR_NEWER // GPU版的粒子系统，此函数更新VisualEffect内部使用的材质，但不执行任何渲染命令。要执行渲染命令，请调用VFXManager.ProcessCamera。再剔除之前调用 VFX.VFXManager.PrepareCamera(camera);#endif // 更新后处理参数 UpdateVolumeFramework(camera, null); // 渲染单个相机 RenderSingleCamera(renderContext, camera); // 捕获EndCameraRendering回调函数的性能消耗 using (new ProfilingScope(null, Profiling.Pipeline.endCameraRendering)) &#123; EndCameraRendering(renderContext, camera); &#125; &#125; &#125; // 捕获EndContextRendering回调函数的性能消耗 using (new ProfilingScope(null, Profiling.Pipeline.endContextRendering)) &#123; EndContextRendering(renderContext, cameras); &#125;&#125; UniversalRenderPipeline.RenderCameraStack，渲染Base相机，以及Base相机的相机栈，此函数调用UniversalRenderPipeline.RenderSingleCamera函数来渲染单个相机: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157static void RenderCameraStack(ScriptableRenderContext context, Camera baseCamera)&#123; // 性能统计 using var profScope = new ProfilingScope(null, ProfilingSampler.Get(URPProfileId.RenderCameraStack)); // 获取相机附加数据 baseCamera.TryGetComponent&lt;UniversalAdditionalCameraData&gt;(out var baseCameraAdditionalData); // Overlay相机，在渲染Base相机的时候渲染 if (baseCameraAdditionalData != null &amp;&amp; baseCameraAdditionalData.renderType == CameraRenderType.Overlay) return; // 检查渲染器是否支持相机栈，并获取相机栈中的相机对象 var renderer = baseCameraAdditionalData?.scriptableRenderer; bool supportsCameraStacking = renderer != null &amp;&amp; renderer.SupportsCameraStackingType(CameraRenderType.Base); List&lt;Camera&gt; cameraStack = (supportsCameraStacking) ? baseCameraAdditionalData?.cameraStack : null; // 相机是否支持后处理 bool anyPostProcessingEnabled = baseCameraAdditionalData != null &amp;&amp; baseCameraAdditionalData.renderPostProcessing; // 找Base相机，相机栈中的相机，Base相机中有激活的Overlay相机，且渲染器和Base相机相同则需要进行相机栈的渲染 int lastActiveOverlayCameraIndex = -1; if (cameraStack != null) &#123; // 获取Base相机的渲染器类型 var baseCameraRendererType = baseCameraAdditionalData?.scriptableRenderer.GetType(); bool shouldUpdateCameraStack = false; // 遍历相机栈的相机 for (int i = 0; i &lt; cameraStack.Count; ++i) &#123; // Base相机栈中有空的，则需要更新一下栈，将栈中空的移除 Camera currCamera = cameraStack[i]; if (currCamera == null) &#123; shouldUpdateCameraStack = true; continue; &#125; // 当前相机是激活的 if (currCamera.isActiveAndEnabled) &#123; // 获取当前栈中的相机数据 currCamera.TryGetComponent&lt;UniversalAdditionalCameraData&gt;(out var data); // 检查是否与Base相机的渲染器相同 var currCameraRendererType = data?.scriptableRenderer.GetType(); if (currCameraRendererType != baseCameraRendererType) &#123; Debug.LogWarning(\"Only cameras with compatible renderer types can be stacked. \" + $\"The camera: &#123;currCamera.name&#125; are using the renderer &#123;currCameraRendererType.Name&#125;, \" + $\"but the base camera: &#123;baseCamera.name&#125; are using &#123;baseCameraRendererType.Name&#125;. Will skip rendering\"); continue; &#125; // 检查当前相机是否支持相机栈化 var overlayRenderer = data.scriptableRenderer; if ((overlayRenderer.SupportedCameraStackingTypes() &amp; 1 &lt;&lt; (int)CameraRenderType.Overlay) == 0) &#123; Debug.LogWarning($\"The camera: &#123;currCamera.name&#125; is using a renderer of type &#123;renderer.GetType().Name&#125; which does not support Overlay cameras in it's current state.\"); continue; &#125; // 相机类型不是Overlay,也不能栈话 if (data == null || data.renderType != CameraRenderType.Overlay) &#123; Debug.LogWarning($\"Stack can only contain Overlay cameras. The camera: &#123;currCamera.name&#125; \" + $\"has a type &#123;data.renderType&#125; that is not supported. Will skip rendering.\"); continue; &#125; // 栈中的相机是否支持后处理 anyPostProcessingEnabled |= data.renderPostProcessing; // 记录栈中最后一个激活的相机 lastActiveOverlayCameraIndex = i; &#125; &#125; // 更新相机栈，将空的，从List中移除 if (shouldUpdateCameraStack) &#123; baseCameraAdditionalData.UpdateCameraStack(); &#125; &#125; // GLES2 不支持后处理 anyPostProcessingEnabled &amp;= SystemInfo.graphicsDeviceType != GraphicsDeviceType.OpenGLES2; // Base相机栈中一旦有一个激活的Overlay相机，则可以进行相机栈渲染 bool isStackedRendering = lastActiveOverlayCameraIndex != -1; // 统计相机开始渲染回调消耗 using (new ProfilingScope(null, Profiling.Pipeline.beginCameraRendering)) &#123; BeginCameraRendering(context, baseCamera); &#125; // 更新后处理参数 UpdateVolumeFramework(baseCamera, baseCameraAdditionalData); // 初始化相机数据 InitializeCameraData(baseCamera, baseCameraAdditionalData, !isStackedRendering, out var baseCameraData); // 初始化相机中创建或从相机的RenderTarget中获取的 RenderTextureDescriptor originalTargetDesc = baseCameraData.cameraTargetDescriptor; // 是否启用了GPU粒子系统#if VISUAL_EFFECT_GRAPH_0_0_1_OR_NEWER VFX.VFXManager.PrepareCamera(baseCamera);#endif // 渲染单个相机（Base相机） RenderSingleCamera(context, baseCameraData, anyPostProcessingEnabled); // 统计相机结束渲染回调消耗 using (new ProfilingScope(null, Profiling.Pipeline.endCameraRendering)) &#123; EndCameraRendering(context, baseCamera); &#125; // 如果此Base相机需要栈化渲染，则渲染Base相机中相机栈列表中的相机 if (isStackedRendering) &#123; // 遍历栈中相机列表 for (int i = 0; i &lt; cameraStack.Count; ++i) &#123; // 没有启用的相机不管 var currCamera = cameraStack[i]; if (!currCamera.isActiveAndEnabled) continue; // 获取当前相机数据 currCamera.TryGetComponent&lt;UniversalAdditionalCameraData&gt;(out var currCameraData); // 相机是Overlay并启用了，则准备渲染Overly相机 if (currCameraData != null) &#123; // 将上面初始化的Base相机的相机数据拷贝到Overlay相机数据中（CameraData是个结构体） CameraData overlayCameraData = baseCameraData; // 检查是否是最后一个Overlay相机，是的话，需要将结果直接渲染到相机目标中 bool lastCamera = i == lastActiveOverlayCameraIndex; // 统计相机开始渲染回调消耗 using (new ProfilingScope(null, Profiling.Pipeline.beginCameraRendering)) &#123; BeginCameraRendering(context, currCamera); &#125;#if VISUAL_EFFECT_GRAPH_0_0_1_OR_NEWER // 是否启用了GPU粒子系统 VFX.VFXManager.PrepareCamera(currCamera);#endif // 更新后处理参数 UpdateVolumeFramework(currCamera, currCameraData); // 初始化附加相机数据 InitializeAdditionalCameraData(currCamera, currCameraData, lastCamera, ref overlayCameraData); // 渲染单个相机（Overlay相机） RenderSingleCamera(context, overlayCameraData, anyPostProcessingEnabled); // 统计相机结束渲染回调消耗 using (new ProfilingScope(null, Profiling.Pipeline.endCameraRendering)) &#123; EndCameraRendering(context, currCamera); &#125; &#125; &#125; &#125;&#125; UniversalRenderPipeline.UpdateVolumeFramework，此函数用于更新后处理参数，主要是计算多个Volume之间插值后的最终参数值: 1234567891011121314151617181920212223242526272829303132333435363738static void UpdateVolumeFramework(Camera camera, UniversalAdditionalCameraData additionalCameraData)&#123; using var profScope = new ProfilingScope(null, ProfilingSampler.Get(URPProfileId.UpdateVolumeFramework)); // Scene相机，Game相机中启用了每帧更新或在编辑器下且不是在运行时才需要更新 bool shouldUpdate = camera.cameraType == CameraType.SceneView; shouldUpdate |= additionalCameraData != null &amp;&amp; additionalCameraData.requiresVolumeFrameworkUpdate;#if UNITY_EDITOR shouldUpdate |= Application.isPlaying == false;#endif // 当禁用了每帧更新 if (!shouldUpdate &amp;&amp; additionalCameraData) &#123; //如果没有后处理栈，则创建一个（只有在第一帧的时候会到这来） if (additionalCameraData.volumeStack == null) &#123; camera.UpdateVolumeStack(additionalCameraData); &#125; // 设置为当前的后处理栈 VolumeManager.instance.stack = additionalCameraData.volumeStack; return; &#125; // 当我们想要每帧更新的时候，则将相机数据对象上的后处理栈销毁，从新再创建一个 if (additionalCameraData &amp;&amp; additionalCameraData.volumeStack != null) &#123; camera.DestroyVolumeStack(additionalCameraData); &#125; // 获取后处理作用的Layer，以及Trigger器的框定的范围 camera.GetVolumeLayerMaskAndTrigger(additionalCameraData, out LayerMask layerMask, out Transform trigger); // 将当前的后处理栈，设置为默认的 VolumeManager.instance.ResetMainStack(); // 然后根据Layer和Trigger框定的范围去查找范围内的Volume组件，并对组件上的相同参数进行插值混合。 VolumeManager.instance.Update(trigger, layerMask);&#125; UniversalRenderPipeline.InitializeCameraData, 初始化相机设置（包括相机的基础参数和渲染目标描述器）大部分设置继承至Base相机的设置: 1234567891011121314151617181920212223242526static void InitializeCameraData(Camera camera, UniversalAdditionalCameraData additionalCameraData, bool resolveFinalTarget, out CameraData cameraData)&#123; using var profScope = new ProfilingScope(null, Profiling.Pipeline.initializeCameraData); // 创建CameraData cameraData = new CameraData(); // 初始化相机的公共数据（Base和Overlay后会有的） InitializeStackedCameraData(camera, additionalCameraData, ref cameraData); // 初始化各种相机单独数据 InitializeAdditionalCameraData(camera, additionalCameraData, resolveFinalTarget, ref cameraData); // 准备渲染目标描述器(Descriptor)需要的参数 // 获取MSSA级别 var renderer = additionalCameraData?.scriptableRenderer; bool rendererSupportsMSAA = renderer != null &amp;&amp; renderer.supportedRenderingFeatures.msaa; int msaaSamples = 1; if (camera.allowMSAA &amp;&amp; asset.msaaSampleCount &gt; 1 &amp;&amp; rendererSupportsMSAA) msaaSamples = (camera.targetTexture != null) ? camera.targetTexture.antiAliasing : asset.msaaSampleCount; // 需要Alpha通道？ bool needsAlphaChannel = Graphics.preserveFramebufferAlpha; // 创建渲染目标描述器(Descriptor) cameraData.cameraTargetDescriptor = CreateRenderTextureDescriptor(camera, cameraData.renderScale, cameraData.isHdrEnabled, msaaSamples, needsAlphaChannel, cameraData.requiresOpaqueTexture);&#125; UniversalRenderPipeline.InitializeStackedCameraData, 初始相机栈中相机的公共设置参数，特定相机特有参数设置参见InitializeAdditionalCameraData函数: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091static void InitializeStackedCameraData(Camera baseCamera, UniversalAdditionalCameraData baseAdditionalCameraData, ref CameraData cameraData)&#123; using var profScope = new ProfilingScope(null, Profiling.Pipeline.initializeStackedCameraData); // 从相机获取targetTexture和cameraType var settings = asset; cameraData.targetTexture = baseCamera.targetTexture; cameraData.cameraType = baseCamera.cameraType; bool isSceneViewCamera = cameraData.isSceneViewCamera; /////////////////////////////////////////////////////////////////// // 环境和后处理设置 / /////////////////////////////////////////////////////////////////// if (isSceneViewCamera) &#123; cameraData.volumeLayerMask = 1; // \"Default\" cameraData.volumeTrigger = null; cameraData.isStopNaNEnabled = false; cameraData.isDitheringEnabled = false; cameraData.antialiasing = AntialiasingMode.None; cameraData.antialiasingQuality = AntialiasingQuality.High; &#125; else if (baseAdditionalCameraData != null) &#123; cameraData.volumeLayerMask = baseAdditionalCameraData.volumeLayerMask; cameraData.volumeTrigger = baseAdditionalCameraData.volumeTrigger == null ? baseCamera.transform : baseAdditionalCameraData.volumeTrigger; cameraData.isStopNaNEnabled = baseAdditionalCameraData.stopNaN &amp;&amp; SystemInfo.graphicsShaderLevel &gt;= 35; // 将shader中Nan/Inf的值用黑色像素替代 cameraData.isDitheringEnabled = baseAdditionalCameraData.dithering; // 应用一个8位的抖动值，防止颜色条带的产生 cameraData.antialiasing = baseAdditionalCameraData.antialiasing; // 抗锯齿模式（FXAA和MSAA） cameraData.antialiasingQuality = baseAdditionalCameraData.antialiasingQuality; // 抗锯齿质量 &#125; else &#123; cameraData.volumeLayerMask = 1; // \"Default\" cameraData.volumeTrigger = null; cameraData.isStopNaNEnabled = false; cameraData.isDitheringEnabled = false; cameraData.antialiasing = AntialiasingMode.None; cameraData.antialiasingQuality = AntialiasingQuality.High; &#125; /////////////////////////////////////////////////////////////////// // 控制相机输出的设置 / /////////////////////////////////////////////////////////////////// //HDR是否启用 cameraData.isHdrEnabled = baseCamera.allowHDR &amp;&amp; settings.supportsHDR; // 相机输出大小 Rect cameraRect = baseCamera.rect; cameraData.pixelRect = baseCamera.pixelRect; cameraData.pixelWidth = baseCamera.pixelWidth; cameraData.pixelHeight = baseCamera.pixelHeight; cameraData.aspectRatio = (float)cameraData.pixelWidth / (float)cameraData.pixelHeight; cameraData.isDefaultViewport = (!(Math.Abs(cameraRect.x) &gt; 0.0f || Math.Abs(cameraRect.y) &gt; 0.0f || Math.Abs(cameraRect.width) &lt; 1.0f || Math.Abs(cameraRect.height) &lt; 1.0f)); // 渲染缩放 const float kRenderScaleThreshold = 0.05f; cameraData.renderScale = (Mathf.Abs(1.0f - settings.renderScale) &lt; kRenderScaleThreshold) ? 1.0f : settings.renderScale; // 确定最后使用的放大过滤器 cameraData.upscalingFilter = ResolveUpscalingFilterSelection(new Vector2(cameraData.pixelWidth, cameraData.pixelHeight), cameraData.renderScale, settings.upscalingFilter); // 渲染缩放大于1则在最终渲染时，需要向下采样，否则向上采样 if (cameraData.renderScale &gt; 1.0f) &#123; cameraData.imageScalingMode = ImageScalingMode.Downscaling; &#125; else if ((cameraData.renderScale &lt; 1.0f) || (cameraData.upscalingFilter == ImageUpscalingFilter.FSR)) &#123; // 启用 FSR 后，我们仍将 100% 渲染比例视为放大操作。 //超分辨率（Super-resolution），有时候又称作放大（upscale, upsize），是一类提高视频或图像分辨率和质量的算法。在视频编辑和图像处理领域，超分辨率非常常见，比如监控视频的处理以及移动设备高分辨率摄像的需求，还有老照片、老电影的高清修复，都会用到超分辨率的技术。而在游戏领域，随着4K/8K分辨率的应用和光线追踪技术的引入，使用原生分辨率渲染图像对于硬件设备的压力确实较高，这时如果使用低分辨率渲染游戏，再通过超分辨率技术使之适应如今高分辨率的屏幕，那么仅会产生少量的画质损失，却能换来可观的帧率提升，必然会成为次时代高画质游戏扩大销量的有效手段之一。 cameraData.imageScalingMode = ImageScalingMode.Upscaling; &#125; else &#123; cameraData.imageScalingMode = ImageScalingMode.None; &#125; //FSR 参数用于控制FSR的锐化强度的 cameraData.fsrOverrideSharpness = settings.fsrOverrideSharpness; cameraData.fsrSharpness = settings.fsrSharpness; // 渲染对象顺序规则 var commonOpaqueFlags = SortingCriteria.CommonOpaque; var noFrontToBackOpaqueFlags = SortingCriteria.SortingLayer | SortingCriteria.RenderQueue | SortingCriteria.OptimizeStateChanges | SortingCriteria.CanvasOrder; bool hasHSRGPU = SystemInfo.hasHiddenSurfaceRemovalOnGPU; bool canSkipFrontToBackSorting = (baseCamera.opaqueSortMode == OpaqueSortMode.Default &amp;&amp; hasHSRGPU) || baseCamera.opaqueSortMode == OpaqueSortMode.NoDistanceSort; cameraData.defaultOpaqueSortFlags = canSkipFrontToBackSorting ? noFrontToBackOpaqueFlags : commonOpaqueFlags; cameraData.captureActions = CameraCaptureBridge.GetCaptureActions(baseCamera);&#125; UniversalRenderPipeline.InitializeAdditionalCameraData, 初始化相机栈中相机特有参数: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283static void InitializeAdditionalCameraData(Camera camera, UniversalAdditionalCameraData additionalCameraData, bool resolveFinalTarget, ref CameraData cameraData)&#123; using var profScope = new ProfilingScope(null, Profiling.Pipeline.initializeAdditionalCameraData); var settings = asset; cameraData.camera = camera; // 初始化阴影参数 bool anyShadowsEnabled = settings.supportsMainLightShadows || settings.supportsAdditionalLightShadows; cameraData.maxShadowDistance = Mathf.Min(settings.shadowDistance, camera.farClipPlane); cameraData.maxShadowDistance = (anyShadowsEnabled &amp;&amp; cameraData.maxShadowDistance &gt;= camera.nearClipPlane) ? cameraData.maxShadowDistance : 0.0f; // 在编辑器时，要设置预览相机的背景颜色#if UNITY_EDITOR if (cameraData.camera.cameraType == CameraType.Preview) &#123; camera.backgroundColor = CoreRenderPipelinePreferences.previewBackgroundColor; &#125;#endif // Scene相机 bool isSceneViewCamera = cameraData.isSceneViewCamera; if (isSceneViewCamera) &#123; cameraData.renderType = CameraRenderType.Base; cameraData.clearDepth = true; cameraData.postProcessEnabled = CoreUtils.ArePostProcessesEnabled(camera); cameraData.requiresDepthTexture = settings.supportsCameraDepthTexture; cameraData.requiresOpaqueTexture = settings.supportsCameraOpaqueTexture; cameraData.renderer = asset.scriptableRenderer; &#125; else if (additionalCameraData != null) &#123; cameraData.renderType = additionalCameraData.renderType; cameraData.clearDepth = (additionalCameraData.renderType != CameraRenderType.Base) ? additionalCameraData.clearDepth : true; cameraData.postProcessEnabled = additionalCameraData.renderPostProcessing; cameraData.maxShadowDistance = (additionalCameraData.renderShadows) ? cameraData.maxShadowDistance : 0.0f; cameraData.requiresDepthTexture = additionalCameraData.requiresDepthTexture; cameraData.requiresOpaqueTexture = additionalCameraData.requiresColorTexture; cameraData.renderer = additionalCameraData.scriptableRenderer; &#125; else &#123; cameraData.renderType = CameraRenderType.Base; cameraData.clearDepth = true; cameraData.postProcessEnabled = false; cameraData.requiresDepthTexture = settings.supportsCameraDepthTexture; cameraData.requiresOpaqueTexture = settings.supportsCameraOpaqueTexture; cameraData.renderer = asset.scriptableRenderer; &#125; // 在GLes2中禁用后处理 cameraData.postProcessEnabled &amp;= SystemInfo.graphicsDeviceType != GraphicsDeviceType.OpenGLES2; cameraData.requiresDepthTexture |= isSceneViewCamera; cameraData.postProcessingRequiresDepthTexture |= CheckPostProcessForDepth(cameraData); cameraData.resolveFinalTarget = resolveFinalTarget; // 是否绘制到最终的目标（相机栈中最后一个相机才需要绘制到最终目标上） // 对于Overlay相机应该禁用深度和颜色拷贝，应当将它们添加到渲染器中，以避免相机栈化后打断渲染器的执行 bool isOverlayCamera = (cameraData.renderType == CameraRenderType.Overlay); if (isOverlayCamera) &#123; cameraData.requiresDepthTexture = false; cameraData.requiresOpaqueTexture = false; cameraData.postProcessingRequiresDepthTexture = false; &#125; // 当Overlay相机与Base相机的视口大小不同时，需要修正投影矩阵 Matrix4x4 projectionMatrix = camera.projectionMatrix; if (isOverlayCamera &amp;&amp; !camera.orthographic &amp;&amp; cameraData.pixelRect != camera.pixelRect) &#123; // m00 = (cotangent / aspect), 因此 m00 * aspect = cotangent. float cotangent = camera.projectionMatrix.m00 * camera.aspect; // 通过除以Base相机的aspectRatio获得新的m00 float newCotangent = cotangent / cameraData.aspectRatio; projectionMatrix.m00 = newCotangent; &#125; cameraData.SetViewAndProjectionMatrix(camera.worldToCameraMatrix, projectionMatrix); cameraData.worldSpaceCameraPos = camera.transform.position;&#125; 相机数据初始化完后，就调用UniversalRenderPipeline.RenderSingleCamera函数进行单个相机的渲染： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677static void RenderSingleCamera(ScriptableRenderContext context, CameraData cameraData, bool anyPostProcessingEnabled)&#123; Camera camera = cameraData.camera; // 相机使用的渲染器 var renderer = cameraData.renderer; if (renderer == null) &#123; Debug.LogWarning(string.Format(\"Trying to render &#123;0&#125; with an invalid renderer. Camera rendering will be skipped.\", camera.name)); return; &#125; // 获取相机的剔除参数 if (!TryGetCullingParameters(cameraData, out var cullingParameters)) return; // 当前渲染器 ScriptableRenderer.current = renderer; bool isSceneViewCamera = cameraData.isSceneViewCamera; // 从池里拿一个CommandBuffer CommandBuffer cmd = CommandBufferPool.Get(); // 获取性能采样器 ProfilingSampler sampler = Profiling.TryGetOrAddCameraSampler(camera); using (new ProfilingScope(cmdScope, sampler)) &#123; // 清除color， depth缓冲区 renderer.Clear(cameraData.renderType); // 设置相机剔除参数 using (new ProfilingScope(null, Profiling.Pipeline.Renderer.setupCullingParameters)) &#123; // 在设置剔除参数之前调用渲染器的OnPreCullRenderPasses,此函数会路由到渲染特性中 renderer.OnPreCullRenderPasses(in cameraData); renderer.SetupCullingParameters(ref cullingParameters, ref cameraData); &#125; context.ExecuteCommandBuffer(cmd); // 发送在CommandBuffer中的所有命令到ScriptableRenderContext上下文 cmd.Clear(); // 根据剔除参数，调用ScriptableRenderContext的剔除函数进行剔除，并返回剔除结果 var cullResults = context.Cull(ref cullingParameters); // 根据剔除结果和其他参数初始化渲染数据(灯光，阴影和后处理数据等，具体参见RenderingData结构) InitializeRenderingData(asset, ref cameraData, ref cullResults, anyPostProcessingEnabled, out var renderingData); // 安排各种Pass和RenderFeature using (new ProfilingScope(null, Profiling.Pipeline.Renderer.setup)) &#123; renderer.Setup(context, ref renderingData); &#125; // 调用渲染器内的的Execute执行安排的Pass和RenderFeature renderer.Execute(context, ref renderingData); // 清理灯数据 CleanupLightData(ref renderingData.lightData); &#125; // 发送在CommandBuffer中的所有命令到ScriptableRenderContext上下文 context.ExecuteCommandBuffer(cmd); // CommandBuffer发送后，放回池里 CommandBufferPool.Release(cmd); // 将发送到ScriptableRenderContext的命令提交到底层执行 using (new ProfilingScope(null, Profiling.Pipeline.Context.submit)) &#123; if (renderer.useRenderPassEnabled &amp;&amp; !context.SubmitForRenderPassValidation()) &#123; renderer.useRenderPassEnabled = false; CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.RenderPassEnabled, false); Debug.LogWarning(\"Rendering command not supported inside a native RenderPass found. Falling back to non-RenderPass rendering path\"); &#125; context.Submit(); &#125; // 清除当前渲染器 ScriptableRenderer.current = null;&#125; 在渲染单个相机之前，需要对渲染数据进行初始化，UniversalRenderPipeline.InitializeRenderingData： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static void InitializeRenderingData(UniversalRenderPipelineAsset settings, ref CameraData cameraData, ref CullingResults cullResults, bool anyPostProcessingEnabled, out RenderingData renderingData)&#123; using var profScope = new ProfilingScope(null, Profiling.Pipeline.initializeRenderingData); // 相机剔除后，对此次渲染有影响的灯光 var visibleLights = cullResults.visibleLights; // 获取主灯光（如果Light窗口指定了太阳光（sun）,则主光将是此太阳光，否则将获取亮度最强的方向光） int mainLightIndex = GetMainLightIndex(settings, visibleLights); bool mainLightCastShadows = false; bool additionalLightsCastShadows = false; // 需要阴影 if (cameraData.maxShadowDistance &gt; 0.0f) &#123; // 主灯光投射阴影? mainLightCastShadows = (mainLightIndex != -1 &amp;&amp; visibleLights[mainLightIndex].light != null &amp;&amp; visibleLights[mainLightIndex].light.shadows != LightShadows.None); // 逐像素灯光才能投射阴影 if (settings.additionalLightsRenderingMode == LightRenderingMode.PerPixel) &#123; for (int i = 0; i &lt; visibleLights.Length; ++i) &#123; if (i == mainLightIndex) continue; Light light = visibleLights[i].light; // 附加方向光目前不支持投射阴影 if ((visibleLights[i].lightType == LightType.Spot || visibleLights[i].lightType == LightType.Point) &amp;&amp; light != null &amp;&amp; light.shadows != LightShadows.None) &#123; additionalLightsCastShadows = true; break; &#125; &#125; &#125; &#125; renderingData.cullResults = cullResults; renderingData.cameraData = cameraData; // 初始化灯光数据 InitializeLightData(settings, visibleLights, mainLightIndex, out renderingData.lightData); // 初始化阴影数据 InitializeShadowData(settings, visibleLights, mainLightCastShadows, additionalLightsCastShadows &amp;&amp; !renderingData.lightData.shadeAdditionalLightsPerVertex, out renderingData.shadowData); // 初始化后处理数据 InitializePostProcessingData(settings, out renderingData.postProcessingData); // 其他渲染数据初始化 renderingData.supportsDynamicBatching = settings.supportsDynamicBatching; renderingData.perObjectData = GetPerObjectLightFlags(renderingData.lightData.additionalLightsCount); renderingData.postProcessingEnabled = anyPostProcessingEnabled; CheckAndApplyDebugSettings(ref renderingData);&#125; URP渲染器 一个渲染管线中可以包含多个渲染器，不同的相机可以使用不同的渲染器进行渲染。 ScriptableRenderer ScriptableRenderer是所有渲染器的基类，提供了基础的功能，比如：设置相机矩阵，Shader中用的到一些变量，执行渲染器中的RendererFeature和Pass等。 1. 首先清除color， depth缓冲区，ScriptableRenderer.Clear： 1234567891011121314151617internal void Clear(CameraRenderType cameraType)&#123; // 激活的颜色缓冲区 m_ActiveColorAttachments[0] = BuiltinRenderTextureType.CameraTarget; for (int i = 1; i &lt; m_ActiveColorAttachments.Length; ++i) m_ActiveColorAttachments[i] = 0; // 激活的深度 m_ActiveDepthAttachment = BuiltinRenderTextureType.CameraTarget; m_FirstTimeCameraColorTargetIsBound = cameraType == CameraRenderType.Base; m_FirstTimeCameraDepthTargetIsBound = true; // 当前相机的颜色和深度缓冲区 m_CameraColorTarget = BuiltinRenderTextureType.CameraTarget; m_CameraDepthTarget = BuiltinRenderTextureType.CameraTarget;&#125; 2. 执行剔除前调用函数，ScriptableRenderer.OnPreCullRenderPasses 123456789101112internal void OnPreCullRenderPasses(in CameraData cameraData)&#123; // 调用到渲染特性中的OnCameraPreCull for (int i = 0; i &lt; rendererFeatures.Count; ++i) &#123; if (!rendererFeatures[i].isActive) &#123; continue; &#125; rendererFeatures[i].OnCameraPreCull(this, in cameraData); &#125;&#125; 3. 构建相机剔除参数ScriptableRenderer.SetupCullingParameters，每个子类具体实现 4. 构建各种渲染Pass, ScriptableRenderer.Setup，每个子类具体实现 5. 执行渲染，ScriptableRenderer.Execute 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150public void Execute(ScriptableRenderContext context, ref RenderingData renderingData)&#123; // 使用场景覆盖时禁用Gizmos。 Gizmos破坏了一些效果，例如Overdraw调试。 bool drawGizmos = DebugDisplaySettings.Instance.RenderingSettings.debugSceneOverrideMode == DebugSceneOverrideMode.None; m_IsPipelineExecuting = true; ref CameraData cameraData = ref renderingData.cameraData; Camera camera = cameraData.camera; CommandBuffer cmd = CommandBufferPool.Get(); CommandBuffer cmdScope = renderingData.cameraData.xr.enabled ? null : cmd; using (new ProfilingScope(cmdScope, profilingExecute)) &#123; // 开始渲染，执行所有Pass的OnCameraSetup函数 InternalStartRendering(context, ref renderingData); // 缓存time信息#if UNITY_EDITOR float time = Application.isPlaying ? Time.time : Time.realtimeSinceStartup;#else float time = Time.time;#endif float deltaTime = Time.deltaTime; float smoothDeltaTime = Time.smoothDeltaTime; // 初始化渲染状态，禁用一些Shader的宏 ClearRenderingState(cmd); // 设置Time信息，在Shader中访问的_Time，_SinTime和_CosTime等 SetShaderTimeValues(cmd, time, deltaTime, smoothDeltaTime); context.ExecuteCommandBuffer(cmd); cmd.Clear(); // 根据RenderPassEvent排序Pass using (new ProfilingScope(null, Profiling.sortRenderPasses)) &#123; SortStable(m_ActiveRenderPassQueue); &#125; SetupNativeRenderPassFrameData(cameraData, useRenderPassEnabled); // 将渲染Pass根据RenderPassEvent分为4个大类（RenderPassBlock.BeforeRendering, RenderPassBlock.MainRenderingOpaque, RenderPassBlock.MainRenderingTransparent和RenderPassBlock.AfterRendering） using var renderBlocks = new RenderBlocks(m_ActiveRenderPassQueue); // 构建灯光数据 using (new ProfilingScope(null, Profiling.setupLights)) &#123; SetupLights(context, ref renderingData); &#125; // 获取渲染前的渲染Pass, 被用于为渲染器准备输入纹理（比如：shadowmaps） if (renderBlocks.GetLength(RenderPassBlock.BeforeRendering) &gt; 0) &#123; // 执行渲染前的所有Pass using var profScope = new ProfilingScope(null, Profiling.RenderBlock.beforeRendering); ExecuteBlock(RenderPassBlock.BeforeRendering, in renderBlocks, context, ref renderingData); &#125; // 开始设置相机属性和相机相关的Shader变量 using (new ProfilingScope(null, Profiling.setupCamera)) &#123; if (cameraData.renderType == CameraRenderType.Base) &#123; // 设置相机属性 context.SetupCameraProperties(camera); // 设置相机相关的的Shader变量（_WorldSpaceCameraPos， _ScreenParams， _ProjectionParams， _ZBufferParams和_ScaledScreenParams等） SetPerCameraShaderVariables(cmd, ref cameraData); &#125; else &#123; // 设置相机相关的的Shader变量（_WorldSpaceCameraPos， _ScreenParams， _ProjectionParams， _ZBufferParams和_ScaledScreenParams等） SetPerCameraShaderVariables(cmd, ref cameraData); // 设置相机的裁剪平面Shader变量（unity_CameraWorldClipPlanes） SetPerCameraClippingPlaneProperties(cmd, in cameraData); // 设置相机的布告属性（unity_BillboardNormal， unity_BillboardTangent和unity_BillboardCameraParams） SetPerCameraBillboardProperties(cmd, ref cameraData); &#125; // 重新设置设置Shader的时间相关的变量，在SetupCameraProperties被修改了 SetShaderTimeValues(cmd, time, deltaTime, smoothDeltaTime);#if VISUAL_EFFECT_GRAPH_0_0_1_OR_NEWER // GPU粒子系统，设置所有全局参数 VFX.VFXManager.ProcessCameraCommand(camera, cmd);#endif &#125; context.ExecuteCommandBuffer(cmd); cmd.Clear(); BeginXRRendering(cmd, context, ref renderingData.cameraData); // 执行主要的不透明和半透明对象的渲染 // 不透明块的渲染 if (renderBlocks.GetLength(RenderPassBlock.MainRenderingOpaque) &gt; 0) &#123; // 执行不透明块中的所有Pass using var profScope = new ProfilingScope(null, Profiling.RenderBlock.mainRenderingOpaque); ExecuteBlock(RenderPassBlock.MainRenderingOpaque, in renderBlocks, context, ref renderingData); &#125; // 透明块 if (renderBlocks.GetLength(RenderPassBlock.MainRenderingTransparent) &gt; 0) &#123; // 执行透明块中的所有Pass using var profScope = new ProfilingScope(null, Profiling.RenderBlock.mainRenderingTransparent); ExecuteBlock(RenderPassBlock.MainRenderingTransparent, in renderBlocks, context, ref renderingData); &#125;#if ENABLE_VR &amp;&amp; ENABLE_XR_MODULE if (cameraData.xr.enabled) cameraData.xr.canMarkLateLatch = false;#endif // 绘制 Gizmos if (drawGizmos) &#123; DrawGizmos(context, camera, GizmoSubset.PreImageEffects); &#125; // 在绘制完不透明和透明对象块中的Pass后，执行AfterRendering块，主要包括后处理，视频捕获等。 if (renderBlocks.GetLength(RenderPassBlock.AfterRendering) &gt; 0) &#123; using var profScope = new ProfilingScope(null, Profiling.RenderBlock.afterRendering); ExecuteBlock(RenderPassBlock.AfterRendering, in renderBlocks, context, ref renderingData); &#125; EndXRRendering(cmd, context, ref renderingData.cameraData); DrawWireOverlay(context, camera); if (drawGizmos) &#123; DrawGizmos(context, camera, GizmoSubset.PostImageEffects); &#125; // 结束渲染，执行所有Pass的FrameCleanup函数，如果是相机栈的最后一个，则还回调用所有Pass的OnFinishCameraStackRendering InternalFinishRendering(context, cameraData.resolveFinalTarget); for (int i = 0; i &lt; m_ActiveRenderPassQueue.Count; ++i) &#123; m_ActiveRenderPassQueue[i].m_ColorAttachmentIndices.Dispose(); m_ActiveRenderPassQueue[i].m_InputAttachmentIndices.Dispose(); &#125; &#125; context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd);&#125; UniversalRenderer UniversalRenderer是ScriptableRenderer的子类，URP的标准渲染器。 它主要在复写的Setup函数中添加了各种的渲染Pass到m_ActiveRenderPassQueue中，核心的代码在基类ScriptableRenderer中。 核心成员： ForwardLights m_ForwardLights; // 前向渲染光照 DeferredLights m_DeferredLights; // 延迟渲染光照 核心Pass: DepthOnlyPass 深度Pass MainLightShadowCasterPass 主灯光阴影投射Pass DrawObjectsPass 绘制对象Pass DrawSkyboxPass 绘制天空盒Pass URP中的Pass Pass是执行一个具体的渲染任务。 ScriptableRenderPass ScriptableRenderPass是渲染Pass的基础类，提供一些Pass都应具备的属性，一些公共的方法和Pass的生命周期函数，具体如下： Pass的共有属性： renderPassEvent 渲染Pass事件（BeforeRendering， BeforeRenderingGbuffer， BeforeRenderingOpaques， AfterRenderingOpaques， BeforeRenderingTransparents， AfterRenderingTransparents， BeforeRenderingPostProcessing和AfterRendering等 ）。 colorAttachment 颜色缓冲区纹理。 depthAttachment 深度缓冲区纹理。 input Pass的输入需求（ScriptableRenderPassInput.Depth,Normal,Color和Motion）。 clearFlag 需要清除的是：Color，Depth和Stencil)。 clearColor 清除的值是什么。 renderTargetWidth/renderTargetHeight 渲染目标的宽度和高度。 depthOnly 是仅深度？ isLastPass 是最后一个Pass。 renderPassQueueIndex 这帧中Pass的索引，就是在ScriptableRenderer.m_ActiveRenderPassQueue排序后的索引 公共的方法： ConfigureInput 配置此Pass的输入需求。 ConfigureInputAttachments 配置输入附件纹理 ConfigureTarget 配置渲染目标（colorAttachment和depthAttachment） ConfigureClear 配置清除标志 Blit 拷贝纹理 CreateDrawingSettings 创建绘制设置（使用的什么Tag的Shader, 排序规则是什么等。） 生命周期函数: OnCameraSetup 渲染一个相机前调用，所有Pass执行之前调用。 Configure 执行一个Pass前调用。 Execute 核心执行函数。 OnCameraCleanup 渲染一个相机完成后调用。 OnFinishCameraStackRendering 整个相机栈渲染完成后调用。 核心Pass DepthOnlyPass 深度Pass 核心函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// Setup函数是由Render中调用过来的，它并不在ScriptableRenderPass基类中定义，它在OnCameraSetup之前调用，并不是所有的Pass都有此函数，如果需要自定义Render可以在Render的Setup中去调用Pass的Setup函数。public void Setup( RenderTextureDescriptor baseDescriptor, RenderTargetHandle depthAttachmentHandle)&#123; this.depthAttachmentHandle = depthAttachmentHandle; baseDescriptor.colorFormat = RenderTextureFormat.Depth; baseDescriptor.depthBufferBits = UniversalRenderer.k_DepthStencilBufferBits; baseDescriptor.msaaSamples = 1; descriptor = baseDescriptor; this.allocateDepth = true; this.shaderTagId = k_ShaderTagId;&#125;// 所有Pass执行之前调用，相机渲染相关资源和数据(renderingData)public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData)&#123; if (this.allocateDepth) cmd.GetTemporaryRT(depthAttachmentHandle.id, descriptor, FilterMode.Point); var desc = renderingData.cameraData.cameraTargetDescriptor; // 配置深度Pass的渲染目标 if (renderingData.cameraData.renderer.useDepthPriming &amp;&amp; (renderingData.cameraData.renderType == CameraRenderType.Base || renderingData.cameraData.clearDepth)) &#123; ConfigureTarget(renderingData.cameraData.renderer.cameraDepthTarget, descriptor.depthStencilFormat, desc.width, desc.height, 1, true); &#125; else &#123; ConfigureTarget(new RenderTargetIdentifier(depthAttachmentHandle.Identifier(), 0, CubemapFace.Unknown, -1), descriptor.depthStencilFormat, desc.width, desc.height, 1, true); &#125; // 配置清除 ConfigureClear(ClearFlag.Depth, Color.black);&#125;// 执行此Passpublic override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)&#123; // 获取一个CommandBuff用于执行渲染 CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.DepthPrepass))) &#123; context.ExecuteCommandBuffer(cmd); cmd.Clear(); // 筛选需要绘制的对象，使用Shader中的Tag为\"DepthOnly\"的Pass进行渲染 var sortFlags = renderingData.cameraData.defaultOpaqueSortFlags; var drawSettings = CreateDrawingSettings(this.shaderTagId, ref renderingData, sortFlags); drawSettings.perObjectData = PerObjectData.None; // 绘制对象 context.DrawRenderers(renderingData.cullResults, ref drawSettings, ref m_FilteringSettings); &#125; context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd);&#125;// 相机渲染完成后public override void OnCameraCleanup(CommandBuffer cmd)&#123; if (cmd == null) throw new ArgumentNullException(\"cmd\"); // 释放分配的资源 if (depthAttachmentHandle != RenderTargetHandle.CameraTarget) &#123; if (this.allocateDepth) cmd.ReleaseTemporaryRT(depthAttachmentHandle.id); depthAttachmentHandle = RenderTargetHandle.CameraTarget; &#125;&#125; MainLightShadowCasterPass 主灯光阴影投射Pass 核心函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219// 构造函数public MainLightShadowCasterPass(RenderPassEvent evt)&#123; base.profilingSampler = new ProfilingSampler(nameof(MainLightShadowCasterPass)); renderPassEvent = evt; // 每个级联阴影矩阵，切片数据和距离 m_MainLightShadowMatrices = new Matrix4x4[k_MaxCascades + 1]; m_CascadeSlices = new ShadowSliceData[k_MaxCascades]; m_CascadeSplitDistances = new Vector4[k_MaxCascades]; // 计算主灯光阴影需要Shader属性 MainLightShadowConstantBuffer._WorldToShadow = Shader.PropertyToID(\"_MainLightWorldToShadow\"); MainLightShadowConstantBuffer._ShadowParams = Shader.PropertyToID(\"_MainLightShadowParams\"); MainLightShadowConstantBuffer._CascadeShadowSplitSpheres0 = Shader.PropertyToID(\"_CascadeShadowSplitSpheres0\"); MainLightShadowConstantBuffer._CascadeShadowSplitSpheres1 = Shader.PropertyToID(\"_CascadeShadowSplitSpheres1\"); MainLightShadowConstantBuffer._CascadeShadowSplitSpheres2 = Shader.PropertyToID(\"_CascadeShadowSplitSpheres2\"); MainLightShadowConstantBuffer._CascadeShadowSplitSpheres3 = Shader.PropertyToID(\"_CascadeShadowSplitSpheres3\"); MainLightShadowConstantBuffer._CascadeShadowSplitSphereRadii = Shader.PropertyToID(\"_CascadeShadowSplitSphereRadii\"); MainLightShadowConstantBuffer._ShadowOffset0 = Shader.PropertyToID(\"_MainLightShadowOffset0\"); MainLightShadowConstantBuffer._ShadowOffset1 = Shader.PropertyToID(\"_MainLightShadowOffset1\"); MainLightShadowConstantBuffer._ShadowOffset2 = Shader.PropertyToID(\"_MainLightShadowOffset2\"); MainLightShadowConstantBuffer._ShadowOffset3 = Shader.PropertyToID(\"_MainLightShadowOffset3\"); MainLightShadowConstantBuffer._ShadowmapSize = Shader.PropertyToID(\"_MainLightShadowmapSize\"); m_MainLightShadowmap.Init(\"_MainLightShadowmapTexture\");&#125;// 设置渲染数据public bool Setup(ref RenderingData renderingData)&#123; using var profScope = new ProfilingScope(null, m_ProfilingSetupSampler); // 不支持的时候，创建一个空（1x1）的阴影贴图 if (!renderingData.shadowData.supportsMainLightShadows) return SetupForEmptyRendering(ref renderingData); // 清除阴影贴图和级联数据 Clear(); // 检查是否有主灯光 int shadowLightIndex = renderingData.lightData.mainLightIndex; if (shadowLightIndex == -1) return SetupForEmptyRendering(ref renderingData); // 主灯光是否支持阴影 VisibleLight shadowLight = renderingData.lightData.visibleLights[shadowLightIndex]; Light light = shadowLight.light; if (light.shadows == LightShadows.None) return SetupForEmptyRendering(ref renderingData); // 只有方向光能作为主灯光 if (shadowLight.lightType != LightType.Directional) &#123; Debug.LogWarning(\"Only directional lights are supported as main light.\"); &#125; // 获取阴影的投射边界 Bounds bounds; if (!renderingData.cullResults.GetShadowCasterBounds(shadowLightIndex, out bounds)) return SetupForEmptyRendering(ref renderingData); // 获取级联数量 m_ShadowCasterCascadesCount = renderingData.shadowData.mainLightShadowCascadesCount; // 根据总分辨率和指定级联数量，计算出单个级联的分辨率 int shadowResolution = ShadowUtils.GetMaxTileResolutionInAtlas(renderingData.shadowData.mainLightShadowmapWidth, renderingData.shadowData.mainLightShadowmapHeight, m_ShadowCasterCascadesCount); renderTargetWidth = renderingData.shadowData.mainLightShadowmapWidth; renderTargetHeight = (m_ShadowCasterCascadesCount == 2) ? renderingData.shadowData.mainLightShadowmapHeight &gt;&gt; 1 : renderingData.shadowData.mainLightShadowmapHeight; // 计算每个级联的矩阵，切片数据和距离 for (int cascadeIndex = 0; cascadeIndex &lt; m_ShadowCasterCascadesCount; ++cascadeIndex) &#123; bool success = ShadowUtils.ExtractDirectionalLightMatrix(ref renderingData.cullResults, ref renderingData.shadowData, shadowLightIndex, cascadeIndex, renderTargetWidth, renderTargetHeight, shadowResolution, light.shadowNearPlane, out m_CascadeSplitDistances[cascadeIndex], out m_CascadeSlices[cascadeIndex]); if (!success) return SetupForEmptyRendering(ref renderingData); &#125; // 获取阴影RT m_MainLightShadowmapTexture = ShadowUtils.GetTemporaryShadowTexture(renderTargetWidth, renderTargetHeight, k_ShadowmapBufferBits); m_MaxShadowDistanceSq = renderingData.cameraData.maxShadowDistance * renderingData.cameraData.maxShadowDistance; m_CascadeBorder = renderingData.shadowData.mainLightShadowCascadeBorder; m_CreateEmptyShadowmap = false; useNativeRenderPass = true; return true;&#125;// 将阴影贴图设置为当前的颜色缓冲区，并清除它public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor)&#123; ConfigureTarget(new RenderTargetIdentifier(m_MainLightShadowmapTexture), m_MainLightShadowmapTexture.depthStencilFormat, renderTargetWidth, renderTargetHeight, 1, true); ConfigureClear(ClearFlag.All, Color.black);&#125;// Pass的执行函数public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)&#123; if (m_CreateEmptyShadowmap) &#123; SetEmptyMainLightCascadeShadowmap(ref context); return; &#125; RenderMainLightCascadeShadowmap(ref context, ref renderingData.cullResults, ref renderingData.lightData, ref renderingData.shadowData);&#125;// 渲染主灯光的阴影贴图void RenderMainLightCascadeShadowmap(ref ScriptableRenderContext context, ref CullingResults cullResults, ref LightData lightData, ref ShadowData shadowData)&#123; int shadowLightIndex = lightData.mainLightIndex; if (shadowLightIndex == -1) return; // 获取主灯光 VisibleLight shadowLight = lightData.visibleLights[shadowLightIndex]; CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, ProfilingSampler.Get(URPProfileId.MainLightShadow))) &#123; // 构建一个绘制阴影的设置 var settings = new ShadowDrawingSettings(cullResults, shadowLightIndex); settings.useRenderingLayerMaskTest = UniversalRenderPipeline.asset.supportsLightLayers; // 设置每个级联的参数 for (int cascadeIndex = 0; cascadeIndex &lt; m_ShadowCasterCascadesCount; ++cascadeIndex) &#123; settings.splitData = m_CascadeSlices[cascadeIndex].splitData; Vector4 shadowBias = ShadowUtils.GetShadowBias(ref shadowLight, shadowLightIndex, ref shadowData, m_CascadeSlices[cascadeIndex].projectionMatrix, m_CascadeSlices[cascadeIndex].resolution); ShadowUtils.SetupShadowCasterConstantBuffer(cmd, ref shadowLight, shadowBias); CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.CastingPunctualLightShadow, false); ShadowUtils.RenderShadowSlice(cmd, ref context, ref m_CascadeSlices[cascadeIndex], ref settings, m_CascadeSlices[cascadeIndex].projectionMatrix, m_CascadeSlices[cascadeIndex].viewMatrix); &#125; // 启用主灯光阴影相关的宏 shadowData.isKeywordSoftShadowsEnabled = shadowLight.light.shadows == LightShadows.Soft &amp;&amp; shadowData.supportsSoftShadows; CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.MainLightShadows, shadowData.mainLightShadowCascadesCount == 1); CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.MainLightShadowCascades, shadowData.mainLightShadowCascadesCount &gt; 1); CoreUtils.SetKeyword(cmd, ShaderKeywordStrings.SoftShadows, shadowData.isKeywordSoftShadowsEnabled); // 设置Shader常量 SetupMainLightShadowReceiverConstants(cmd, shadowLight, shadowData.supportsSoftShadows); &#125; context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd);&#125;void SetupMainLightShadowReceiverConstants(CommandBuffer cmd, VisibleLight shadowLight, bool supportsSoftShadows)&#123; Light light = shadowLight.light; bool softShadows = shadowLight.light.shadows == LightShadows.Soft &amp;&amp; supportsSoftShadows; int cascadeCount = m_ShadowCasterCascadesCount; for (int i = 0; i &lt; cascadeCount; ++i) m_MainLightShadowMatrices[i] = m_CascadeSlices[i].shadowTransform; // 设置级联阴影矩阵 Matrix4x4 noOpShadowMatrix = Matrix4x4.zero; noOpShadowMatrix.m22 = (SystemInfo.usesReversedZBuffer) ? 1.0f : 0.0f; for (int i = cascadeCount; i &lt;= k_MaxCascades; ++i) m_MainLightShadowMatrices[i] = noOpShadowMatrix; float invShadowAtlasWidth = 1.0f / renderTargetWidth; float invShadowAtlasHeight = 1.0f / renderTargetHeight; float invHalfShadowAtlasWidth = 0.5f * invShadowAtlasWidth; float invHalfShadowAtlasHeight = 0.5f * invShadowAtlasHeight; float softShadowsProp = softShadows ? 1.0f : 0.0f; // 根据距离计算缩放和Bias偏移 ShadowUtils.GetScaleAndBiasForLinearDistanceFade(m_MaxShadowDistanceSq, m_CascadeBorder, out float shadowFadeScale, out float shadowFadeBias); // 设置Shader变量（贴图纹理，世界到阴影的矩阵，阴影参数） cmd.SetGlobalTexture(m_MainLightShadowmap.id, m_MainLightShadowmapTexture); cmd.SetGlobalMatrixArray(MainLightShadowConstantBuffer._WorldToShadow, m_MainLightShadowMatrices); cmd.SetGlobalVector(MainLightShadowConstantBuffer._ShadowParams, new Vector4(light.shadowStrength, softShadowsProp, shadowFadeScale, shadowFadeBias)); // 设置级联参数 if (m_ShadowCasterCascadesCount &gt; 1) &#123; cmd.SetGlobalVector(MainLightShadowConstantBuffer._CascadeShadowSplitSpheres0, m_CascadeSplitDistances[0]); cmd.SetGlobalVector(MainLightShadowConstantBuffer._CascadeShadowSplitSpheres1, m_CascadeSplitDistances[1]); cmd.SetGlobalVector(MainLightShadowConstantBuffer._CascadeShadowSplitSpheres2, m_CascadeSplitDistances[2]); cmd.SetGlobalVector(MainLightShadowConstantBuffer._CascadeShadowSplitSpheres3, m_CascadeSplitDistances[3]); cmd.SetGlobalVector(MainLightShadowConstantBuffer._CascadeShadowSplitSphereRadii, new Vector4( m_CascadeSplitDistances[0].w * m_CascadeSplitDistances[0].w, m_CascadeSplitDistances[1].w * m_CascadeSplitDistances[1].w, m_CascadeSplitDistances[2].w * m_CascadeSplitDistances[2].w, m_CascadeSplitDistances[3].w * m_CascadeSplitDistances[3].w)); &#125; // 软阴影参数 if (supportsSoftShadows) &#123; cmd.SetGlobalVector(MainLightShadowConstantBuffer._ShadowOffset0, new Vector4(-invHalfShadowAtlasWidth, -invHalfShadowAtlasHeight, 0.0f, 0.0f)); cmd.SetGlobalVector(MainLightShadowConstantBuffer._ShadowOffset1, new Vector4(invHalfShadowAtlasWidth, -invHalfShadowAtlasHeight, 0.0f, 0.0f)); cmd.SetGlobalVector(MainLightShadowConstantBuffer._ShadowOffset2, new Vector4(-invHalfShadowAtlasWidth, invHalfShadowAtlasHeight, 0.0f, 0.0f)); cmd.SetGlobalVector(MainLightShadowConstantBuffer._ShadowOffset3, new Vector4(invHalfShadowAtlasWidth, invHalfShadowAtlasHeight, 0.0f, 0.0f)); cmd.SetGlobalVector(MainLightShadowConstantBuffer._ShadowmapSize, new Vector4(invShadowAtlasWidth, invShadowAtlasHeight, renderTargetWidth, renderTargetHeight)); &#125;&#125; DrawObjectsPass 绘制对象Pass 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// 构造函数public DrawObjectsPass(string profilerTag, ShaderTagId[] shaderTagIds, bool opaque, RenderPassEvent evt, RenderQueueRange renderQueueRange, LayerMask layerMask, StencilState stencilState, int stencilReference)&#123; base.profilingSampler = new ProfilingSampler(nameof(DrawObjectsPass)); m_ProfilerTag = profilerTag; m_ProfilingSampler = new ProfilingSampler(profilerTag); foreach (ShaderTagId sid in shaderTagIds) m_ShaderTagIdList.Add(sid); renderPassEvent = evt; m_FilteringSettings = new FilteringSettings(renderQueueRange, layerMask); m_RenderStateBlock = new RenderStateBlock(RenderStateMask.Nothing); m_IsOpaque = opaque; if (stencilState.enabled) &#123; m_RenderStateBlock.stencilReference = stencilReference; m_RenderStateBlock.mask = RenderStateMask.Stencil; m_RenderStateBlock.stencilState = stencilState; &#125;&#125;public override void OnCameraSetup(CommandBuffer cmd, ref RenderingData renderingData)&#123; // 设置深度 if (renderingData.cameraData.renderer.useDepthPriming &amp;&amp; m_IsOpaque &amp;&amp; (renderingData.cameraData.renderType == CameraRenderType.Base || renderingData.cameraData.clearDepth)) &#123; m_RenderStateBlock.depthState = new DepthState(false, CompareFunction.Equal); m_RenderStateBlock.mask |= RenderStateMask.Depth; &#125; else if (m_RenderStateBlock.depthState.compareFunction == CompareFunction.Equal) &#123; m_RenderStateBlock.depthState = new DepthState(true, CompareFunction.LessEqual); m_RenderStateBlock.mask |= RenderStateMask.Depth; &#125;&#125;public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)&#123; CommandBuffer cmd = CommandBufferPool.Get(); using (new ProfilingScope(cmd, m_ProfilingSampler)) &#123; // 包含各种设置的全局渲染通道数据。 // x,y,z are currently unused // w 用于了解对象是不透明的 (1) 还是 alpha 混合的 (0) Vector4 drawObjectPassData = new Vector4(0.0f, 0.0f, 0.0f, (m_IsOpaque) ? 1.0f : 0.0f); cmd.SetGlobalVector(s_DrawObjectPassDataPropID, drawObjectPassData); // RT反转符号 float flipSign = (renderingData.cameraData.IsCameraProjectionMatrixFlipped()) ? -1.0f : 1.0f; Vector4 scaleBias = (flipSign &lt; 0.0f) ? new Vector4(flipSign, 1.0f, -1.0f, 1.0f) : new Vector4(flipSign, 0.0f, 1.0f, 1.0f); cmd.SetGlobalVector(ShaderPropertyId.scaleBiasRt, scaleBias); context.ExecuteCommandBuffer(cmd); cmd.Clear(); // 获取对象的排序规则 Camera camera = renderingData.cameraData.camera; var sortFlags = (m_IsOpaque) ? renderingData.cameraData.defaultOpaqueSortFlags : SortingCriteria.CommonTransparent; if (renderingData.cameraData.renderer.useDepthPriming &amp;&amp; m_IsOpaque &amp;&amp; (renderingData.cameraData.renderType == CameraRenderType.Base || renderingData.cameraData.clearDepth)) sortFlags = SortingCriteria.SortingLayer | SortingCriteria.RenderQueue | SortingCriteria.OptimizeStateChanges | SortingCriteria.CanvasOrder; var filterSettings = m_FilteringSettings;#if UNITY_EDITOR // 预览相机的时候强制渲染所有 if (renderingData.cameraData.isPreviewCamera) &#123; filterSettings.layerMask = -1; &#125;#endif // 调用基类的CreateDrawingSettings函数创建绘制设置对象（用于描述哪些对象使用哪些Shader的Pass去绘制，以及渲染对象的排序规则） DrawingSettings drawSettings = CreateDrawingSettings(m_ShaderTagIdList, ref renderingData, sortFlags); // 调试绘制 var activeDebugHandler = GetActiveDebugHandler(renderingData); if (activeDebugHandler != null) &#123; activeDebugHandler.DrawWithDebugRenderState(context, cmd, ref renderingData, ref drawSettings, ref filterSettings, ref m_RenderStateBlock, (ScriptableRenderContext ctx, ref RenderingData data, ref DrawingSettings ds, ref FilteringSettings fs, ref RenderStateBlock rsb) =&gt; &#123; ctx.DrawRenderers(data.cullResults, ref ds, ref fs, ref rsb); &#125;); &#125; else &#123; // 告诉Unity底层绘制对象 context.DrawRenderers(renderingData.cullResults, ref drawSettings, ref filterSettings, ref m_RenderStateBlock); // 没有匹配任意着色器Pass则使用错误Shader进行渲染 RenderingUtils.RenderObjectsWithError(context, ref renderingData.cullResults, camera, filterSettings, SortingCriteria.None); &#125; &#125; context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd);&#125; DrawSkyboxPass 绘制天空盒Pass 123456789101112131415161718 public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)&#123; CameraData cameraData = renderingData.cameraData; Camera camera = cameraData.camera; var activeDebugHandler = GetActiveDebugHandler(renderingData); if (activeDebugHandler != null) &#123; if (activeDebugHandler.IsScreenClearNeeded) &#123; return; &#125; &#125; // 删除VR和XR的代码 // 告诉Unity绘制天空盒子 context.DrawSkybox(camera);&#125;","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"Rendering","slug":"Rendering","permalink":"http://yoursite.com/tags/Rendering/"}]},{"title":"Unity ShaderLab","slug":"Unity/Graphics/ShaderLab","date":"2022-06-28T11:57:02.000Z","updated":"2025-04-26T11:06:24.129Z","comments":true,"path":"2022/06/28/Unity/Graphics/ShaderLab/","link":"","permalink":"http://yoursite.com/2022/06/28/Unity/Graphics/ShaderLab/","excerpt":"在开始学习ShaderLab之前，我们先简单的了解一下着色器语言。着色器语言主要分为离线渲染时使用的着色器语言和实时渲染中使用的着色器语言。 离线作色器语言 RenderMan 着色语言（RSL） Houdini VEX 着色语言 Gelato 着色语言 开放着色器编程语言（OSL） 实时着色器语言 ARB汇编语言（英语：ARB assembly language） OpenGL 着色语言（GLSL） Cg语言 DirectX 着色器汇编语言 DirectX 高级着色器语言（HLSL） Metal 着色语言 ShaderLab与以上的作色语言有什么区别呢？ShaderLab是Unity定义的一个Shader描述性语言，不能直接在对应图形平台运行，并且是夸平台。为了更好的理解它们的区别，先来看一下OpenGL是如何通过GLSL着色语言对一个三角形进行着色的。","text":"在开始学习ShaderLab之前，我们先简单的了解一下着色器语言。着色器语言主要分为离线渲染时使用的着色器语言和实时渲染中使用的着色器语言。 离线作色器语言 RenderMan 着色语言（RSL） Houdini VEX 着色语言 Gelato 着色语言 开放着色器编程语言（OSL） 实时着色器语言 ARB汇编语言（英语：ARB assembly language） OpenGL 着色语言（GLSL） Cg语言 DirectX 着色器汇编语言 DirectX 高级着色器语言（HLSL） Metal 着色语言 ShaderLab与以上的作色语言有什么区别呢？ShaderLab是Unity定义的一个Shader描述性语言，不能直接在对应图形平台运行，并且是夸平台。为了更好的理解它们的区别，先来看一下OpenGL是如何通过GLSL着色语言对一个三角形进行着色的。 OpenGL如何渲染一个三角形 首先创建一个窗口用于显示渲染对象 123456789101112131415161718192021222324252627282930313233343536373839404142int main()&#123; // 使用glfw初始化和配置窗口 glfwInit(); glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); // 创建窗口 GLFWwindow* window = glfwCreateWindow(SCR_WIDTH, SCR_HEIGHT, \"LearnOpenGL\", NULL, NULL); if (window == NULL) &#123; std::cout &lt;&lt; \"Failed to create GLFW window\" &lt;&lt; std::endl; glfwTerminate(); return -1; &#125; // 构建窗口上限文信息 glfwMakeContextCurrent(window); glfwSetFramebufferSizeCallback(window, framebuffer_size_callback); // 使用glad库加载所有的OpenGL函数指正 if (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress)) &#123; std::cout &lt;&lt; \"Failed to initialize GLAD\" &lt;&lt; std::endl; return -1; &#125;&#125;// 处理所欲输入信息// ---------------------------------------------------------------------------------------------------------void processInput(GLFWwindow *window)&#123; if (glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS) glfwSetWindowShouldClose(window, true);&#125;// 窗口大小改变时执行的回调函数// ---------------------------------------------------------------------------------------------void framebuffer_size_callback(GLFWwindow* window, int width, int height)&#123; glViewport(0, 0, width, height);&#125; 构建用于着色的着色器程序 顶点着色器代码: 1234567#version 330 corelayout (location = 0) in vec3 aPos;void main()&#123; gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0);&#125; 片元着色器代码: 1234567#version 330 coreout vec4 FragColor;void main()&#123; FragColor = vec4(1.0f, 0.5f, 0.2f, 1.0f);&#125; 编译链接着色器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354int main()&#123; // 构建和编译着色器程序 // 顶点着色器 // 创建一个顶点着色器对象 unsigned int vertexShader = glCreateShader(GL_VERTEX_SHADER); // 将顶点着色器的类容给着色器对象 glShaderSource(vertexShader, 1, &amp;vertexShaderSource, NULL); // vertexShaderSource就是上面的“顶点着色器代码”中的类容，为了好看单独放上面 // 编译着色器对象 glCompileShader(vertexShader); // 检查编译错误 int success; char infoLog[512]; glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &amp;success); if (!success) &#123; glGetShaderInfoLog(vertexShader, 512, NULL, infoLog); std::cout &lt;&lt; \"ERROR::SHADER::VERTEX::COMPILATION_FAILED\\n\" &lt;&lt; infoLog &lt;&lt; std::endl; &#125; // 片元着色器 // 创建一个片元着色器对象 unsigned int fragmentShader = glCreateShader(GL_FRAGMENT_SHADER); // 将片元着色器的类容给着色器对象 glShaderSource(fragmentShader, 1, &amp;fragmentShaderSource, NULL); //fragmentShaderSource 就是上面的“片元着色器代码”中的类容，为了好看单独放上面 // 编译片元着色器对象 glCompileShader(fragmentShader); // 检查编译错误 glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &amp;success); if (!success) &#123; glGetShaderInfoLog(fragmentShader, 512, NULL, infoLog); std::cout &lt;&lt; \"ERROR::SHADER::FRAGMENT::COMPILATION_FAILED\\n\" &lt;&lt; infoLog &lt;&lt; std::endl; &#125; // 链接着色器程序 // 创建一个着色器程序对象 unsigned int shaderProgram = glCreateProgram(); // 将编译后的顶点着色器对象和片元着色器对象给着色器程序进行编译 glAttachShader(shaderProgram, vertexShader); glAttachShader(shaderProgram, fragmentShader); // 开始链接着色器程序，当链接着色器至一个程序的时候，它会把每个着色器的输出链接到下个着色器的输入。当输出和输入不匹配的时候，你会得到一个连接错误。 glLinkProgram(shaderProgram); // 检查链接错误 glGetProgramiv(shaderProgram, GL_LINK_STATUS, &amp;success); if (!success) &#123; glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog); std::cout &lt;&lt; \"ERROR::SHADER::PROGRAM::LINKING_FAILED\\n\" &lt;&lt; infoLog &lt;&lt; std::endl; &#125; // 链接完后，将顶点和片元着色器对象删除 glDeleteShader(vertexShader); glDeleteShader(fragmentShader);&#125; 创建三角形顶点数据 12345678910111213141516171819202122232425262728293031int main()&#123; // 设置顶点数据并配置属性 float vertices[] = &#123; -0.5f, -0.5f, 0.0f, // left 0.5f, -0.5f, 0.0f, // right 0.0f, 0.5f, 0.0f // top &#125;; unsigned int VBO, VAO; glGenVertexArrays(1, &amp;VAO); // 向GPU申请一个顶点数组对象 glGenBuffers(1, &amp;VBO); // 向GPU申请一个顶点缓冲对象 // 绑定顶点数组对象 glBindVertexArray(VAO); // 绑定顶点缓冲对象 glBindBuffer(GL_ARRAY_BUFFER, VBO); // 将顶点数据复制到顶点缓冲对象中 glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); // 告知GPU顶点数据的格式，启用顶点属性，顶点属性默认是禁用的 glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0); // 解除绑定VBO对象 glBindBuffer(GL_ARRAY_BUFFER, 0); // 解除绑定VAO对象 glBindVertexArray(0); &#125; 使用三角形顶点数据和着色器程序绘制三角形 1234567891011121314151617181920212223int main()&#123; // 渲染循环 while (!glfwWindowShouldClose(window)) &#123; // 处理输入 processInput(window); // 渲染 glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // 绘制三角形 glUseProgram(shaderProgram); glBindVertexArray(VAO); // 由于我们只有一个 VAO，因此无需每次都绑定它，但我们会这样做以使事情更有条理 glDrawArrays(GL_TRIANGLES, 0, 3); // glBindVertexArray(0); // 不需要每次都解绑 // 交换前后缓冲区并轮询IO事件 glfwSwapBuffers(window); glfwPollEvents(); &#125;&#125; 至此一个三角形就绘制出来了，为了逻辑更加清晰以上的代码在排版上做了调整，不能用于直接运行，最明显的就是每个阶段都有main函数，这主要时为了好看，完整的代码参见：LearnOpenGL网站。渲染图如下： HelloTriangle ShaderLab ShaderLab中相关术语 Shader这个术语通常是令人困惑，人们通常使用shader这个词代表不同的意思。在本文中，Shader这个术语有如下： Shader或Shader program 指的是在GPU上运行的一段程序代码。 Shader object 指的是一个Shader类的实例。一个Shader对象包含了Shader程序和其他信息。 ShaderLab 指的是Unity专用的写Shader的着色器语言。 Shader Graph 指的是一个不需要写代码就能创建Shader的工具 Shader asset 指的是扩展名为.shader的着色器对象文件。 Shader Graph asset 指的是由Shader Graph创建的着色器对象。 ShaderLab做了哪些事 定义了Shader object(着色器对象)的结构。 暴露了属性给材质球。 使用命令去设置了GPU的渲染状态。 使用HLSL代码块进行着色代码编写。 可以指定子Shader和Pass的包依赖。 使用Fallback定义了兜底行为。 Shader object(着色器对象) Shader object(着色器对象)就是C#的Shader类的一个实例，Shader类主要是用来管理一个.shader着色器文件，以及Shader的一些全局设置。比如：Shader.Find, Shader.globalKeywords, Shader.PropertyToID, Shader.SetGlobalXXXX和Shader.GetGlobalXXXX等。 Shader类主要是用来管理如下的ShaderLab代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&#x2F;&#x2F; URPUnlitShaderBasic.shaderShader &quot;Example&#x2F;URPUnlitShaderBasic&quot;&#123; &#x2F;&#x2F; 属性块 Properties &#123; &#x2F;&#x2F; 定义了一个纹理属性， 可以通过Shader.FindPropertyIndex查找索引，让后通过索引获取属性相关的信息Shader.GetPropertyXXX [MainTexture] _MainTex (&quot;Texture&quot;, 2D) &#x3D; &quot;white&quot; &#123;&#125; &#125; &#x2F;&#x2F; SubShader块，包含着色器代码 SubShader &#123; &#x2F;&#x2F; 定义Shader的LOD级别 LOD 100 &#x2F;&#x2F; 定义Shader的Tag, 可以通过Shader.FindPassTagValue获取对应的值 Tags &#123; &quot;RenderType&quot; &#x3D; &quot;Opaque&quot; &quot;RenderPipeline&quot; &#x3D; &quot;UniversalPipeline&quot; &#125; Pass &#123; &#x2F;&#x2F; HLSL代码块 SRPHLSL语言. HLSLPROGRAM &#x2F;&#x2F; 指明顶点着色器是哪个函数 #pragma vertex vert &#x2F;&#x2F; 指明片元着色器是哪个函数 #pragma fragment frag &#x2F;&#x2F; 引入包含文件，Core.hlsl文件包含了一些在HLSL中频繁使用的宏和函数 #include &quot;Packages&#x2F;com.unity.render-pipelines.universal&#x2F;ShaderLibrary&#x2F;Core.hlsl&quot; &#x2F;&#x2F; 这个结构定义了传入顶点着色器的参数 struct Attributes &#123; &#x2F;&#x2F; 顶点的对象空间位置 float4 positionOS : POSITION; &#x2F;&#x2F; 顶点的纹理坐标 float2 uv : TEXCOORD0; &#125;; &#x2F;&#x2F; 定义了顶点着色器的输出结果 struct Varyings &#123; &#x2F;&#x2F; 定义了输出到裁剪空间中的位置 float4 positionHCS : SV_POSITION; &#x2F;&#x2F; 顶点的纹理坐标 float2 uv : TEXCOORD0; &#125;; &#x2F;&#x2F; 这个宏定义了_MainTex是一个2D纹理对象 TEXTURE2D(_MainTex); &#x2F;&#x2F; 这个宏声明了_MainTex的采样器 SAMPLER(sampler_MainTex); &#x2F;&#x2F; 设置纹理的平铺和偏移 float4 _MainTex_ST; &#x2F;&#x2F; 顶点着色器函数 Varyings vert(Attributes IN) &#123; &#x2F;&#x2F; 声明一个顶点的输出数据 Varyings OUT; &#x2F;&#x2F; 将输入的对象空间的坐标转换到裁剪空间中 OUT.positionHCS &#x3D; TransformObjectToHClip(IN.positionOS.xyz); &#x2F;&#x2F; 计算平铺和偏移后的纹理坐标并传递给下一个阶段 OUT.uv &#x3D; TRANSFORM_TEX(IN.uv, _MainTex); &#x2F;&#x2F; 返回输出的结果 return OUT; &#125; &#x2F;&#x2F; 片元着色器函数 half4 frag() : SV_Target &#123; &#x2F;&#x2F; 采样纹理，并返回采样后的纹理颜色 half4 color &#x3D; SAMPLE_TEXTURE2D(_MainTex, sampler_MainTex, IN.uv); return color; &#125; ENDHLSL &#125; &#125; SubShader &#123; &#x2F;&#x2F; 子Shader，Unity在使用此着色器的时候，会激活硬件兼容的子Shader，一个着色器同时只能有一个子Shader被激活。 &#125; &#x2F;&#x2F; 如果都没有兼容的子Shader，Fallback则用来保底 Fallback &quot;ExampleOtherShader&quot;&#125; 材质属性 在Shader中可以定义材质属性，这些属性存储在材质资源，并且可以持久化。 使用材质属性： 可以通过调用材质的函数设置材质属性（比如：Material.SetFloat等）。 可以在Inspector中查看和编辑材质属性。 Unity保存这些属性在材质资源中。 不使用材质属性： 还是可以通过材质函数访问材质属性。 在Inspector中将不显示这些属性。 改变不能够持久化。 材质属性格式如下： 1[optional: attribute] name(&quot;display text in Inspector&quot;, type name) &#x3D; default value \"attribute\"可用值参见官网：https://docs.unity3d.com/2022.2/Documentation/Manual/SL-Properties.html#material-property-attributes 内建的MaterialPropertyDrawer:https://docs.unity3d.com/2022.2/Documentation/ScriptReference/MaterialPropertyDrawer.html \"type name\"可用值参见官网：https://docs.unity3d.com/2022.2/Documentation/Manual/SL-Properties.html#material-property-declaration-syntax-by-type 自定义材质编辑器: 123456789public class ExampleShaderGUI : ShaderGUI &#123; public override void OnGUI (MaterialEditor materialEditor, MaterialProperty[] properties) &#123; // Custom code that controls the appearance of the Inspector goes here base.OnGUI (materialEditor, properties); &#125;&#125; 1234567891011121314Shader &quot;Examples&#x2F;UsesCustomEditor&quot;&#123; &#x2F;&#x2F; The Unity Editor uses the class ExampleCustomEditor to configure the Inspector for this shader asset CustomEditor &quot;ExampleShaderGUI&quot; CustomEditorForRenderPipeline &quot;ExampleRenderPipelineShaderGUI&quot; &quot;ExampleRenderPipelineAsset&quot; CustomEditorForRenderPipeline &quot;OtherExampleRenderPipelineShaderGUI&quot; &quot;OtherExampleRenderPipelineAsset&quot; SubShader &#123; Pass &#123; &#125; &#125;&#125; SubShader 一个Shader object中可以包含多个SubShader块，在SubShader块中，你能： 指定LOD； 使用Tags设置一些键值对； 添加GPU指令或着色器代码； 定义一个或多个Pass 通过PackageRequirements指定依赖的包 基础格式如下： 1234567SubShader&#123; &lt;optional: LOD&gt; &lt;optional: tags&gt; &lt;optional: commands&gt; &lt;One or more Pass definitions&gt;&#125; SubShader中的Tag Tag是定义的一些键值对，主要作用是标识出Shader的一些特性，以便Unity引擎去确定该如何以及合适使用给定的SubShader，我们也可以创建和自定义Tag，通过C#的Shader和Material类去访问这些Tag。 注意：SubShader中的Tag块和Pass中的Tag块是不同的，不能将SubShader的Tag给Pass,也不能将你Pass的Tag给SubShader。 Tag的基础格式: 1Tags &#123; “[name1]” &#x3D; “[value1]” “[name2]” &#x3D; “[value2]”&#125; RenderPipeline Tag RenderPipeline tag用于指定是否兼容于URP和HDRP管线。可以能值： UniversalRenderPipeline ：兼容URP； HighDefinitionRenderPipeline : 兼容HDRP； 其他值 ： 不兼容URP和HDRP，主要是自定义的值，比如你新增了一个自定义的渲染你指定你自定的值； Queue Tag 告诉Unity渲染几何对象的顺序，可能的值： Background, Geometry, AlphaTest, Transparent, Overlay； Background+1(偏移)等； RenderType Tag 使用RenderType tag覆写Shader对象的行为。在内建渲染管线中，你能是使用shader replacement技术在运行时替换使用的SubShader。这个技术就需要RenderType tag来匹配SubShader。这在产生一些深度纹理的时候是有用的。 内建值：https://docs.unity3d.com/2022.2/Documentation/Manual/SL-ShaderReplacement.html 自定义值 ForceNoShadowCasting Tag ForceNoShadowCasting tag 不需要阴影投射pass，可能的值： True False DisableBatching Tag DisableBatching tag禁止将其加入到动态批处理中。这对于执行对象空间操作的着色器程序很有用。 动态批处理将所有几何体转换为世界空间，这意味着着色器程序不能再访问对象空间。 因此，依赖对象空间的着色器程序无法正确渲染。 为避免此问题，请使用此 SubShader tag来防止 Unity 应用动态批处理。可能的值： True False LodFading, Unity会阻止对属于LODGroup且Fade Mode值不是“None”的所有几何体进行动态批处理。 否则，Unity不会阻止动态批处理。 PreviewType Tag PreviewType tag用于指定材质的预览对象类型，可能的值： Sphere, 球体 Plane, 平面 Skybox, 天空盒子 CanUseSpriteAtlas Tag 在使用Legacy Sprite Packer的项目中使用此SubShader tag来警告用户您的着色器依赖于原始纹理坐标，因此他们不应将其纹理打包到图集中。可能的值： True ：兼容老的Sprite Packer， 默认值 False: 不兼容老的Sprite Pakcer，此SubShder依赖于原始纹理坐标，不将其打包到纹理中。 Tag详细信息 SubShader中的LOD LOD标识SubShader的LOD级别，SubShader的LOD级别不是依靠到相机的距离，而是通过Shader.maximumLOD来进行设置的。当此值改变后，Unity又会重新选择一个SubShader来使用。我们在编写带LOD的SubShader时，应该你将LOD值大的SubShader放在前面，应为Unity会选择第一个小于Shader.maximumLOD的SubShader作为激活的。 SubShader中的GPU命令 ShaderLab命令分为以下几类： 设置GPU渲染状态命令。 创建具有特定目的的Pass命令（UsePass和GrapPass）。 老的“固定管线风格”命令，允许你创建一个不带HLSL着色器代码的着色器程序。 可用命令如下： Category分组命令，使用Category块去分组设置渲染状态命令，所以在这个块中的SubShader块都能继承这个些渲染状态设置。 123456789101112Shader &quot;example&quot; &#123;Category &#123; Blend One One SubShader &#123; &#x2F;&#x2F; ... &#125; SubShader &#123; &#x2F;&#x2F; ... &#125; &#x2F;&#x2F; ...&#125;&#125; AlphaToMask, 启用或禁用GPU的alpha-to-coverage模式。 alpha-to-coverage是指在多重采样阶段，将此片元的采样覆盖率的计算加入alpha的因素，比如在一个4x的多重采样下，4个样本计算的颜色值是(0.8, 0.8,0.8),此片元的alpha的值是0.5，那么最终的颜色值将是(0.4, 0.4, 0.4)。 1234567891011121314151617181920212223Shader &quot;Examples&#x2F;CommandExample&quot;&#123; SubShader &#123; Pass &#123; &#x2F;&#x2F; 作用于此Pass AlphaToMask On &#125; &#125;&#125;Shader &quot;Examples&#x2F;CommandExample&quot;&#123; SubShader &#123; &#x2F;&#x2F; 作用于整个SubShader AlphaToMask On Pass &#123; &#125; &#125;&#125; Blend, 混合命令，指定混合因子 启用混合后，将禁用一些在GPU上的优化（大部分隐藏表面被移除或Early-Z），这将导致GPU的帧时间增加。 如果混合被启用，下列事情将发生： 如果BlendOp命令被指定则使用它指定的操作进行混合，否则将使用默认的Add进行混合。 如果这个混合操作时Add, Sub, RevSub, Min或Max时，GPU将片元着色器的输出值乘以源因子。 如果这个混合操作时Add, Sub, RevSub, Min或Max时，GPU将渲染目标中的值乘以目标因子。 GPU对结果值执行混合操作。 混合方程为： 1finalValue &#x3D; sourceFactor * sourceValue operation destinationFactor * destinationValue 在这方程中： finalValue 是GPU将写入帧缓存中的值。 sourceFactor 定义当前片元输出时使用的因子。 sourceValue 当前片元输出的颜色值。 operation 混合操作类型。 destinationFactor 定义帧缓冲中使用的因子。 destinationValue 帧缓冲区中的值。 Blend命令格式: 命令格式 实例 功能说明 Blend [state] Blend Off 禁止混合（默认值） Blend [render target] [state] Blend 1 Off 同上一个相同，不同的是指定对应的渲染目标 Blend [source factor] [destination factor] Blend One Zero 对默认渲染目标，设置RGBA的混合因子 Blend [render target] [source factor] [destination factor] Blend 1 One Zero 同上一个相同，唯一不同的是指定了渲染目标 Blend [source factor RGB] [destination factor RGB], [source factor alpha] [destination factor alpha] Blend One Zero, Zero One 分别指定了RGB和A同的混合因子 Blend [render target] [source factor RGB] [destination factor RGB], [source factor alpha] [destination factor alpha] Blend 1 One Zero, Zero One 同上一个相同，唯一不同的是指定了特定的渲染目标0 Blend命令参数: 参数 值 功能说明 rander taget 0-7 渲染目标的索引 state Off 禁用混合 factor One 输入值是1，表示使用原或目标颜色 factor Zero 输入值是0，表示移除原或目标值 factor SrcColor GPU 将此输入的值乘以源颜色值 factor SrcAlpha GPU 将此输入的值乘以源alpha值 factor DstColor GPU 将此输入的值乘以帧缓冲区源颜色值 factor DstAlpha GPU 将此输入的值乘以帧缓冲区源alpha值 factor OneMinusSrcColor GPU 将此输入的值乘以（1 - 源颜色） factor OneMinusSrcAlpha GPU 将此输入的值乘以（1 - 源alpha） factor OneMinusDstColor GPU 将此输入的值乘以（1 - 目标颜色） factor OneMinusDstAlpha GPU 将此输入的值乘以（1 - 目标alpha） BlendOp，混合操作，指定混合时使用的操作 不是所有的混合操作在所有的设备上都支持，这依赖图像API和硬件。不同的图像API处理不支持的混合操作时是不同的：GL跳过不支持的操作，Vulkan和Meta将使用Add操作。 命令格式： 1BlendOp &lt;operation&gt; BlendOp命令参数: 参数 值 功能说明 operation Add 原加目标 - Sub 原减目标 - RevSub 目标减原 - Min 原和目标谁小使用谁 - Max 原和目标谁大使用谁 [更多参数参见]https://docs.unity3d.com/2022.2/Documentation/Manual/SL-BlendOp.html ColorMask, 颜色蒙版 设置颜色通道写入蒙版，防止GPU写入渲染目标中的通道。默认情况下，GPU写入所有通道(RGBA)。对于某些效果，您可能希望不修改某些通道； 例如，您可以禁用颜色渲染来渲染无色阴影。 另一个常见的用例是完全禁用颜色写入，这样您就可以用数据填充一个缓冲区而无需写入其他缓冲区； 例如，您可能想要填充模板缓冲区 无需写入渲染目标。 命令格式： 命令格式 实例 功能说明 命令格式 实例 功能说明 ColorMask [channels] ColorMask RGB 写入指定通道到渲染目标中 ColorMask [channels] [target] ColorMask RGB 2 同上一, 指定了渲染目标 Conservative, 保守光栅化 光栅化是一种将矢量数据（三角形投影）转换为像素数据（渲染目标）通过确定哪些像素被三角形覆盖。通常，GPU通过对像素内的点进行采样，判断是否被三角形覆盖，来判断是否对像素进行光栅化；如果覆盖足够，则 GPU 确定该像素被覆盖。保守光栅化意味着GPU会光栅化被三角形部分覆盖的像素，而不管覆盖范围如何。这在需要确定性时很有用，例如在执行遮挡剔除,GPU上的碰撞检测，或可见性检测。保守的光栅化意味着GPU在三角形边缘上生成更多的片元；这导致更多的片元着色器调用，这可能导致GPU帧时间增加。 命令格式： 1Conservative &lt;True&#x2F;False&gt; Cull, 剔除操作 设置GPU应该基于多边形相对于摄像机的方向剔除哪些多边形。剔除是确定不绘制什么的过程。剔除提高了渲染效率，因为不会浪费 GPU时间来绘制在最终图像中不可见的内容。默认情况下，GPU 执行背面剔除；这意味着它不绘制背对观察者的多边形。一般来说，渲染工作量减少得越多越好；因此，只在必要时才应更改此设置。 命令格式： 1Cull &lt;Back&#x2F;Front&#x2F;Off&gt; Offset, 深度偏移 设置 GPU 上的深度偏差。深度偏差，也称为深度偏移，是GPU上的一个设置，决定了GPU绘制几何体的深度。调整深度偏差以强制GPU在具有相同深度的其他几何体之上绘制几何体。这可以帮助您避免不需要的视觉效果，例如深度冲突和阴影暗斑。要为特定几何体设置深度偏差，请使用此命令或 RenderStateBlock 。要设置影响所有几何体的全局深度偏差，请使用 CommandBuffer.SetGlobalDepthBias。除了全局深度偏差之外，GPU还为特定几何体应用深度偏差。 为了减少阴影暗斑，您可以使用 light bias 设置实现类似的视觉效果；但是，这些设置的工作方式不同，并且不会更改 GPU 上的状态。有关更多信息，请参阅阴影故障排除。 命令格式： Offset [factor], [units] Offset命令参数: 参数 值 功能说明 factor 浮点数，范围-1到1 缩放最大 Z 斜率，也称为深度斜率，以生成每个多边形的可变深度偏移。不平行于近剪裁平面和远剪裁平面的多边形具有 Z 斜率。调整此值以避免此类多边形上出现视觉瑕疵。 units 浮点数，范围 –1 到 1。 缩放最小可分辨深度缓冲区值，以产生恒定的深度偏移。最小可分辨深度缓冲区值（一个 unit）因设备而异。负值意味着 GPU 将多边形绘制得更靠近摄像机。正值意味着 GPU 将多边形绘制得更远离摄像机。 Stencil, 模板命令 模板缓冲区为帧缓冲区中的每个像素存储一个 8 位整数值。为给定像素执行片元着色器之前，GPU 可以将模板缓冲区中的当前值与给定参考值进行比较。这称为模板测试。如果模板测试通过，则 GPU 会执行深度测试。如果模板测试失败，则 GPU 会跳过对该像素的其余处理。这意味着可以使用模板缓冲区作为遮罩来告知 GPU 要绘制的像素以及要丢弃的像素。通常会将模板缓冲区用于特殊效果，例如门户或镜子。此外，在渲染硬阴影或者构造型实体几何 (CSG) 时，有时会使用模板缓冲区。 使用 Ref、ReadMask 和 Comp 参数可配置模板测试。使用 Ref、WriteMask、Pass、Fail 和 ZFail 参数可配置模板写入操作。 模板测试方程为： 1(ref &amp; readMask) comparisonFunction (stencilBufferValue &amp; readMask) 命令格式： 123456789101112131415161718Stencil&#123; Ref &lt;ref&gt; ReadMask &lt;readMask&gt; WriteMask &lt;writeMask&gt; Comp &lt;comparisonOperation&gt; Pass &lt;passOperation&gt; Fail &lt;failOperation&gt; ZFail &lt;zFailOperation&gt; CompBack &lt;comparisonOperationBack&gt; PassBack &lt;passOperationBack&gt; FailBack &lt;failOperationBack&gt; ZFailBack &lt;zFailOperationBack&gt; CompFront &lt;comparisonOperationFront&gt; PassFront &lt;passOperationFront&gt; FailFront &lt;failOperationFront&gt; ZFailFront &lt;zFailOperationFront&gt;&#125; 命令参数： 参见官网 UsePass, 使用Pass命令 UsePass 命令插入来自另一个 Shader 对象的指定通道。可以使用此命令来减少着色器源文件中的代码重复。 GrabPass GrabPass 是一个创建特殊类型通道的命令，该通道将帧缓冲区的内容抓取到纹理中。在后续通道中即可使用此纹理，从而执行基于图像的高级效果。此命令会显著增加 CPU 和 GPU 帧时间。除了快速原型制作之外，您通常应该避免使用此命令，并尝试通过其他方式实现您的效果。如果您确实使用了此命令，尽量减少屏幕抓取操作的次数；方法是减少您对该命令的使用，或者使用将屏幕抓取到命名纹理的签名（如果适用）。 命令格式： 格式 功能说明 GrabPass { } 将帧缓冲区内容抓取到一个纹理中，使您可以在同一个子着色器中的后续通道中使用该纹理。使用 _GrabTexture 名称引用该纹理。当您使用此签名时，Unity 每次渲染包含此命令的批处理时都会抓取屏幕。这意味着 Unity 可以每帧多次抓取屏幕：每批次一次。 GrabPass { \"ExampleTextureName\" } 将帧缓冲区内容抓取到一个纹理中，使您可以在同一帧的后续通道中跨多个子着色器访问该纹理。使用给定名称引用该纹理。当您使用此签名时，Unity 会在渲染批处理的帧中第一次抓取屏幕，该批处理包含具有给定纹理名称的此命令。 ZClip， 深度裁剪 设置 GPU 的深度裁剪模式，从而确定 GPU 如何处理近平面和远平面之外的片元。将 GPU 的深度裁剪模式设置为钳位对于模板阴影渲染很有用；这意味着当几何体超出远平面时不需要特殊处理，从而减少渲染操作。但是，它可能会导致不正确的 Z 排序。 命令格式： ZClip [enabled] 命令参数： 参数 值 功能说明 enabled True 将深度裁剪模式设置为裁剪。这是默认设置。 False 将深度裁剪模式设置为钳位。比近平面更近的片元正好在近平面，而比远平面更远的片元正好在远平面。 ZTest, 深度测试 设置几何体是否通过深度测试的条件。深度测试可使具有 “Early-Z” 功能的 GPU 在管线早期拒绝几何体，并确保几何体的正确排序。通过改变深度测试的条件，您可以实现物体遮挡等视觉效果。 命令格式： ZTest [operation] 命令参数： 参数 值 功能说明 operation Less 绘制位于现有几何体前面的几何体。不绘制位于现有几何体相同距离或后面的几何体。 LEqual 绘制位于现有几何体前面或相同距离的几何体。不绘制位于现有几何体后面的几何体。这是默认值。 ... 其他的值，参见官网 ZWrite, 深度写入 设置在渲染过程中是否更新深度缓冲区内容。通常，ZWrite 对不透明对象启用，对半透明对象禁用。禁用 ZWrite 会导致不正确的深度排序。这种情况下，您需要在 CPU 上对几何体进行排序。 命令格式： ZWrite [state] 命令参数： 参数 值 功能说明 state On 启用写入深度缓冲区。 Off 禁用写入深度缓冲区。 SubShader中的Pass Pass是Shader object中的基础元素，它包含了GPU状态设置指令，以及着色器程序。简单的Shader object可能包含仅单个Pass,但更复杂的Shader能包含多个Pass。通过多个Pass可以定义Shader object中不同的部分；例如，需要更改渲染状态、不同着色器程序或不同LightMode Pass标签。 注意：在SRP管线中，能通过RenderStateBlock去改变GPU的渲染器状态，不需要单独的Pass去改变状态。 在一个Pass块中，你能： 通过Name块指定一个Pass的名字； 使用Tags块去指定Tags的键值对； 执行GPU命令； 添加着色器代码； 通过PackageRequirements指定依赖的包 格式如下： 1234567Pass&#123; &lt;optional: name&gt; &lt;optional: tags&gt; &lt;optional: commands&gt; &lt;optional: shader code&gt;&#125; Pass中的Tags Pass中，比较通用的Tag是LightMode, 此Tag可以用于所有的渲染管线。Pass中其他Tag根据渲染管线的不同而不同。 - 在内建管线，Unity预定义的Pass中使用的Tag - 在URP中，Unity预定义的Pass中使用的Tag 在SubShader和Pass块中都能使用Tags块，但是它们的工作是不同的。在SubShader中的Tags在Pass中将没有效果，反之亦然。 在内建渲染管线，如果你不设置LightMode tag, Unity渲染时将不使用灯光和阴影，这本质上就是讲LightMode设置为Always。在SRP中，你能使用SRPDefaultUnlit值来达到同样的效果。 内置管线的Pass中的Tags参见官网 Pass中的Commands Pass中的使用的Commands和SubShader中的是相同的。 Pass中的Shader code 在 Unity 中，您通常使用 HLSL 编写着色器程序。要将 HLSL 代码添加到您的着色器资源，应将该代码放在一个着色器代码块 中。注意：Unity 还支持使用其他语言编写着色器程序，不过通常不需要或不推荐这样做。 着色器代码块的类型： HLSL: HLSLPROGRAM , ENDHLSL 表示着色器代码块 HLSLINCLUDE, ENDHLSL 表示着色器include块 CG: CGPROGRAM, ENDCG 表示着色器代码块 CGINCLUDE, ENDCG 表示着色器include块 CG一般在老版本中使用，不能在新的URP和HDRP中使用，但HLSL却能在所有的管线中使用，建议都直接使用HLSL。 示例： 12345678910111213141516171819202122232425262728293031323334353637Shader &quot;Examples&#x2F;ExampleShader&quot;&#123; SubShader &#123; HLSLINCLUDE &#x2F;&#x2F; 在此编写要共享的 HLSL 代码 ENDHLSL Pass &#123; Name &quot;ExampleFirstPassName&quot; Tags &#123; &quot;LightMode&quot; &#x3D; &quot;ExampleLightModeTagValue&quot; &#125; &#x2F;&#x2F; 在此编写设置渲染状态的 ShaderLab 命令 HLSLPROGRAM &#x2F;&#x2F; 此 HLSL 着色器程序自动包含上面的 HLSLINCLUDE 块的内容 &#x2F;&#x2F; 在此编写 HLSL 着色器代码 ENDHLSL &#125; Pass &#123; Name &quot;ExampleSecondPassName&quot; Tags &#123; &quot;LightMode&quot; &#x3D; &quot;ExampleLightModeTagValue&quot; &#125; &#x2F;&#x2F; 在此编写设置渲染状态的 ShaderLab 命令 HLSLPROGRAM &#x2F;&#x2F; 此 HLSL 着色器程序自动包含上面的 HLSLINCLUDE 块的内容 &#x2F;&#x2F; 在此编写 HLSL 着色器代码 ENDHLSL &#125; &#125;&#125; HLSL着色器语言 HLSL语言有两种语法：旧版的 DirectX 9 样式语法以及更现代的 DirectX 10+ 样式语法。不同之处主要在于纹理采样函数的工作方式： 旧版语法使用 sampler2D、tex2D() 和类似函数。此语法适用于所有平台。 DX10+ 语法使用 Texture2D、SamplerState 和 .Sample() 函数。由于纹理和采样器在 OpenGL 中不是不同对象，因此该语法的某些形式在 OpenGL 平台上无效。 Unity 提供了包含预处理器宏的着色器库来帮助您管理这些差异。有关更多信息，请参阅内置着色器宏。 HLSL中的预处理器指令 在内部，着色器编译有多个阶段。第一阶段是预处理，一个称为预处理器的程序为编译准备代码。预处理器指令是用于预处理器的指令。本文的这一部分包含有关使用 HLSL 预处理器指令和 Unity 独有的 HLSL 预处理器指令的特定于 Unity 的方法的信息。它不包含有关 HLSL 支持的所有预处理器指令的详尽文档，或有关在 HLSL 中使用预处理器指令的一般信息。有关该信息，请参阅 HLSL 文档：预处理器指令 (HLSL)。 HLSL中的include和include_with_pragmas指令 在 HLSL 中，#include指令是一种预处理指令。它们指示编译器将一个 HLSL 文件的内容包含在另一个文件中。它们包含的文件称为包含文件。在 Unity 中，常规#include指令的工作方式与标准 HLSL 中的相同。Unity 还提供了一个额外的、特定于 Unity 的#include_with_pragmas指令。该#include_with_pragmas指令的工作方式与常规#include指令相同，但它还允许您#pragma在包含文件中使用指令。这意味着该#include_with_pragmas指令允许您#pragma在多个文件之间共享指令。 注意：要使用#include_with_pragmas指令，您必须启用缓存着色器预处理器(Project setting-&gt;Editor)。 pragma预处理指令 在 HLSL 中，#pragma指令是一种预处理指令。它们为着色器编译器提供其他类型的预处理器指令未涵盖的附加信息。 使用 pragma 指令 您可以将#pragma指令放在 HLSL 代码中的任何位置，但通常将它们放在开头，如下所示： 123456#pragma target 3.0#pragma exclude_renderers vulkan#pragma vertex vert#pragma fragment frag&#x2F;&#x2F; The rest of your HLSL code goes here 限制 pragma指令的限制如下： 你能使用#pragma指令在#if指令内部，如果表达式依赖如下： 任意自定义的#define定义 下列平台关键字：SHADER_API_MOBILE, SHADER_API_DESKTOP, UNITY_NO_RGBM, UNITY_USE_NATIVE_HDR, UNITY_FRAMEBUFFER_FETCH_AVAILABLE, UNITY_NO_CUBEMAP_ARRAY UNITY_VERSION宏 您只能在.shader文件以及包含在#include_with_pragmas指令中的文件中使用Unity特定的#pragma指令。 Unity在您使用 #include指令包含的文件中不支持它们； 编译器会忽略它们。 您只能在使用#include指令包含的文件中使用标准HLSL #pragma指令。 Unity不支持它们在.shader文件中，或者在您使用 #include_with_pragmas指令包含的文件中；编译器会忽略它们。 支持的pragma预处理指令列表 在一个常规的包含文件中，unity支持所有标准HLSL中的指令。更多HLSL的指令参见HLSL文档。 另外，Unity支持以下Unity特定的#pragma指令： 表面着色器: #pragma surface [surface function] [lighting model] [optional parameters], 表面着色器 着色器阶段: #pragma vertex [name] 指定顶点着色器函数 #pragma hull [name] 指定曲面细分着色器的hull函数 #pragma domain [name]指定曲面细分着色器的domain函数 #pragma geometry [name] 指定几何着色器函数 #pragma fragment [name] 指定片元着色器函数 着色器变体和关键字: #pragma multi_compile [keywords] 声明关键字的集合。编译器包含构建中的所有关键字。您可以使用诸如_local设置附加选项的后缀。 #pragma shader_feature [keywords] 声明关键字的集合。编译器会从构建中排除未使用的关键字。您可以使用诸如_local设置附加选项的后缀。 #pragma hardware_tier_variants [values] 仅内置渲染管道：为给定的图形 API 编译时为图形层添加关键字。 #pragma skip_variants [list of keywords] 剥离指定的关键字。 GPU要求和着色器模型支持: #pragma target [value] 此着色器程序兼容的最小着色器模型。将 [value] 替换为有效值。 #pragma require [value] 不要为给定的图形 API 编译此着色器程序。将 [value' 替换为以空格分隔的有效值列表。 图形API: #pragma only_renderers [value] 仅为给定的图形API编译此着色器程序。将[values]替换为以空格分隔的有效值列表。 #pragma exclude_renderers [value] 不要为给定的图形 API 编译此着色器程序。将 [value] 替换为以空格分隔的有效值列表。 其他pragma指令: #pragma instancing_options [options] 使用给定选项在此着色器中启用GPU实例化。有关详细信息，请参阅GPU实例化 #pragma once 将此指令放在一个文件中，以确保编译器在着色器程序中只包含一次该文件。注意： Unity 仅在启用缓存着色器预处理器时支持此指令。 #pragma enable_d3d11_debug_symbols 为支持的图形API生成着色器调试符号，并禁用所有图形API的优化。使用它在外部工具中调试着色器代码。Unity 为 Vulkan、DirectX 11 和 12 以及支持的控制台平台生成调试符号。 #pragma skip_optimizations [value] 强制关闭给定图形API的优化。将 [values] 替换为以空格分隔的有效值列表。 #pragma hlslcc_bytecode_disassembly 将反汇编的 HLSLcc 字节码嵌入到翻译后的着色器中。 #pragma disable_fastmath 启用涉及NaN处理的精确IEEE 754规则。这目前仅影响 Metal 平台。 #pragma editor_sync_compilation 强制同步编译。这仅影响 Unity 编辑器。 #pragma enable_cbuffer 使用HLSLSupport中的 CBUFFER_START(name) 和 CBUFFER_END 宏时发出 cbuffer(name)，即使当前平台不支持常量缓冲区。 在HLSL中Shader Model目标和GPU特性 您可以使用#pragma指令来指示一个着色器需要某些GPU功能。在运行时，Unity使用此信息来确定着色器程序是否与当前硬件兼容。您可以使用#pragma require指令指定单个 GPU 功能，或使用指令#pragma target指定着色器模型。着色器模型是一组GPU功能的简写；在内部，它与#pragma require具有相同功能列表的指令相同。正确描述着色器所需的GPU功能非常重要。如果您的着色器使用了未包含在要求列表中的功能，这可能会导致编译时错误，或者导致设备在运行时无法支持着色器。 默认情况下，Unity使用编译着色器#pragma require derivatives，对应于#pragma target 2.5. 着色器阶段的特殊要求 如果您的着色器定义了某些着色器阶段，Unity 会自动将项目添加到需求列表中。 如果着色器定义了几何阶段（使用#pragma geometry），Unity 会自动添加geometry到需求列表中。 如果着色器定义了曲面细分阶段（使用#pragma hull或#pragma domain），Unity 会自动添加tessellation到需求列表中。 如果需求列表（或等效的目标值）尚未包含这些值，Unity 在编译着色器时会显示一条警告消息，表明它已添加这些需求。为避免看到此警告消息，请在代码中显式添加要求或使用适当的目标值。 指定 GPU 功能或着色器模型 要指定所需的功能，请使用该#pragma require指令，后跟以空格分隔的值列表。例如： 1#pragma require integers mrt8 您还可以使用该#pragma require指令后跟一个冒号和一个以空格分隔的着色器关键字列表。这意味着该要求仅适用于在启用任何给定关键字时使用的变体。例如: 1#pragma require integers mrt8 : EXAMPLE_KEYWORD OTHER_EXAMPLE_KEYWORD 您可以使用多#pragma require行。在此示例中，着色器integers在所有情况下都需要，并且mrt8如果启用了 EXAMPLE_KEYWORD。 12#pragma require integers#pragma require integers mrt8 : EXAMPLE_KEYWORD 要指定着色器模型，请使用#pragma target指令。例如： 1#pragma target 4.0 您还可以使用该#pragma target指令后跟以空格分隔的着色器关键字列表。这意味着该要求仅适用于在启用任何给定关键字时使用的变体。例如： 1#pragma target 4.0 EXAMPLE_KEYWORD OTHER_EXAMPLE_KEYWORD '#pragma target' 值列表 '#pragma require' 值列表 声明和使用着色器关键字 在您的HLSL代码中，使用#pragma指令来声明着色器关键字，并使用#if指令来指示着色器代码的一部分取决于着色器关键字的状态。 您可以在常规图形着色器（包括表面着色器）中使用着色器关键字)和计算着色器。 声明着色器关键字 要声明着色器关键字，请在 HLSL 代码中使用以下 #pragma 指令之一： #pragma multi_compile 声明一组关键字。默认情况下，这些关键字具有全局范围并影响所有着色器阶段。构建过程包括该集合中的所有关键字。 #pragma shader_feature 声明一组关键字，并指示编译器编译丢弃没有启用的关键字的变体。默认情况下，这些关键字具有全局范围并影响所有着色器阶段。构建过程包括该集合中在构建时使用的关键字。 注意：如果您将着色器添加到图形设置窗口中的 Always Included Shaders列表中，Unity会在构建中包含来自所有集合的所有关键字，即使它们是用声明的#pragma shader_feature。 您还可以为这些指令添加后缀来修改它们的行为： 添加_local表示一组关键字具有局部作用域，不能被全局关键字覆盖；否则，关键字具有全局范围，并且可以被全局关键字覆盖。您可以将此后缀添加到#pragma multi_compile、 或#pragma shader_feature指令后面；例如，#pragma multi_compile_local和#pragma shader_feature_local是有效的。 添加_vertex, _fragment, _hull, _domain, _geometry, 或_raytracing表示一组关键字仅影响给定的着色器阶段，这可以减少不需要的着色器变体的数量。您可以将这些后缀添加到指令#pragma multi_compile或#pragma shader_feature指令中，无论是独立的还是在_local修饰符之后；例如，#pragma multi_compile_vertex并且#pragma shader_feature_local_fragment是有效的。 此外，还有一些#pragma multi_compile添加预定义关键字集的“快捷方式”变体。有关这些的更多信息，请参阅multi_compile 快捷方式。 声明一组关键字 您在集合中声明关键字。一个集合包含互斥的关键字。要声明一组关键字，请使用#pragma multi_compileor#pragma_shader_feature指令，后跟以空格分隔的关键字列表。此示例演示如何声明一组四个关键字： 1#pragma multi_compile QUALITY_LOW QUALITY_MEDIUM QUALITY_HIGH QUALITY_ULTRA 在内部，这通过使用#define指令来工作。当 Unity 编译着色器时，它会生成四个变体：一个定义了 QUALITY_LOW，一个定义了 QUALITY_MEDIUM，一个定义了 QUALITY_HIGH，一个定义了 QUALITY_ULTRA。在运行时，Unity 使用适当的变体，具体取决于启用了哪些关键字。 当您使用#pragma shader_feature声明一组关键字时，Unity 还会编译一个变体，其中没有定义该组中的任何关键字。这允许您在不使用附加关键字的情况下定义行为。 减少关键字的数量在几个方面是有益的：它可以减少 Unity 编译的变体总数，从而提高构建时间和运行时性能；它减少了着色器使用的关键字总数，从而防止它达到着色器关键字限制；并且它使得从 C# 脚本管理关键字状态变得更简单，因为启用和禁用的关键字更少。这个例子演示了如何声明一个只包含一个关键字的集合： 1#pragma shader_feature EXAMPLE_ON 您也可以在使用#pragma multi_compile 时指示 Unity 执行此操作。 为此，请在集合中添加一个“空白”关键字，其名称为一个或多个下划线 (_)，如下所示： 1#pragma multi_compile __ EXAMPLE_ON 限制 声明关键字集的方式有一些限制： 您不能在同一个集合中多次包含相同的关键字；但是，您可以在不同的集合中声明相同的关键字。 您不能在着色器程序中多次声明同一组关键字。 着色器可以使用的关键字数量是有限制的。着色器源文件中声明的每个关键字及其依赖项都计入此限制。 使用着色器关键字 要编译仅在启用给定着色器关键字时使用的代码，请使用#if指令，如下所示： 1234567&#x2F;&#x2F; 声明关键字集#pragma multi_compile QUALITY_LOW QUALITY_MEDIUM QUALITY_HIGH QUALITY_ULTRA#if QUALITY_ULTRA&#x2F;&#x2F; 此处的代码针对启用关键字 QUALITY_ULTRA 时使用的变体进行编译#endif multi_compile 快捷方式 Unity为声明着色器关键字提供了几种“快捷方式”表示法。以下快捷键与内置渲染管线中的光照、阴影和光照贴图相关： multi_compile_fwdbase添加这组关键字：DIRECTIONAL LIGHTMAP_ON DIRLIGHTMAP_COMBINED DYNAMICLIGHTMAP_ON SHADOWS_SCREEN SHADOWS_SHADOWMASK LIGHTMAP_SHADOW_MIXING LIGHTPROBE_SH。PassType.ForwardBase需要这些变体。 multi_compile_fwdbasealpha添加这组关键字：DIRECTIONAL LIGHTMAP_ON DIRLIGHTMAP_COMBINED DYNAMICLIGHTMAP_ON LIGHTMAP_SHADOW_MIXING VERTEXLIGHT_ON LIGHTPROBE_SH。PassType.ForwardBase需要这些变体。 multi_compile_fwdadd添加这组关键字：POINT DIRECTIONAL SPOT POINT_COOKIE DIRECTIONAL_COOKIE。PassType.ForwardAdd需要这些变体。 multi_compile_fwdadd_fullshadows添加这组关键字：POINT DIRECTIONAL SPOT POINT_COOKIE DIRECTIONAL_COOKIE SHADOWS_DEPTH SHADOWS_SCREEN SHADOWS_CUBE SHADOWS_SOFT SHADOWS_SHADOWMASK LIGHTMAP_SHADOW_MIXING。这与 相同multi_compile_fwdadd，但这增加了灯光具有实时阴影的能力。 multi_compile_lightpass添加这组关键字：POINT DIRECTIONAL SPOT POINT_COOKIE DIRECTIONAL_COOKIE SHADOWS_DEPTH SHADOWS_SCREEN SHADOWS_CUBE SHADOWS_SOFT SHADOWS_SHADOWMASK LIGHTMAP_SHADOW_MIXING。这实际上是与实时光和阴影相关的所有功能的全能快捷方式，除了 Light Probes。 multi_compile_shadowcaster添加这组关键字：SHADOWS_DEPTH SHADOWS_CUBE。PassType.ShadowCaster需要这些变体。 multi_compile_shadowcollector添加这组关键字：SHADOWS_SPLIT_SPHERES SHADOWS_SINGLE_CASCADE。它还编译没有任何这些关键字的变体。屏幕空间阴影需要这些变体。 multi_compile_prepassfinal添加这组关键字：LIGHTMAP_ON DIRLIGHTMAP_COMBINED DYNAMICLIGHTMAP_ON UNITY_HDR_ON SHADOWS_SHADOWMASK LIGHTPROBE_SH。它还编译没有任何这些关键字的变体。PassType.LightPrePassFinal和PassType.Deferred需要这些变体。 以下快捷方式与其他设置相关： multi_compile_particles添加与内置粒子系统相关的关键字：SOFTPARTICLES_ON。它还编译没有这个关键字的变体。有关详细信息，请参阅内置粒子系统。 multi_compile_fog添加了这组与雾相关的关键字：FOG_LINEAR、FOG_EXP、FOG_EXP2。它还编译没有任何这些关键字的变体。您可以在图形设置窗口中控制此行为。 multi_compile_instancing添加与实例化相关的关键字。如果着色器使用程序实例化，它会添加这组关键字：INSTANCING_ON PROCEDURAL_ON。否则，它会添加此关键字：INSTANCING_ON。它还编译没有任何这些关键字的变体。您可以在图形设置窗口中控制此行为。 这些快捷方式中的大多数都包含多个关键字。如果您知道项目不需要它们，您可以使用#pragma skip_variants删除其中的一些。例如： 12# pragma multi_compile_fwdadd# pragma skip_variants POINT POINT_COOKIE 这告诉编译器删除关键字POINT或POINT_COOKIE从其他指令中删除。 着色器语义 编写 HLSL 着色器程序时， 输入和输出变量需要通过语义来表明 其“意图”。这是HLSL着色器语言中的标准概念；请参阅MSDN上的语义(Semantics)文档以了解更多详细信息。 顶点着色器输入语义 主顶点着色器函数（由 #pragma vertex 指令表示）需要在所有输入参数上都有语义。 这些对应于各个网格数据元素，如顶点位置、法线网格和纹理坐标 常用语义： 语义 数据类型 功能说明 POSITION float3 或float4 顶点位置 NORMAL float3 顶点法线 BINORMAL float4 顶点副法线 TEXCOORD[n] float2, float3或float4 UV坐标 TANGENT float4 顶点切线 COLOR float4 顶点颜色 当网格数据包含的组件少于顶点着色器输入所需的组件时，其余部分用零填充，但.w 默认为1。例如，网格纹理坐标通常是只有 x 和 y 组件的 2D 向量。如果顶点着色器声明一个带有TEXCOORD0语义的float4的输入，则顶点着色器将包含(x,y,0,1)。 顶点着色器输出和片段着色器输入语义 顶点着色器需要输出一个顶点的最终裁剪空间位置，以便 GPU 知道在屏幕上的哪个位置对它进行光栅化，以及在什么深度。这个输出需要有SV_POSITION语义，并且是一个float4类型。 顶点着色器产生的任何其他输出（“插值器”或“变量”）都是您特定的着色器需要的。顶点着色器输出的值将在渲染三角形的面上进行插值，每个像素的值将作为输入传递给片段着色器。 许多现代 GPU 并不真正关心这些变量的语义。然而，一些旧系统（尤其是 Direct3D 9 上的着色器模型 2 GPU）确实有关于语义的特殊规则： TEXCOORD0等TEXCOORD1用于指示任意高精度数据，例如纹理坐标和位置。 COLOR0顶点输出和COLOR1片段输入的语义适用于低精度、0-1 范围的数据（如简单的颜色值）。 为了获得最佳跨平台支持，请将顶点输出和片段输入标记为TEXCOORDn语义。 常用语义： 语义 数据类型 功能说明 SV_POSITION float3 或float4 顶点位置 COLOR float4 输出的顶点颜色 插值器计数限制： 总共可以使用多少个插值器变量来将信息从顶点传递到片段着色器是有限制的。限制取决于平台和 GPU，一般准则是： 最多 8 个插值器：OpenGL ES 2.0 (Android)、Direct3D 11 9.x 级别（Windows Phone）和 Direct3D 9 着色器模型 2.0（旧 PC）。由于插值器的数量是有限的，但每个插值器可以是一个 4 分量向量，一些着色器将事物打包在一起以保持在限制范围内。例如，可以在一个float4变量中传递两个纹理坐标（.xy 代表一个坐标，.zw 代表第二个坐标）。 最多 10 个插值器：Direct3D 9 着色器模型 3.0 ( #pragma target 3.0)。 最多 16 个插值器：OpenGL ES 3.0 (Android)、Metal (iOS)。 最多 32 个插值器：Direct3D 10 着色器模型 4.0 ( #pragma target 4.0)。 无论您的特定目标硬件如何，出于性能原因，使用尽可能少的插值器通常是一个好主意。 片段着色器输出语义 大多数情况下，片段（像素）着色器输出颜色，并具有 SV_Target语义。上面示例中的片段着色器正是这样做的： 1fixed4 frag (v2f i) : SV_Target 常用语义： 语义 数据类型 功能说明 SV_Target[n] fixed4 SV_Target1, SV_Target2, 等：这些是着色器写入的附加颜色。这在一次渲染到多个渲染目时使用（称为多渲染目标渲染技术，或 MRT）。SV_Target0是一样的SV_Target。 SV_Depth float4 通常片段着色器不会覆盖 Z 缓冲区值，并且使用来自正三角形的默认值光栅化. 但是，对于某些效果，输出每个像素的自定义 Z 缓冲区深度值很有用。 注意，在许多 GPU 上，这会关闭一些深度缓冲区 优化，所以不要在没有充分理由的情况下覆盖 Z 缓冲区值。所产生的成本因SV_DepthGPU 架构而异，但总体而言，它与 alpha 测试的成本非常相似（使用 HLSL 中的内置clip()函数）。 其他特殊语义 语义 数据类型 功能说明 VPOS UNITY_VPOS_TYPE 片段着色器可以接收被渲染为特殊VPOS语义的像素位置。此功能仅从着色器模型 3.0 开始存在，因此着色器需要具有#pragma target 3.0编译指令。在不同的平台上，屏幕空间位置输入的底层类型会有所不同，因此为了获得最大的可移植性，请使用它的UNITY_VPOS_TYPE类型（在大多数平台上它将是float4，在Direct3D 9上为 float2）。此外，使用像素位置语义使得裁剪空间位置 (SV_POSITION) 和 VPOS 很难在相同的在顶点到片段结构中。所以顶点着色器应该将裁剪空间位置作为一个单独的“out”变量输出。 VFACE float 片段着色器可以接收一个变量，该变量指示渲染的表面是否面向相机，或背对相机。这在渲染应该从两侧可见的几何图形时很有用——通常用于树叶和类似的薄物体。正值为三角形正面，负值为三角形的背面。此功能仅从着色器模型 3.0 开始存在，因此着色器需要具有#pragma target 3.0编译指令。 SV_VertexID uint 顶点着色器可以接收具有“顶点编号”作为无符号整数的变量。当您想从纹理或ComputeBuffers获取额外的每个顶点数据时，这非常有用。此功能仅存在于 DX10（着色器模型 4.0）和 GLCore / OpenGL ES 3 中，因此着色器需要具有#pragma target 3.5编译指令。 在 Cg/HLSL 中访问着色器属性 ShaderLab 中的属性类型以如下方式映射到 Cg/HLSL 变量类型： Color 和 Vector 属性映射到 float4、half4 或 fixed4 变量。 Range 和 Float 属性映射到 float、half 或 fixed 变量。 对于普通 (2D) 纹理，Texture 属性映射到 sampler2D 变量；立方体贴图 (Cubemap) 映射到 samplerCUBE；3D纹理映射到sampler3D。 如何向着色器提供属性值 在下列位置中查找着色器属性值并提供给着色器： MaterialPropertyBlock 中设置的每渲染器值。这通常是“每实例”数据（例如，全部共享相同材质的许多对象的自定义着色颜色）。 在渲染的对象上使用的材质中设置的值。 全局着色器属性，通过 Unity 渲染代码自身设置（请参阅内置着色器变量），或通过您自己的脚本来设置（例如 Shader.SetGlobalTexture）。 优先顺序如上所述：每实例数据覆盖所有内容；然后使用材质数据；最后，如果这两个地方不存在着色器属性，则使用全局属性值。最终，如果在任何地方都没有定义着色器属性值，则将提供“默认值”（浮点数的默认值为零，颜色的默认值为黑色，纹理的默认值为空的白色纹理）。 序列化和运行时材质属性 材质可以同时包含序列化的属性值和运行时设置的属性值。 序列化的数据是在着色器的 Properties 代码块中定义的所有属性。通常，这些是需要存储在材质中的值，并且可由用户在材质检视面板中进行调整。 材质也可以具有着色器使用的一些属性，但不在着色器的 Properties 代码块中声明。通常，这适用于在运行时从脚本代码（例如，通过 Material.SetColor）设置的属性。请注意，矩阵和数组只能作为非序列化的运行时属性存在（因为无法在 Properties 代码块中定义它们）。 特殊纹理属性 对于设置为着色器/材质属性的每个纹理，Unity 还会在其他矢量属性中设置一些额外信息。 纹理平铺和偏移 材质通常具有其纹理属性的 Tiling 和 Offset 字段。此信息将传递到着色器中的 float4 {TextureName}_ST 属性： x 包含 X 平铺值 y 包含 Y 平铺值 z 包含 X 偏移值 w 包含 Y 偏移值 例如，如果着色器包含名为 _MainTex 的纹理，则平铺信息将位于 _MainTex_ST 矢量中。 纹理大小 {TextureName}_TexelSize - float4 属性包含纹理大小信息： x 包含 1.0/宽度 y 包含 1.0/高度 z 包含宽度 w 包含高度 纹理 HDR 参数 {TextureName}_HDR - 一个 float4 属性，其中包含有关如何根据所使用的颜色空间解码潜在 HDR（例如 RGBM 编码）纹理的信息。请参阅 UnityCG.cginc 着色器 include 文件中的 DecodeHDR 函数。 颜色空间和颜色/矢量着色器数据 使用线性颜色空间时，所有材质颜色属性均以 sRGB 颜色提供，但在传递到着色器时会转换为线性值。 例如，如果 Properties 着色器代码块包含名为“MyColor“的 Color 属性，则相应的”MyColor”HLSL 变量将获得线性颜色值。 对于标记为 Float 或 Vector 类型的属性，默认情况下不进行颜色空间转换；而是假设它们包含非颜色数据。可为浮点/矢量属性添加 [Gamma] 特性，以表示它们是以 sRGB 空间指定，就像颜色一样 内置宏 Unity在编译着色器程序时会定义几个预处理器宏。 内置着色器 helper 函数 Unity 具有许多内置实用函数，旨在使编写着色器更简单，更轻松。 内置着色器变量 Unity 的内置文件包含着色器的全局变量：当前对象的变换矩阵、光源参数、当前时间等等。就像任何其他变量一样，可在着色器程序中使用这些变量，但如果已经包含相关的 include 文件，则不必声明这些变量。参见官网 着色器数据类型和精度 Unity 中的标准着色器语言为 HLSL，支持一般 HLSL 数据类型。但是，Unity 对 HLSL 类型有一些补充，特别是为了在移动平台上提供更好的支持。 基本数据类型 着色器中的大多数计算是对浮点数（在 C# 等常规编程语言中为 float）进行的。浮点类型有几种变体：float、half 和 fixed（以及它们的矢量/矩阵变体，比如 half3 和 float4x4）。这些类型的精度不同（因此性能或功耗也不同）： 高精度：float 最高精度浮点值；一般是 32 位（就像常规编程语言中的 float）。 完整的 float 精度通常用于世界空间位置、纹理坐标或涉及复杂函数（如三角函数或幂/取幂）的标量计算。 中等精度：half 中等精度浮点值；通常为 16 位（范围为 –60000 至 +60000，精度约为 3 位小数）。 半精度对于短矢量、方向、对象空间位置、高动态范围颜色非常有用。 低精度：fixed 最低精度的定点值。通常是 11 位，范围从 –2.0 到 +2.0，精度为 1/256。 固定精度对于常规颜色（通常存储在常规纹理中）以及对它们执行简单运算非常有用。 整数数据类型 整数（int 数据类型）通常用作循环计数器或数组索引。为此，它们通常可以在各种平台上正常工作。 根据平台的不同，GPU 可能不支持整数类型。例如，Direct3D 9 和 OpenGL ES 2.0 GPU 仅对浮点数据进行运算，并且可以使用相当复杂的浮点数学指令来模拟简单的整数表达式（涉及位运算或逻辑运算）。 Direct3D 11、OpenGL ES 3、Metal 和其他现代平台都对整数数据类型有适当的支持，因此使用位移位和位屏蔽可以按预期工作。 复合矢量/矩阵类型 HLSL 具有从基本类型创建的内置矢量和矩阵类型。例如，float3 是一个 3D 矢量，具有分量 .x、.y 和 .z，而 half4 是一个中等精度 4D 矢量，具有分量 .x、.y、.z 和 .w。或者，可使用 .r、.g、.b 和 .a 分量来对矢量编制索引，这在处理颜色时很有用。 矩阵类型以类似的方式构建；例如 float4x4 是一个 4x4 变换矩阵。请注意，某些平台仅支持方形矩阵，最主要的是 OpenGL ES 2.0。 纹理/采样器类型 通常按照如下方式在 HLSL 代码中声明纹理： 12sampler2D _MainTex;samplerCUBE _Cubemap; 对于移动平台，这些转换为“低精度采样器”，即纹理中应该有低精度数据。 您可以使用着色器精度模型下拉菜单在Player Setting中更改整个Unity项目的默认采样器精度。 如果您知道您的纹理包含HD 颜色，您可能需要使用半精度采样器： 12sampler2D_half _MainTex;samplerCUBE_half _Cubemap; 或者，如果纹理包含完整浮点精度数据（例如深度纹理），请使用完整精度采样器： 12sampler2D_float _MainTex;samplerCUBE_float _Cubemap; 精度、硬件支持和性能 使用 float/half/fixed 数据类型的一个难题是：PC GPU 始终为高精度。也就是说，对于所有 PC (Windows/Mac/Linux) GPU，在着色器中编写 float、half 还是 fixed 数据类型都无关紧要。这些 GPU 将始终以 32 位浮点精度来计算所有数据。 仅当目标平台是移动端 GPU 时，half 和 fixed 类型才变得重要，在这种情况下，这些类型主要面临功耗（有时候是性能）约束。请记住，要确认是否遇到精度/数值问题，必须在移动设备上测试着色器。 即使在移动端 GPU 上，不同的精度支持也会因 GPU 产品系列而异。 大多数现代移动端 GPU 实际上只支持 32 位数字（用于 float 类型）或 16 位数字（用于 half 和 fixed 类型）。一些较旧的 GPU 对顶点着色器和片元着色器计算具有不同的精度。 使用较低的精度通常可以更快，这可能是由于改进的 GPU 寄存器分配，或是由于某些低精度数学运算的特殊“快速路径”执行单元。即使没有原始性能优势，使用较低的精度通常也会降低 GPU 的功耗，从而延长电池续航时间。 一般的经验法则是全部都从半精度开始（但位置和纹理坐标除外）。仅当半精度对于计算的某些部分不足时，才增加精度。 支持无穷大、非数字和其他特殊浮点值 对特殊浮点值的支持可能会有所不同，具体取决于运行的 GPU 产品系列（主要是移动端）。 支持 Direct3D 10 的所有 PC GPU 都支持非常明确的 IEEE 754 浮点标准。这意味着，在 CPU 上，浮点数的行为与常规编程语言完全相同。 移动端 GPU 的支持程度可能稍有不同。在某些移动端 GPU 中，将零除以零可能会导致 NaN（“非数字”）；在其他移动端 GPU 上，它可能会导致无穷大、零或任何其他不明值。务必在目标设备上测试着色器以检查着色器是否受支持。 使用采样器状态 耦合的纹理和采样器 大多数时候，在着色器中采样纹理时，纹理采样状态应该来自纹理设置——本质上，纹理和采样器是耦合在一起的。这是使用 DX9 样式着色器语法时的默认行为： 123sampler2D _MainTex;&#x2F;&#x2F; ...half4 color &#x3D; tex2D(_MainTex, uv); 使用 HLSL 关键字 sampler2D、sampler3D 和 samplerCUBE 可声明纹理和采样器。 大部分情况下，这是您想要的结果，而且在较旧的图形 API (OpenGL ES) 中，这是唯一受支持的选项。 单独的纹理和采样器 许多图形 API 和 GPU 允许使用比纹理更少的采样器，并且耦合的纹理+采样器语法可能不允许编写更复杂的着色器。例如，Direct3D 11 允许在单个着色器中使用多达 128 个纹理，但最多只能使用 16 个采样器。 Unity 允许使用 DX11 风格的 HLSL 语法来声明纹理和采样器，但需要通过一个特殊的命名约定来让它们匹配：名称为“sampler”+TextureName 格式的采样器将从该纹理中获取采样状态。 以上部分中的着色器代码片段可以用 DX11 风格的 HLSL 语法重写，并且也会执行相同的操作： 1234Texture2D _MainTex;SamplerState sampler_MainTex; &#x2F;&#x2F;&quot;sampler&quot;+&quot;_MainTex&quot;&#x2F;&#x2F; ...half4 color &#x3D; _MainTex.Sample(sampler_MainTex, uv); 但是，通过这种方式，可以编写着色器以“重用”来自其他纹理的采样器，同时采样多个纹理。在下面的示例中，对三个纹理进行了采样，但所有这些都只使用了一个采样器： 12345678exture2D _MainTex;Texture2D _SecondTex;Texture2D _ThirdTex;SamplerState sampler_MainTex; &#x2F;&#x2F;&quot;sampler&quot;+&quot;_MainTex&quot;&#x2F;&#x2F; ...half4 color &#x3D; _MainTex.Sample(sampler_MainTex, uv);color +&#x3D; _SecondTex.Sample(sampler_MainTex, uv);color +&#x3D; _ThirdTex.Sample(sampler_MainTex, uv); 但是请注意，DX11 风格的 HLSL 语法在某些较旧的平台（例如，OpenGL ES 2.0）上无效.您可能需要指定 #pragma target 3.5（请参阅着色器编译目标）以避免较旧的平台使用着色器。 Unity 提供了一些着色器宏帮助您使用这种“单独采样器”方法来声明和采样纹理，请参阅内置宏。以上示例可以采用所述的宏重写为下列形式： 1234567UNITY_DECLARE_TEX2D(_MainTex);UNITY_DECLARE_TEX2D_NOSAMPLER(_SecondTex);UNITY_DECLARE_TEX2D_NOSAMPLER(_ThirdTex);&#x2F;&#x2F; ...half4 color &#x3D; UNITY_SAMPLE_TEX2D(_MainTex, uv);color +&#x3D; UNITY_SAMPLE_TEX2D_SAMPLER(_SecondTex, _MainTex, uv);color +&#x3D; UNITY_SAMPLE_TEX2D_SAMPLER(_ThirdTex, _MainTex, uv); 以上代码将在 Unity 支持的所有平台上进行编译，但会在 DX9 等旧平台上回退到使用三个采样器。 内联采样器状态 除了能识别名为“sampler”+TextureName 的 HLSL SamplerState 对象，Unity 还能识别采样器名称中的某些其他模式。这对于直接在着色器中声明简单硬编码采样状态很有用。例如： 1234Texture2D _MainTex;SamplerState my_point_clamp_sampler;&#x2F;&#x2F; ...half4 color &#x3D; _MainTex.Sample(my_point_clamp_sampler, uv); 名称 “my_point_clamp_sampler”将被识别为应该使用点纹理过滤和钳制纹理包裹模式的采样器。采样器名称被识别为“内联”采样器状态（全都不区分大小写）： *“Point”、“Linear”或“Trilinear”（必需）设置纹理过滤模式。 *“Clamp”、“Repeat”、“Mirror”或“MirrorOnce”（必需）设置纹理包裹模式。可根据每个轴 (UVW) 来指定包裹模式，例如\"ClampU_RepeatV\"。 *“Compare”（可选）设置用于深度比较的采样器；与 HLSL SamplerComparisonState 类型和 SampleCmp/SampleCmpLevelZero 函数配合使用。 就像单独的纹理 + 采样器语法一样，某些平台不支持内联采样器状态。目前它们在 Direct3D 11/12 和 Metal 上实现。请注意，大多数移动 GPU/API 不支持“MirrorOnce”纹理环绕模式，并且在不支持时将回退到镜像模式。 着色器加载 默认情况下，Unity 的运行时着色器加载行为如下： 1.当 Unity 加载场景或使用在运行时加载资源功能加载内容时，它会将所有需要的 Shader 对象加载到 CPU 内存。 2.Unity 第一次需要使用着色器变体渲染几何体时，它将该变体的数据传递给图形驱动程序。图形驱动程序在 GPU 上创建该变体的表示，并执行平台所需的任何其他工作。 这种行为的好处是着色器变体没有前期的 GPU 内存使用或加载时间。缺点是第一次使用变体时可能会出现明显的停顿，因为图形驱动程序必须在 GPU 上创建着色器程序并执行任何额外的工作。 预热着色器变体 为避免在性能开销大时出现明显的停顿，Unity 可以要求图形驱动程序在首次需要着色器变体之前创建它们的 GPU 表示形式。这称为预热。 警告：在选择如何执行预热之前，请查看有关图形 API 支持的说明。在 DX12、Vulkan 和 Metal 等现代图形 API 上，只有实验性的 ShaderWarmup API 完全支持，因为它允许您指定顶点格式。 您可以按照以下方式执行预热： 使用实验性 ShaderWarmup API 预热给定的 Shader 对象或着色器变体集合。 在应用程序启动时预热着色器变体集合，方法是将它们添加到 Preloaded shaders section of the Graphics Settings 窗口。 使用 ShaderVariantCollection.WarmUp API 预热着色器变体集合。 使用 Shader.WarmupAllShaders API 预热当前内存中的所有 Shader 对象的所有变体。 用于着色器加载的性能分析器标记 性能分析器标记 Shader.Parse 用于表示 Unity 将创建的着色器变量数据表示发送到 GPU。性能分析器标记 CreateGPUProgram 用于表示将着色器程序上传到 GPU 并等待 GPU 执行任何所需工作。 平台差异 在某些情况下，不同图形 API 之间的图形渲染行为方式存在差异。大多数情况下，Unity 编辑器会隐藏这些差异，但在某些情况下，编辑器无法为您执行此操作。下面列出了这些情况以及发生这些情况时需要采取的操作。 渲染纹理坐标 垂直纹理坐标约定在两种类型的平台之间有所不同，分别是 Direct3D 类和 OpenGL 类平台。 Direct3D 类：顶部坐标为 0 并向下增加。此类型适用于 Direct3D、Metal 和游戏主机。 OpenGL 类：底部坐标为 0 并向上增加。此类适用于 OpenGL 和 OpenGL ES。 除了渲染到渲染纹理的情况下，这种差异不会对您的项目产生任何影响。在 Direct3D 类平台上渲染到纹理时，Unity 会在内部上下翻转渲染。这样就会使坐标约定在平台之间匹配，并以 OpenGL 类平台约定作为标准。 在着色器中，有两种常见情况需要您采取操作确保不同的坐标约定不会在项目中产生问题，这两种情况就是图像效果和 UV 空间中的渲染。 图像效果 使用图像效果(OnRenderImage)和抗锯齿时，系统不会翻转为图像效果生成的源纹理来匹配OpenGL类平台约定。在这种情况下，Unity渲染到屏幕以获得抗锯齿效果，然后将渲染解析为渲染纹理，以便通过图像效果进行进一步处理。 如果您的图像效果是一次处理一个渲染纹理的简单图像效果，则Graphics.Blit会处理不一致的坐标。但是，如果您在图像效果中一起处理多个渲染纹理，则在Direct3D类平台中以及在您使用抗锯齿时，渲染纹理很可能以不同的垂直方向出现。要标准化坐标，必须在顶点着色器中手动上下“翻转”屏幕纹理，使其与OpenGL类坐标标准匹配。 以下代码示例演示了如何执行此操作： 12345678&#x2F;&#x2F; 翻转纹理的采样：&#x2F;&#x2F; 主纹理的&#x2F;&#x2F; 纹理像素大小将具有负 Y。# if UNITY_UV_STARTS_AT_TOPif (_MainTex_TexelSize.y &lt; 0) uv.y &#x3D; 1-uv.y;# endif 在UV空间中渲染 在纹理坐标 (UV) 空间中渲染特殊效果或工具时，您可能需要调整着色器，以便在 Direct3D 类和 OpenGL 类系统之间进行一致渲染。您还可能需要在渲染到屏幕和渲染到纹理之间进行渲染调整。为进行此类调整，应上下翻转 Direct3D 类投影，使其坐标与 OpenGL 类投影坐标相匹配。 内置变量 ProjectionParams.x 包含值 +1 或 –1。-1 表示投影已上下翻转以匹配 OpenGL 类投影坐标，而 +1 表示尚未翻转。 您可以在着色器中检查此值，然后执行不同的操作。下面的示例将检查是否已翻转投影，如果已翻转，则再次进行翻转，然后返回 UV 坐标以便匹配。 123456789101112float4 vert(float2 uv : TEXCOORD0) : SV_POSITION&#123; float4 pos; pos.xy &#x3D; uv; &#x2F;&#x2F; 此示例使用上下翻转的投影进行渲染， &#x2F;&#x2F; 因此也翻转垂直 UV 坐标 if (_ProjectionParams.x &lt; 0) pos.y &#x3D; 1 - pos.y; pos.z &#x3D; 0; pos.w &#x3D; 1; return pos;&#125; 裁剪空间坐标 与纹理坐标类似，裁剪空间坐标（也称为投影后空间坐标）在 Direct3D 类和 OpenGL 类平台之间有所不同： Direct3D 类：裁剪空间深度从近平面的 +1.0 到远平面的 0.0。此类型适用于 Direct3D、Metal 和游戏主机。 OpenGL 类：裁剪空间深度从近平面的 –1.0 到远平面的 +1.0。此类适用于 OpenGL 和 OpenGL ES。 在着色器代码内，可使用内置宏 UNITY_NEAR_CLIP_VALUE 来获取基于平台的近平面值。 在脚本代码内，使用 GL.GetGPUProjectionMatrix 将 Unity 的坐标系（遵循 OpenGL 类约定）转换为 Direct3D 类坐标（如果这是平台所期望的）。 着色器计算的精度 要避免精度问题，请确保在目标平台上测试着色器。移动设备和 PC 中的 GPU 在处理浮点类型方面有所不同。PC GPU 将所有浮点类型（浮点精度、半精度和固定精度）视为相同；PC GPU 使用完整 32 位精度进行所有计算，而许多移动设备 GPU 并不是这样做。 着色器中的 const 声明 const 的使用在 Microsoft HSL（请参阅 msdn.microsoft.com）和 OpenGL 的 GLSL（请参阅 Wikipedia）着色器语言之间有所不同。 Microsoft 的 HLSL const 与 C# 和 C++ 中的含义大致相同：声明的变量在其作用域内是只读的，但可按任何方式初始化。 OpenGL 的 GLSL const 表示变量实际上是编译时常量，因此必须使用编译时约束（文字值或其他对于 const 的计算）进行初始化。 最好是遵循 OpenGL 的 GLSL 语义，并且只有当变量真正不变时才将变量声明为 const。避免使用其他一些可变值初始化 const 变量（例如，作为函数中的局部变量）。这一原则也适用于 Microsoft 的 HLSL，因此以这种方式使用 const 可以避免在某些平台上混淆错误。 着色器使用的语义 要让着色器在所有平台上运行，一些着色器值应该使用以下语义： 顶点着色器输出（裁剪空间）位置：SV_POSITION。有时，着色器使用 POSITION 语义来使着色器在所有平台上运行。请注意，这不适用于 Sony PS4 或有曲面细分的情况。 片元着色器输出颜色：SV_Target。有时，着色器使用COLOR或COLOR0来使着色器在所有平台上运行。请注意，这不适用于 Sony PS4。 将网格渲染为点时，从顶点着色器输出 PSIZE 语义（例如，将其设置为 1）。某些平台（如 OpenGL ES 或 Metal）在未从着色器写入点大小时会将点大小视为“未定义”。 Direct3D着色器编译器语法 Direct3D 平台使用 Microsoft 的 HLSL 着色器编译器。对于各种细微的着色器错误，HLSL 编译器比其他编译器更严格。例如，它不接受未正确初始化的函数输出值。 使用此编译器时，您可能遇到的最常见情况是： 部分初始化的值。例如，函数返回 float4，但代码只设置它的.xyz 值。如果只需要三个值，请设置所有值或更改为 float3。 在顶点着色器中使用 tex2D。这是无效的，因为顶点着色器中不存在 UV 导数。这种情况下，您需要采样显式 Mip 级别；例如，使用 tex2Dlod (tex, float4(uv,0,0))。此外，还需要添加 #pragma target 3.0，因为 tex2Dlod 是着色器模型 3.0 的功能。 使用着色器帧缓冲提取 一些 GPU（最明显的是 iOS 上基于 PowerVR 的 GPU）允许您通过提供当前片元颜色作为片元着色器的输入来进行某种可编程混合（请参阅 khronos.org 上的 EXT_shader_framebuffer_fetch）。 可在 Unity 中编写使用帧缓冲提取功能的着色器。要执行此操作，请在使用 HLSL（Microsoft 的着色语言，请参阅 msdn.microsoft.com）或 Cg（Nvidia 的着色语言，请参阅 nvidia.co.uk）编写片元着色器时使用 inout 颜色参数。 以下示例采用的是 Cg 语言。 123456789101112CGPROGRAM&#x2F;&#x2F; 只为可能支持该功能的平台（目前是 gles、gles3 和 metal）&#x2F;&#x2F; 编译着色器# pragma only_renderers framebufferfetchvoid frag (v2f i, inout half4 ocol : SV_Target)&#123; &#x2F;&#x2F; ocol 可以被读取（当前帧缓冲区颜色） &#x2F;&#x2F; 并且可以被写入（将颜色更改为该颜色） &#x2F;&#x2F; ...&#125; ENDCG 着色器中的深度 (Z) 方向 深度 (Z) 方向在不同的着色器平台上不同。 DirectX 11, DirectX 12, Metal: 反转方向 深度 (Z) 缓冲区在近平面处为 1.0，在远平面处减小到 0.0。 裁剪空间范围是 [near,0]（表示近平面处的近平面距离，在远平面处减小到 0.0）。 其他平台：传统方向 深度 (Z) 缓冲区值在近平面处为 0.0，在远平面处为 1.0。 裁剪空间取决于具体平台： 在 Direct3D 类平台上，范围是 [0,far]（表示在近平面处为 0.0，在远平面处增加到远平面距离）。 在 OpenGL 类平台上，范围是 [-near,far]（表示在近平面处为负的近平面距离，在远平面处增加到远平面距离）。 请注意，使反转方向深度 (Z) 与浮点深度缓冲区相结合，可显著提高相对于传统方向的深度缓冲区精度。这样做的优点是降低 Z 坐标的冲突并改善阴影，特别是在使用小的近平面和大的远平面时。 因此，在使用深度 (Z) 发生反转的平台上的着色器时： 定义了 UNITY_REVERSED_Z。 _CameraDepth 纹理的纹理范围是 1（近平面）到 0（远平面）。 裁剪空间范围是“near”（近平面）到 0（远平面）。 但是，以下宏和函数会自动计算出深度 (Z) 方向的任何差异： - Linear01Depth(float z) - LinearEyeDepth(float z) - UNITY_CALC_FOG_FACTOR(coord) 提取深度缓冲区 如果要手动提取深度 (Z) 缓冲区值，则可能需要检查缓冲区方向。以下是执行此操作的示例： 1234float z &#x3D; tex2D(_CameraDepthTexture, uv);# if defined(UNITY_REVERSED_Z) z &#x3D; 1.0f - z;# endif 使用裁剪空间 如果要手动使用裁剪空间 (Z) 深度，则可能还需要使用以下宏来抽象化平台差异： float clipSpaceRange01 = UNITY_Z_0_FAR_FROM_CLIPSPACE(rawClipSpace); 注意：此宏不会改变 OpenGL 或 OpenGL ES 平台上的裁剪空间，因此在这些平台上，此宏返回“-near”1（近平面）到 far（远平面）之间的值。 投影矩阵 如果处于深度 (Z) 发生反转的平台上，则 GL.GetGPUProjectionMatrix() 返回一个还原了 z 的矩阵。 但是，如果要手动从投影矩阵中进行合成（例如，对于自定义阴影或深度渲染），您需要通过脚本按需自行还原深度 (Z) 方向。 以下是执行此操作的示例： 123456789101112131415161718var shadowProjection &#x3D; Matrix4x4.Ortho(...); &#x2F;&#x2F;阴影摄像机投影矩阵var shadowViewMat &#x3D; ... &#x2F;&#x2F;阴影摄像机视图矩阵var shadowSpaceMatrix &#x3D; ... &#x2F;&#x2F;从裁剪空间到阴影贴图纹理空间 &#x2F;&#x2F;当引擎通过摄像机投影计算设备投影矩阵时，&#x2F;&#x2F;&quot;m_shadowCamera.projectionMatrix&quot;被隐式反转m_shadowCamera.projectionMatrix &#x3D; shadowProjection; &#x2F;&#x2F;&quot;shadowProjection&quot;在连接到&quot;m_shadowMatrix&quot;之前被手动翻转，&#x2F;&#x2F;因为它被视为着色器的其他矩阵。if(SystemInfo.usesReversedZBuffer) &#123; shadowProjection[2, 0] &#x3D; -shadowProjection[2, 0]; shadowProjection[2, 1] &#x3D; -shadowProjection[2, 1]; shadowProjection[2, 2] &#x3D; -shadowProjection[2, 2]; shadowProjection[2, 3] &#x3D; -shadowProjection[2, 3];&#125;m_shadowMatrix &#x3D; shadowSpaceMatrix * shadowProjection * shadowViewMat; 深度 (Z) 偏差 Unity自动处理深度(Z)偏差，以确保其与Unity的深度(Z)方向匹配。但是，如果要使用本机代码渲染插件，则需要在C或C++代码中消除（反转）深度(Z)偏差。 深度 (Z) 方向检查工具 使用SystemInfo.usesReversedZBuffer可确认所在平台是否使用反转深度(Z)。 性能分析与调试 优化着色器运行时性能 不同的平台具有截然不同的性能；与低端移动端 GPU 相比，高端 PC GPU 在图形和着色器方面的处理能力要高得多。即使在单一平台上也是如此；快速的 GPU 比慢速的集成 GPU 快几十倍。 移动平台和低端 PC 上的 GPU 性能可能远低于您的开发机器上的性能。 建议您手动优化着色器以减少计算和纹理读取，以便在低端 GPU 机器上获得良好的性能。 仅执行所需的计算 着色器代码需要执行的计算和处理越多，它对游戏性能的影响就越大。例如，支持每种材质的颜色可以使着色器更加灵活，但如果始终将该颜色设置为白色，则会对屏幕上渲染的每个顶点或像素执行无用的计算。 计算的频率也会影响游戏的性能。通常，与顶点数（顶点着色器执行次数）相比，渲染的像素数会更多（因此像素着色器执行次数也更多），而渲染的顶点数比渲染的对象更多。在可能的情况下，可将计算从像素着色器代码移动到顶点着色器代码中，或者将它们完全移出着色器并在脚本中设置值。 计算的精度 用 Cg/HLSL 编写着色器时，有三种基本数字类型：float、half 和 fixed。好的性能总是应该尽可能的使用更低的精度。这在低端设备上特别重要。好的经验法则是： 对于世界空间位置和纹理坐标，请使用 float 精度。 对于所有其他情况（矢量、HDR 颜色等），请首先尝试half精度。仅在必要的情况下再提高精度。 要对纹理数据进行非常简单的运算，请使用fixed精度。 实际上，具体应该使用哪种数字类型取决于平台和GPU。一般来说： - 所有新款的桌面端GPU将始终以完整float精度进行所有计算，因此float/half/fixed最终产生完全相同的结果。这可能会使测试变得困难，因为更难以确定half/fixed精度是否真正够用，因此请始终在目标设备上测试着色器以获得准确的结果。 - 移动端GPU实际支持half精度。这种精度通常速度更快，并且使用更少的性能来执行计算。 - Fixed精度通常仅对于较旧的移动端GPU有用。大部分新款GPU（可运行 OpenGL ES 3 或 Metal的GPU）在内部以相同方式来处理fixed和half精度。 复杂的数学计算 复杂的数学函数（例如 pow、exp、log、cos、sin、tan）非常耗费资源，因此请尽可能避免在低端硬件上使用它们。 如果适用，请考虑使用查找纹理作为复杂数学计算的替代方法。 避免编写自己的运算（如 normalize、dot、inversesqrt）。Unity 的内置选项确保驱动程序可以生成好得多的代码。请记住，Alpha 测试 (discard) 运算通常会使片元着色器变慢。 Alpha测试 固定函数 AlphaTest（或者其可编程的等效函数 clip()）在不同平台上具有不同的性能特征： 通常，在使用该函数来移除大多数平台上的完全透明像素时，可获得少量优势。 但是，在iOS和某些Android设备的PowerVR GPU上，Alpha测试是资源密集型任务。不要试图在这些平台上使用这种测试进行性能优化，因为它会导致游戏运行速度比平常慢。 颜色遮罩(Color Mask) 在某些平台（主要是 iOS 和 Android 设备的移动端 GPU）上，使用 ColorMask 省略一些通道（例如 ColorMask RGB）可能是资源密集型的操作，所以除非绝对需要，否则请不要使用。 参考 Unity官方手册 着色器语言 学习OpenGL URP官方文档 Anti-aliased Alpha Test: The Esoteric Alpha To Coverage New shader preprocessor","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"Shader","slug":"Shader","permalink":"http://yoursite.com/tags/Shader/"}]},{"title":"六边形网格","slug":"游戏/Map/六边形网格","date":"2021-11-29T04:00:02.000Z","updated":"2025-04-26T11:06:24.134Z","comments":true,"path":"2021/11/29/游戏/Map/六边形网格/","link":"","permalink":"http://yoursite.com/2021/11/29/%E6%B8%B8%E6%88%8F/Map/%E5%85%AD%E8%BE%B9%E5%BD%A2%E7%BD%91%E6%A0%BC/","excerpt":"这篇文章覆盖了制作六边形地图所涉及到相关的方法，通用公式和算法。为了文章更加优雅简洁，文中的实例都将使用伪代码进行描述。 几何性质 六边形是由六个边组成的。正六边形的每条边长度相同。我们这里假设所有的六边形都是正六边形。六边形的方向有两个： 1. 垂直列方向(六边形的任意两条平行边在垂直方向上) 水平行反向(六边形的任意两条平行边在水平方向上) 六边形有6个边和6个角。每个边被两个六边形共享。每个角被三个六边形共享。更多关于中心点(centers), 边(sides), 和 角(corners)可以参见这篇文章（四边形，六边性和三角形）。","text":"这篇文章覆盖了制作六边形地图所涉及到相关的方法，通用公式和算法。为了文章更加优雅简洁，文中的实例都将使用伪代码进行描述。 几何性质 六边形是由六个边组成的。正六边形的每条边长度相同。我们这里假设所有的六边形都是正六边形。六边形的方向有两个： 1. 垂直列方向(六边形的任意两条平行边在垂直方向上) 水平行反向(六边形的任意两条平行边在水平方向上) 六边形有6个边和6个角。每个边被两个六边形共享。每个角被三个六边形共享。更多关于中心点(centers), 边(sides), 和 角(corners)可以参见这篇文章（四边形，六边性和三角形）。 角度 一个正六边的内角是\\(120^°\\)。有六个“楔形”，每个都是一个内角为\\(60^°\\)的等边三角形。每个角到中心的距离相等定义为它的大小。 使用以下代码去绘制一个六边形： 123456function pointy_hex_corner(center, size, i): var angle_deg = 60 * i //垂直方向 //var angle_deg = 60 * i - 30°//水平方向 var angle_rad = PI / 180 * angle_deg return Point(center.x + size * cos(angle_rad), center.y + size * sin(angle_rad)) 通过pointy_hex_corner(..., 0) 到pointy_hex_corner(..., 5)分别计算出六边形的6个顶点。然后绘制线连接各个顶点，则形成了六边形的外框了。 两个不同方向的六边形的角度是不一样的。 1. 垂直方向的角度是：\\(0^°\\)，\\(60^°\\)，\\(120^°\\)，\\(180^°\\)，\\(240^°\\)，\\(300^°\\) 水平方向的角度是：\\(30^°\\)，\\(90^°\\)，\\(150^°\\)，\\(210^°\\)，\\(270^°\\)，\\(330^°\\) 说明：图中的y轴向下，角度的正方向是顺时针的。如果你y轴是向上的，角度的正方向是逆时针的，则需要做相应的调整。 在数学上，六边形的外接圆半径就是从中心到顶点的距离，我们称之为六边形的大小；内接圆半径是从中心点到边的距离。“最大直径”是外接圆半径的两倍，“最小直径”是内接圆半径的两倍。 大小(size)和间距(Spacing) 现在我们将多个六边形放在一起。其中size被定义为从中心点到任意角的距离。 1. 在垂直方向上(平直边平行于水平轴)，六边形的宽w = 2 * size, 高h = \\(2 * \\frac{\\sqrt{3}}{2} * size\\) = \\(\\sqrt{3} * size\\)(其中\\(\\sqrt{3}\\)是根据\\(sin(60^°)\\)而来)。两个相邻六边形中心点水平之间的距离被定义为水平间距（horizontal spacing）= \\(w * \\frac{3}{4}\\), 两中心点垂直方向的间距被定义为垂直间距（vertical spacing）= h。如下图： 在水平方向上(平直边平行于垂直轴)，六边形的宽w = \\(\\sqrt{3} * size\\),高h = 2 * size。两个相邻六边形中心点水平之间的距离被定义为水平间距（horizontal spacing）= w, 两中心点垂直方向的间距被定义为垂直间距（vertical spacing）= \\(h * \\frac{3}{4}\\) 一些游戏使用像素风格可能导像素数不能匹配到真实的正多边形上。在这个章节描述角度和间距可能不能够匹配到你的六边形大小。这篇文章的其余部分描述了六边形在六边形网格上的算法，即使你的六边形被拉伸或收缩了一点，我也会在这篇文章说明如何处理拉伸。 坐标系统 现在我们将六边形组装成一个网格。 对于方形网格，方法就比较明显了，可以将六边形逐行拼接即可。 对于六边形来说，有多种方法。我喜欢用于算法的立方体坐标（Cube coordinates）和轴向坐标（Axial coordinates）或用于存储的双倍坐标（Doubled coordinates）。 偏移坐标（Offset coordinates） 偏移坐标是将正六变形在每一列或行进行偏移组成的一个网格，列被命名为col(q), 行被命名为row(r)。我们可以选择偏移奇或偶（列/行），所以在水平或垂直方向上又有两种变体（奇偏移或偶偏移）。如下图： 立方体坐标（Cube coordinates） 另一种观察六边形网格的方式是六边形定义有三个轴，不像偏移坐标这种只有两个轴的方形网格，立方体坐标多了一些对称美。 让我们取一个立方体网格并在x + y + z = 0的位置切出一个对角平面。如下图： 这虽然是个奇怪的想法，但它可以帮助我们使用六边形网格算法： 1. 在3D笛卡尔积坐标系中，有一些标准的向量操作：加或减，乘或除等运算。我们能在六边形网格中使用这些操作。偏移坐标不支持这些操作。 2. 在3D笛卡尔积坐标系中，有一些现存的算法，比如：距离，旋转，反射，绘线，转换坐标等。这些算法也是适用于六边形网格。 立方体坐标是六边形网格坐标系的合理选择。强制约束q + r + s = 0，所以算法必须保证这一约束。该约束还确保每个六边形都有一个规范坐标（q+r+s=0）。 轴向坐标（Axial coordinates） 轴向坐标和立方体坐标是相同的，轴向坐标中我们不用存储s轴坐标的值，因为我们限制q+r+s=0, 当我们需要s时，可以通过s=-q-r得到。 轴向/立方体坐标，允许在六边形网格中使用加，减，乘和除。偏移坐标不允许这样做，这使得使用轴向/立方体坐标在算法使用上更简单。 双倍坐标（Doubled coordinates） 虽然我推荐使用轴向坐标（Axial coordinates）和立方体坐标（Cube coordinates）,如果你还是执意于偏移坐标（Offset coordinates），那么可以考虑的它的另一种形式即双倍坐标（Doubled coordinates）。它能确保大部分的算法更加容易实现。双倍坐标不是交替，而是水平或垂直步长是偏移坐标的两倍。它有个限制即：(col+row)%2==0。在水平（尖顶在上）布局中，每个六边形将列增加2；在垂直（平顶在上）布局中，每个六边形将行增加2。这允许介于两者之间的六边形使用中间值。如图： 推荐 推荐什么？ 我推荐：如果你仅打算使用没有旋转的矩形地图，考虑使用双倍坐标（Doubled coordinates）或偏移坐标（Offset coordinates）。对于有旋转或非矩形形状的地图使用轴向坐标（Axial coordinates）或立方体坐标（Cube coordinates）。对于立方体坐标你可以选择存储s（立方体坐标）或在需要时通过-q-r计算得到（轴向坐标）。 坐标转换 您可能会在项目中使用轴向或偏移坐标，但许多算法更容易以轴向/立方体坐标表示。 因此，您需要能够来回转换。 轴向坐标 轴向和立方体坐标本质上是相同的。在立方体坐标统中，我们存储第三个轴，s。在轴向坐标统中，我们不存储它，但在我们需要时不得不通过，s=-q-r进行计算。 12345678910function cube_to_axial(cube): var q = cube.q var r = cube.r return Hex(q,r)function axial_to_cube(hex): var q = hex.q var r = hex.r var s = -q-r return Cube(q, r, s) 偏移坐标 偏移坐标转换，首先需要确定使用的是-r（尖顶在上）还是-q（平顶在上）。这两种坐标系的转换是不同的： 123456789101112131415161718192021222324252627282930313233343536373839404142// axial from/to oddrfunction axial_to_oddr(hex): var col = hex.q + (hex.r - (hex.r &amp; 1)) / 2 var row = hex.r return OffsetCoord(col, row)function oddr_to_axial(hex): var q = hex.col - (hex.row - (hex.row &amp; 1)) / 2 var r = hex.row return Hex(q,r)// axial from/to evenrfunction axial_to_evenr(hex): var col = hex.q + (hex.r - (hex.r &amp; 1)) / 2 var row = hex.r return OffsetCoord(col, row)function evenr_to_axial(hex): var q = hex.col - (hex.row - (hex.row &amp; 1)) / 2 var r = hex.row return Hex(q,r)// axial from/to oddqfunction axial_to_oddq(hex): var col = hex.q var row = hex.r + (hex.q - (hex.q&amp;1)) / 2 return OffsetCoord(col, row)function oddq_to_axial(hex): var q = hex.col var r = hex.row - (hex.col - (hex.col&amp;1)) / 2 return Hex(q, r)// axial from/to evenqfunction axial_to_evenq(hex): var col = hex.q var row = hex.r + (hex.q + (hex.q&amp;1)) / 2 return OffsetCoord(col, row)function evenq_to_axial(hex): var q = hex.col var r = hex.row - (hex.col + (hex.col&amp;1)) / 2 return Hex(q, r) 上述代码中使用了位运算&amp;对2进行求模运算 双倍坐标 12345678910111213141516171819function doubleheight_to_axial(hex): var q = hex.col var r = (hex.row - hex.col) / 2 return Hex(q, r)function axial_to_doubleheight(hex): var col = hex.q var row = 2 * hex.r + hex.q return DoubledCoord(col, row)function doublewidth_to_axial(hex): var q = (hex.col - hex.row) / 2 var r = hex.row return Hex(q, r)function axial_to_doublewidth(hex): var col = 2 * hex.q + hex.r var row = hex.r return DoubledCoord(col, row) 如果你使用的是立方体坐标，使用立方体坐标Cube（q, r, -q-r）代替轴向坐标Hex(q, r)。 相邻六边形（Neighbors） 给定一个六边形，哪6个六边形与它相邻？ 如您所料，立方体坐标的答案是最简单的，轴向坐标仍然非常简单，而偏移坐标则稍微复杂一些。我们可能还想计算6个“对角线”六边形。 立方体坐标 在六边形坐标中移动一个格子涉及改变立方体坐标中3个分量中的其中一个+1且另一个-1（必须保证和为0）。立方体坐标中3个轴都有可能+1，剩下的2个轴可能-1。在相邻的6个六边形中，每个相当于六边形的6个方向。最简单和最快速的方法是预先计算存储它们在一个表中： 12345678910111213var cube_direction_vectors = [ Cube(+1, 0, -1), Cube(+1, -1, 0), Cube(0, -1, +1), Cube(-1, 0, +1), Cube(-1, +1, 0), Cube(0, +1, -1), ]function cube_direction(direction): return cube_direction_vectors[direction]function cube_add(hex, vec): return Cube(hex.q + vec.q, hex.r + vec.q, hex.s + vec.s)function cube_neighbor(cube, direction): return cube_add(cube, cube_direction(direction)) 使用立方体坐标系统，我们能存储不同两个不同的坐标值为一个“向量”，并且这些坐标可以相加得到另外一个坐标。这就是cube_add函数所做的功能。轴坐标和双倍坐标也支持此运算，但偏移坐标不支持。 轴坐标 轴坐标除了不存储第三个坐标以外，其他的和立方体坐标是相同的。代码除了不写第三个轴外，其他的都和立方体坐标相同： 12345678910111213var cube_direction_vectors = [ Cube(+1, 0), Cube(+1, -1), Cube(0, -1), Cube(-1, 0), Cube(-1, +1), Cube(0, +1), ]function cube_direction(direction): return cube_direction_vectors[direction]function cube_add(hex, vec): return Cube(hex.q + vec.q, hex.r + vec.q)function cube_neighbor(cube, direction): return cube_add(cube, cube_direction(direction)) 偏移坐标 与立方体和轴坐标一样，我们将构建一个需要添加到 col 和 row 的数字的表格。然而偏移坐标系不能够安全的进行加减。例如，从 (0, 0) 向东南移动会将我们带到 (0, +1)，因此我们可以将 (0, +1) 放入表中以向东南移动。 但是从 (0, +1) 向东南移动会将我们带到 (+1, +2)，因此我们需要将 (+1, +1) 放入该表中。需要添加的数量依赖我们在表格中的位置。 因此偏移坐标对于奇和偶列/行是有区别的，我们需要分两个列表来存储邻接六边形列表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// odd-rvar oddr_direction_differences = [ // even rows [[+1, 0], [ 0, -1], [-1, -1], [-1, 0], [-1, +1], [ 0, +1]], // odd rows [[+1, 0], [+1, -1], [ 0, -1], [-1, 0], [ 0, +1], [+1, +1]],]function oddr_offset_neighbor(hex, direction): var parity = hex.row &amp; 1 var diff = oddr_direction_differences[parity][direction] return OffsetCoord(hex.col + diff[0], hex.row + diff[1])// even-rvar evenr_direction_differences = [ // even rows [[+1, 0], [+1, -1], [ 0, -1], [-1, 0], [ 0, +1], [+1, +1]], // odd rows [[+1, 0], [ 0, -1], [-1, -1], [-1, 0], [-1, +1], [ 0, +1]],]function evenr_offset_neighbor(hex, direction): var parity = hex.row &amp; 1 var diff = evenr_direction_differences[parity][direction] return OffsetCoord(hex.col + diff[0], hex.row + diff[1])// odd-qvar oddq_direction_differences = [ // even cols [[+1, 0], [+1, -1], [ 0, -1], [-1, -1], [-1, 0], [ 0, +1]], // odd cols [[+1, +1], [+1, 0], [ 0, -1], [-1, 0], [-1, +1], [ 0, +1]],]function oddq_offset_neighbor(hex, direction): var parity = hex.col &amp; 1 var diff = oddq_direction_differences[parity][direction] return OffsetCoord(hex.col + diff[0], hex.row + diff[1])// even-qvar evenq_direction_differences = [ // even cols [[+1, +1], [+1, 0], [ 0, -1], [-1, 0], [-1, +1], [ 0, +1]], // odd cols [[+1, 0], [+1, -1], [ 0, -1], [-1, -1], [-1, 0], [ 0, +1]],]function evenq_offset_neighbor(hex, direction): var parity = hex.col &amp; 1 var diff = evenq_direction_differences[parity][direction] return OffsetCoord(hex.col + diff[0], hex.row + diff[1]) 双倍坐标 不像偏移坐标，双倍坐标的邻接六边形不依赖于我们使用的是列/行。它们在任何地方都是相同的，与轴向/立方体坐标。同样与偏移坐标不同，我们可以安全地减去和添加双倍坐标，这使得它们比偏移坐标更容易使用。 123456789101112131415161718192021222324252627// double widthvar doublewidth_direction_vectors = [ DoubledCoord(+2, 0), DoubledCoord(+1, -1), DoubledCoord(-1, -1), DoubledCoord(-2, 0), DoubledCoord(-1, +1), DoubledCoord(+1, +1), ]function doublewidth_add(hex, diff): return DoubleCoord(hex.col + diff.col, hex.row + diff.row)function doublewidth_neighbor(hex, direction): var vec = doublewidth_direction_vectors[direction] return doublewidth_add(hex, vec)// double height var doubleheight_direction_vectors = [ DoubledCoord(+1, +1), DoubledCoord(+1, -1), DoubledCoord( 0, -2), DoubledCoord(-1, -1), DoubledCoord(-1, +1), DoubledCoord( 0, +2), ]function doubleheight_add(hex, diff): return DoubleCoord(hex.col + diff.col, hex.row + diff.row)function doubleheight_neighbor(hex, direction): var vec = doubleheight_direction_vectors[direction] return doubleheight_add(hex, vec) 对角线 移动到六边形坐标中的“对角线”空间会使 3 个立方体坐标中的一个改变 ±2，另外两个改变 ∓1（总和必须保持为 0）。 1234567var cube_diagonal_vectors = [ Cube(+2, -1, -1), Cube(+1, -2, +1), Cube(-1, -1, +2), Cube(-2, +1, +1), Cube(-1, +2, -1), Cube(+1, +1, -2), ]function cube_diagonal_neighbor(cube, direction): return cube_add(cube, cube_diagonal_vectors[direction]) 距离 立方体坐标 由于六边形立方体坐标是基于3D立方体坐标的，所以我们在六边形网格中也能适用于3D中的距离计算。每个六边形网格相当于3D空间中的一个立方体。在六边形网格中，邻接六边形的距离是1但在立方体网格中其距离是2。对于在立方体网格中步长为2的，在六边形形网格中步长则为1。在3D立方体网格中，经典的曼哈顿距离是abs(dx)+abs(dy)+abs(dz)，那么在六边形网格中的距离则是它的一半。 1234567function cube_subtract(a, b): return Cube(a.q - b.q, a.r - b.r, a.s - b.s)function cube_distance(a, b): var vec = cube_subtract(a, b) return (abs(vec.q) + abs(vec.r) + abs(vec.s)) / 2 // or: (abs(a.q - b.q) + abs(a.r - b.r) + abs(a.s - b.s)) / 2 我们注意三个坐标中的一个必须是其他两个坐标的总和，然后选择这个作为距离。这也是一种计算距离等效的方法，您可能更喜欢上面的“除以二”形式，或者这里的“最大”形式，但它们给出的结果相同： 1234567function cube_subtract(a, b): return Cube(a.q - b.q, a.r - b.r, a.s - b.s)function cube_distance(a, b): var vec = cube_subtract(a, b) return max(abs(vec.q), abs(vec.r), abs(vec.s)) // or: max(abs(a.q - b.q), abs(a.r - b.r), abs(a.s - b.s)) 距离 轴坐标 在轴的系统中，第三个轴是隐式的。我们总是能够转换一个轴坐标到立方体坐标去计算距离： 1234function axial_distance(a, b): var ac = axial_to_cube(a) var bc = axial_to_cube(b) return cube_distance(ac, bc) 一旦我们内联这些函数，结果将如下： 1234function axial_distance(a, b): return (abs(a.q - b.q) + abs(a.q + a.r - b.q - b.r) + abs(a.r - b.r)) / 2 上面的形式也能被写成： 12345678function axial_subtract(a, b): return Hex(a.q - b.q, a.r - b.r)function axial_distance(a, b): var vec = axial_subtract(a, b) return (abs(vec.q) + abs(vec.q + vec.r) + abs(vec.r)) / 2 偏移坐标 与轴坐标一样，我们将转换偏移坐标到轴/立方体坐标，然后使用轴/立方坐标的计算方法。 1234function offset_distance(a, b): var ac = offset_to_axial(a) var bc = offset_to_axial(b) return axial_distance(ac, bc) 我们将使用相同的模式对于大多数的算法：转换偏移到轴/立方体坐标，然后应用的轴/立方体坐标的算法，并转换任意的轴/立方体坐标的结果回偏移坐标。 双倍坐标 虽然转换双倍坐标到轴/立方体坐标也是可行的，但是在rot.js手册中，有直接的距离计算公式： 123456789function doublewidth_distance(a, b): var dcol = abs(a.col - b.col) var drow = abs(a.row - b.row) return drow + max(0, (dcol-drow)/2)function doubleheight_distance(a, b): var dcol = abs(a.col - b.col) var drow = abs(a.row - b.row) return dcol + max(0, (drow−dcol)/2) 绘线 如何去绘制一条从一个六边形格子到另一个六边形格子的线呢？我使用线性插值来绘制线条。在N+1个点上均匀地对线进行采样，并且查找出这些采样点对应的六边形格子。 1. 首先我们计算N=x为端点的六边形网格距离。 2. 然后均匀的在点A和点B之间进行N+1点的采样。使用线性插值，每个点将是A+(B-A)1.0/N i，值i从0到N。这采样的结果点坐标是浮点型的。 3. 转换浮点型的坐标到六边形的整形坐标。这个算法被叫做cube_round。 将上面步骤整合后绘制一条从A到B的线。 1234567891011121314function lerp(a, b, t): // for floats return a + (b - a) * tfunction cube_lerp(a, b, t): // for hexes return Cube(lerp(a.q, b.q, t), lerp(a.r, b.r, t), lerp(a.s, b.s, t))function cube_linedraw(a, b): var N = cube_distance(a, b) var results = [] for each 0 ≤ i ≤ N: results.append(cube_round(cube_lerp(a, b, 1.0/N * i))) return results 移动范围 坐标范围 给定一个六边形中心和和范围N, 哪些六边形在距离它的N步内？我们依据前面讲解的距离公式，distance=max(abs(q),abs(r), abs(s)),去查找都在N范围内的六边形，我们需要max(abs(q),abs(r), abs(s)) &lt;= N。这意味着需要满足\\(abs(q)\\le N，abs(r)&lt;=N\\)以及\\(abs(s)\\le N\\)。移除绝对值符号，我们将得到\\(-N \\le q \\le +N\\), \\(-N \\le r \\le +N\\)以及\\(-N \\le s \\le +N\\) 123456var results = []for each -N ≤ q ≤ +N: for each -N ≤ r ≤ +N: for each -N ≤ s ≤ +N: if q + r + s == 0: results.append(cube_add(center, Cube(q, r, s))) 这个循环是能够正常运行的，但是有些低效。s的所有值循环完后只有一个满足q+r+s=0的限制。我们可以直接计算满足限制的s的值： 12345var results = []for each -N ≤ q ≤ +N: for each max(-N, -q-N) ≤ r ≤ min(+N, -q+N): var s = -q-r results.append(cube_add(center, Cube(q, r, s))) 这个循环迭代完确切需要的坐标。在图中，每个范围都是一对线。 每条线都是一个不等式（半平面）。我们选择满足所有六个不等式的所有六边形。这个循环在轴坐标中也有效： 1234var results = []for each -N ≤ q ≤ +N: for each max(-N, -q-N) ≤ r ≤ min(+N, -q+N): results.append(axial_add(center, Hex(q, r))) 相交范围 如果你要查找多个范围的六边形，你能在产生一个六边形列表前将这些范围相交。 你可以用代数和几何方法来思路这个问题。 代数，每个六边形范围可以用一个不等\\(-N \\le dq \\le +N\\)表示，并且我们将解决这些约束的交集。 几何，在3D空间中，每个范围是一个立方体，并且我们准备在3D空间中让这两个立方体相交。然后投影回q+r+s=0的平面去获得对应的六边形。我们打算使用代数方式去解决相交的问题： 首先，我们重写\\(-N \\le dq \\le +N\\)限制为一般形式，\\(q_{min} \\le q \\le q_{max}\\) 并且设置q_{min} = center.q-N，q_{max} = center.q+N。对于r和s也是一样，最后从上一节的代码中概括得到： 1234var results = []for each $q_&#123;min&#125;$ ≤ q ≤ $q_&#123;max&#125;$: for each max($r_&#123;min&#125;$, -q-$s_&#123;max&#125;$) ≤ r ≤ min($r_&#123;max&#125;$, -q-$s_&#123;min&#125;$): results.append(Hex(q, r)) 两个范围\\(a \\le q \\le b\\)和\\(c \\le q \\le d\\)相交得\\(max(a,c) \\le q \\le min(b, d)\\)。由于六边形区域表示为 q, r, s 上的范围，我们可以分别与 q, r, s 范围中的每一个相交，然后使用嵌套循环在相交处生成六边形列表。对于一个六边形区域，我们设置 \\(q_{min} = H.q - N\\) 和 \\(q_{max} = H.q + N\\) 并且对于 r 和 s 也是如此。 对于相交的两个六边形区域，我们设置 \\(q_{min} = max(H1.q - N, H2.q - N)\\) 和 \\(q_{max} = min(H1.q + N, H2.q + N)\\)，对于 r 和 s 也是如此。这样的模式对于多个区域也是有效的，并且能够泛化到其他的形状（三角形，梯形，菱形和不规则的六边形）。 障碍 如果有障碍物，最简单的做法是有限距离的洪水填充（flood fill）（广度优先搜索）。 在此图中，限制设置为移动。 在代码中， fringes[k] 是可以在 k 步内到达的所有六边形的数组。 每次通过主循环，我们将级别 k-1 扩展为级别 k。 这同样适用于任何六边形坐标系（立方体、轴向、偏移、加倍）。 12345678910111213141516function hex_reachable(start, movement): var visited = set() # set of hexes add start to visited var fringes = [] # array of arrays of hexes fringes.append([start]) for each 1 &lt; k ≤ movement: fringes.append([]) for each hex in fringes[k-1]: for each 0 ≤ dir &lt; 6: var neighbor = hex_neighbor(hex, dir) if neighbor not in visited and not blocked: add neighbor to visited fringes[k].append(neighbor) return visited","categories":[{"name":"Map","slug":"Map","permalink":"http://yoursite.com/categories/Map/"}],"tags":[{"name":"Map","slug":"Map","permalink":"http://yoursite.com/tags/Map/"},{"name":"翻译","slug":"翻译","permalink":"http://yoursite.com/tags/%E7%BF%BB%E8%AF%91/"}]},{"title":"Lua虚拟机源码阅读","slug":"编程语言/Lua/Lua虚拟机源码阅读","date":"2021-11-29T04:00:02.000Z","updated":"2025-04-26T11:06:24.134Z","comments":true,"path":"2021/11/29/编程语言/Lua/Lua虚拟机源码阅读/","link":"","permalink":"http://yoursite.com/2021/11/29/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Lua/Lua%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"文件结构 Lua文件结构 提供外部使用的头文件 lua.h 定义了Lua的核心API。 lauxlib.h 定义了Lua的辅助API，此文件中定义的函数，是对lua.h中函数的简单封装，方便使用。 lualib.h 定义了Lua的标准库（coroutine， table, io, os, string, utf8, math, debug, package）。 luaconf.h 定义编译相关的宏，主要用于区分别不同平台的特性以及一些其他编译特性。 核心代码 Lua的核心代码主要分为以下几个模块： 核心API lapi.h 定义了几个栈的操作宏（api_incr_top：栈顶加一， adjustresults：调节栈顶以容纳更多的返回值， api_checknelems：检查栈中是否有对应的栈空间）。 lapi.c lua.h头文件中定义核心API的实现。 lctype.h C语言的标准字符检查函数头文件，主要检查单个字符的类型，增加了EOZ，允许传入-1（EOZ）进行检测。 lctype.c 定义了符号表。 ldo.c 处理Lua的栈和调用结构。 lfunc.c 定义了用于操纵函数原型和闭包的辅助函数。 llimits.c 定义了一些基础的类型和安装依赖信息。 lmen.c 内存管理接口。 lobject.h Lua中的对象类型定义。 lobject.c 对象上的一些操作。 lstate.h 定义了lua_State相关的结构，这是Lua的核心结构。 lstate.c lstate.h中定义结构的相关操作。 lstring.c Lua字符串表，处理和缓存Lua中使用的字符串。 ltable.c 定义了Lua中Tale相关的操作。 ltm.c 元表相关的定义和操作。 ldump.c 将Lua函数转储为预编译块。 lundump.c 加载预编译的Lua块。 lzio.c 定义了一个buffer流，用于读取数据。 GC管理 lgc.c 垃圾回收器 词法，语法和语义分析器 llex.c 词法分析器。 lparser.h 语法和语义分析器。 Lua虚拟机（解释器） lopcodes.h 定义了Lua虚拟机的操作码（OP_MOVE， OP_LOADI等）跟汇编的MOV指令类型，虚拟机就是用来执行这些指令的。 lopnames.h 定义了操作码对应的名字。 ljumptab.h 定义了Lua解释器跳转表相关的宏（vmdispatch, vmcase, vmbreak）。 lvm.c Lua的虚拟机用于执行opcode。 调试 ldebug.c 获取调试信息（函数调用, 行号信息等）。","text":"文件结构 Lua文件结构 提供外部使用的头文件 lua.h 定义了Lua的核心API。 lauxlib.h 定义了Lua的辅助API，此文件中定义的函数，是对lua.h中函数的简单封装，方便使用。 lualib.h 定义了Lua的标准库（coroutine， table, io, os, string, utf8, math, debug, package）。 luaconf.h 定义编译相关的宏，主要用于区分别不同平台的特性以及一些其他编译特性。 核心代码 Lua的核心代码主要分为以下几个模块： 核心API lapi.h 定义了几个栈的操作宏（api_incr_top：栈顶加一， adjustresults：调节栈顶以容纳更多的返回值， api_checknelems：检查栈中是否有对应的栈空间）。 lapi.c lua.h头文件中定义核心API的实现。 lctype.h C语言的标准字符检查函数头文件，主要检查单个字符的类型，增加了EOZ，允许传入-1（EOZ）进行检测。 lctype.c 定义了符号表。 ldo.c 处理Lua的栈和调用结构。 lfunc.c 定义了用于操纵函数原型和闭包的辅助函数。 llimits.c 定义了一些基础的类型和安装依赖信息。 lmen.c 内存管理接口。 lobject.h Lua中的对象类型定义。 lobject.c 对象上的一些操作。 lstate.h 定义了lua_State相关的结构，这是Lua的核心结构。 lstate.c lstate.h中定义结构的相关操作。 lstring.c Lua字符串表，处理和缓存Lua中使用的字符串。 ltable.c 定义了Lua中Tale相关的操作。 ltm.c 元表相关的定义和操作。 ldump.c 将Lua函数转储为预编译块。 lundump.c 加载预编译的Lua块。 lzio.c 定义了一个buffer流，用于读取数据。 GC管理 lgc.c 垃圾回收器 词法，语法和语义分析器 llex.c 词法分析器。 lparser.h 语法和语义分析器。 Lua虚拟机（解释器） lopcodes.h 定义了Lua虚拟机的操作码（OP_MOVE， OP_LOADI等）跟汇编的MOV指令类型，虚拟机就是用来执行这些指令的。 lopnames.h 定义了操作码对应的名字。 ljumptab.h 定义了Lua解释器跳转表相关的宏（vmdispatch, vmcase, vmbreak）。 lvm.c Lua的虚拟机用于执行opcode。 调试 ldebug.c 获取调试信息（函数调用, 行号信息等）。 标准库代码 lauxlib.c 实现了Lua的辅助API，此文件中定义的函数，是对lua.h中函数的简单封装，方便使用。 lbaselib.c 实现了Lua中使用的一些全局函数（assert, dofile, error, ipairs, pairs, print, getmetatable, setmetatable, xpcall, tostring, type等）。 lcorolib.c 协程(coroutine)。 ldblib.c 调试库(debug)。 liolib 标准IO操作（io）。 lmathlib.c 数学库(math)。 loadlib.c 加载库（package）。 loslib.c 系统库（os）。 lstrlib.c 字符串库（string）。 ltablib.c 表（table）。 lutf8lib.c utf8库（utf8）。 linit.c 注册所有的标注库到Lua, 核心函数luaL_openlibs，会遍历注册所有的标准库。 解释器和编译器 lua.c 通过上面的API构建的一个Lua解释器。 luac.c Lua的编译器，主要用于将Lua源文件编译成二进制格式。 核心代码 核心结构 global_State 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 全局的State, 被所用State共享的，用于记录一些全局数据struct global_State &#123; lua_Alloc frealloc; // 内存分配函数,现在是使用的realloc(有连续空间，扩展内存是不会进行拷贝数的) void *ud; // frealloc 的辅助数据，现在没有用，如果使用其他的内存分配函数可以使用这个预留的字段，在lauxlib.c中的l_alloc函数中是未使用的 l_mem totalbytes; // 当前分配的总内存（单位：Byte） l_mem GCdebt; // 垃圾收集器还未归还的内存（单位：Byte） lu_mem GCestimate; // 正在使用的非垃圾内存的预估值（单位：Byte） lu_mem lastatomic; // 代垃圾收集的一个标志 stringtable strt; // 用于存储Lua中使用的短字符（长度小于等于40，通过宏LUAI_MAXSHORTLEN定义的）串表 TValue l_registry; // 注册表（Table类型），默认存储了全局主线程的State和全局Table，主要用来存储用户数据的元表 TValue nilvalue; // 定义的全局nil值 unsigned int seed; // 随机数种子 // 垃圾收集相关 lu_byte currentwhite; lu_byte gcstate; // 垃圾收集器状态(GCSpropagate, GCSenteratomic, GCSatomic, GCSswpallgc, GCSswpfinobj, GCSswptobefnz, GCSswpend, GCScallfin, GCSpause) lu_byte gckind; // 当前运行的GC类型（KGC_INC（增量GC）， KGC_GEN（分代GC）） lu_byte gcstopem; // 是否停止紧急收集 lu_byte genminormul; // 次代收集倍数 lu_byte genmajormul; // 主代收集倍数 lu_byte gcrunning; // GC是否在运行 lu_byte gcemergency; // 是紧急收集 lu_byte gcpause; // 两个连续GC的暂停阈值 lu_byte gcstepmul; // GC的步长，用于控制GC的速度 lu_byte gcstepsize; // 单步长收集的的大小 GCObject *allgc; // 所有可收集对象的列表 GCObject **sweepgc; // 当前扫到的位置 GCObject *finobj; // 进入了析构的对象列表（还未真正的释放） GCObject *gray; // 灰色对象列表 GCObject *grayagain; /* list of objects to be traversed atomically */ GCObject *weak; // 弱值弱引用表 GCObject *ephemeron; // 弱key引用表 GCObject *allweak; // 所有的若引用 GCObject *tobefnz; // 要GC的userdata列表 GCObject *fixedgc; // 不被GC回收的对象列表 // 分代收集器相关的字段 GCObject *survival; // 存活了一个GC周期的对象头 GCObject *old1; /* start of old1 objects */ GCObject *reallyold; /* objects more than one cycle old (\"really old\") */ GCObject *firstold1; /* first OLD1 object in the list (if any) */ GCObject *finobjsur; /* list of survival objects with finalizers */ GCObject *finobjold1; /* list of old1 objects with finalizers */ GCObject *finobjrold; /* list of really old objects with finalizers */ struct lua_State *twups; // 开放了上值的现场列表 lua_CFunction panic; // 为保护模式下发生异常时调用的函数 struct lua_State *mainthread; // 主线程的State对象 TString *memerrmsg; // 缓存的一个内存分配的错误消息 TString *tmname[TM_N];// 元方法名字（__index, __len等） struct Table *mt[LUA_NUMTAGS]; 基础类型的元表() TString *strcache[STRCACHE_N][STRCACHE_M]; // 字符串缓存，避免在使用一些字符串时频繁创建 lua_WarnFunction warnf; // 警告函数 void *ud_warn; // 警告函数的辅助数据&#125;; lua_State 1234567891011121314151617181920212223242526// 每个线程的Statestruct lua_State &#123; CommonHeader; // 公共头(所有可回收对象都有)宏struct GCObject *next; lu_byte tt; lu_byte marked lu_byte status; // 线程状态LUA_OK，LUA_YIELD，LUA_ERRRUN，LUA_ERRSYNTAX，LUA_ERRMEM，UA_ERRERR lu_byte allowhook; // 是否允许hook unsigned short nci; // 调用信息（CallInfo）的数量 StkId top; // 当前可以使用的栈值（也就是栈顶的上一个值）StkId（ typedef StackValue *StkId） global_State *l_G; // 关联的全局State CallInfo *ci; // 当前函数的调用信息 StkId stack_last; // 栈底的下一个栈值 StkId stack; // 栈 UpVal *openupval; // 在栈中的开放上值（open upvalues）列表 StkId tbclist; // to-be-closed变量列表 GCObject *gclist; // GC对象列表 struct lua_State *twups; // 使用了开放上值（open upvalues）的线程列表 struct lua_longjmp *errorJmp; //保存setjump和longjump上下问环境，用于错误恢复 CallInfo base_ci; // 第一级调用信息（c 调用 Lua） volatile lua_Hook hook; // 钩子（hook）函数指针 ptrdiff_t errfunc; // 当前的错误处理函数的栈索引 l_uint32 nCcalls; // 内嵌的调用数量（non-yieldable | C） int oldpc; // 记录最后一次PC（指令）指针的位置 int basehookcount; // hook的基础计数 int hookcount; // 当前的hook计数 volatile l_signalT hookmask; // hook的掩码&#125;; CallInfo 12345678910111213141516171819202122232425262728struct CallInfo &#123; StkId func; // 函数在栈中的位置（一个StackValue的地址） StkId top; // 这个函数的栈顶位置(栈顶的地址) struct CallInfo *previous, *next; // 调用链的前后指针 union &#123; struct &#123; const Instruction *savedpc; // 虚拟机的指令列表 volatile l_signalT trap; // int nextraargs; // 变长函数的额外参数个数（...） &#125; l; // Lua函数 struct &#123; lua_KFunction k; /* continuation in case of yields */ ptrdiff_t old_errfunc; lua_KContext ctx; /* context info. in case of yields */ &#125; c; // c函数 &#125; u; union &#123; int funcidx; 当前调用函数在栈中的索引（func的地址-L.stack的地址求出函数在栈中的索引），仅用于在保护模式下调用的c函数 int nyield; // yielded的值的数量 int nres; // 返回的值的数量， 字段“nres”仅在从C函数返回时关闭tbc变量时使用 struct &#123; unsigned short ftransfer; 首个传递值的偏移 unsigned short ntransfer; 传递值的数量 &#125; transferinfo; &#125; u2; // 从hook调用或返回时传递的值的信息 short nresults; 从这个函数返回的期望的结果数量 unsigned short callstatus; // 调用的状态（CIST_OAH:运行hook, CIST_C:函数调用，等）&#125; CallInfo; TString 123456789101112// 字符串struct TString &#123; CommonHeader; // 所有的GC对象都必须包含此宏 #define CommonHeader struct GCObject *next; lu_byte tt; lu_byte marked lu_byte extra; // 保留关键字（and, break, do 等）的枚举值, 此字段有非0值，表示是关键字。 lu_byte shrlen; // 短字符串（小于40）长度，短字符串会被缓存到global_State.strt中 unsigned int hash; // 长字符串对应的hash值 union &#123; size_t lnglen; // 长字符串的长度 struct TString *hnext; // 链接的字符串列表 &#125; u; char contents[1]; // 字符串本身的内容&#125;; Closure 1234567891011121314151617181920212223// 闭包的公共头#define ClosureHeader \\ CommonHeader; lu_byte nupvalues; GCObject *gclist// C闭包typedef struct CClosure &#123; ClosureHeader; lua_CFunction f; // C函数指针 TValue upvalue[1];// 上值列表 &#125; CClosure;// Lua闭包typedef struct LClosure &#123; ClosureHeader; struct Proto *p; // Lua函数原型 UpVal *upvals[1];// 上值列表&#125; LClosure;// 包含C和Lua的闭包typedef union Closure &#123; CClosure c; LClosure l;&#125; Closure; Proto 12345678910111213141516171819202122232425// 函数原型typedef struct Proto &#123; CommonHeader; // 所有GC独享都有的 lu_byte numparams; // 函数的固定参数数量 lu_byte is_vararg; // 有变成参数吗 lu_byte maxstacksize; // 这个函数需要的寄存器数量（也就是有多少参数要压栈） int sizeupvalues; // 函数的上值数量 int sizek; // 函数使用的常量数量 int sizecode; // 指令的数量 int sizelineinfo; // 行号的数量 int sizep; // 函数的子函数(内部函数)的数量 int sizelocvars; // 本地变量的数量 int sizeabslineinfo; // 每条指令对应的行号信息 int linedefined; // 函数的开始行， 用于调试 int lastlinedefined; // 函数的结束行， 用于调试 TValue *k; // 函数使用的常量值 Instruction *code; // 函数的指令列表 struct Proto **p; // 函数的内部函数原型列表 Upvaldesc *upvalues; // 函数的上值信息 ls_byte *lineinfo; // 函数源码的行列表， 用于调试 AbsLineInfo *abslineinfo; // 函数指令的行列表 LocVar *locvars; // 本地变量， 用于调试 TString *source; // 函数源码， 用于调试 GCObject *gclist; // &#125; Proto; Upvaldesc 1234567// 函数原型的上值描述typedef struct Upvaldesc &#123; TString *name; // 上值得名字，用于调试 lu_byte instack; // 是否在栈上 lu_byte idx; // 在栈上或外部函数列表的索引 lu_byte kind; // 类型（0:常规， 1：常量， 2：to-be-closed（超出作用域会调用__close原方法，local t&lt;clost&gt; = xx 5.4新增的）, 3:编译时常量）&#125; Upvaldesc; UpVal 12345678910111213// 闭包的上值typedef struct UpVal &#123; CommonHeader; lu_byte tbc; // 是否是to-be-closed变量 TValue *v; // 指向堆栈或其自身的值 union &#123; struct &#123; struct UpVal *next; 列表的Next指针 struct UpVal **previous; 列表的Previous指针 &#125; open; // open 的上值 TValue value; closed的上值 &#125; u;&#125; UpVal; Udata 1234567891011121314151617// userdata的头，分配的内存区域放在此结构的后面typedef struct Udata &#123; CommonHeader; unsigned short nuvalue; // 用户值的数量 size_t len; // bytes大小 struct Table *metatable; // userdata的元表 GCObject *gclist; UValue uv[1]; // 用户值&#125; Udata;// 其实就是TValue，只是为了保证最大化的字节对齐在里面放了所有类型#define LUAI_MAXALIGN lua_Number n; double u; void *s; lua_Integer i; long ltypedef union UValue &#123; TValue uv; LUAI_MAXALIGN; /* ensures maximum alignment for udata bytes */&#125; UValue; Table 1234567891011121314151617181920212223// Lua的核心数据结构，我们常用的表的结构typedef struct Table &#123; CommonHeader; lu_byte flags; // 表的标志位 lu_byte lsizenode; // 表的Hash部分的大小（2的lsizenode次幂），lsizenode存的是2的指数部分的值 unsigned int alimit; // 表的数组部分的大小 TValue *array; // 数组部分的开始地址 Node *node; // Hash部分的开始地址 Node *lastfree; // 用于记录最后一个空闲节点 struct Table *metatable; // 元表 GCObject *gclist;&#125; Table;// hash表使用的节点，为什么Key的类型和值不能直接使用TValue? 因为字节对齐的原因分开定义能减少NodeKey的内存占用大小typedef union Node &#123; struct NodeKey &#123; TValuefields; // 值（带类型的Value）#define TValuefields Value value_; lu_byte tt_ lu_byte key_tt; // Key的类型 int next; // Hash冲突时，用于表示下一个Key Value key_val; // Key的值 &#125; u; TValue i_val; // 直接访问节点的TValue（只是为了方便访问值）&#125; Node; flags字段说明： 7 位 表示alimit字段是否代表了数组的真实大小（0:是，1:不是），否则数组的真实大小是不小于alimit的2的整数次幂。 0-6 位 表示是否有TM_INDEX, TM_NEWINDEX, TM_GC, TM_MODE, TM_LEN, TM_EQ(0:有，1：没有)主要用于快速检测这几个常用的元方法是否存在。 Table同时使用数组和Hash表的结构，当使用的key是整形且不大于alimit时，Lua会使用array字段，当大于alimit时，则会转换成hash表的方式来。当数组或Hash表的大小需要扩容，应尽量避免这些没有避免的性能消耗。解决Hash冲突的方式是结合了开放寻址和链表的形式。 TValue 12345678910111213141516// Lua中的值（所有类型都是用的此结构）typedef union Value &#123; struct GCObject *gc; // 可收集(GC)对象 void *p; // 轻量级userdata lua_CFunction f; // 轻量级c函数 lua_Integer i; // 整型 lua_Number n; // 浮点型数据&#125; Value;// 带Tag(标记)的值(定义了TValue的字段)，value_：为值 tt_ 为(Tag)标记字段,#define TValuefields Value value_; lu_byte tt_typedef struct TValue &#123; TValuefields;&#125; TValue; Tag(标记)字段说明： 0-3 位 表示LUA_T*定义的类型(最多16种类型) 4-5 位 表示变体标志（额外附加的一些标志位，比如要表示一个Number类型是整形还是浮点型，则可以加上一个变体标志来表示，如果LUA_TNUMBER(3)宏默认定义为整形，为了表示是浮点型，则可以 1 &amp; (3&lt;&lt;4) 这就在变体位加上了一个1，假设想区分单/双精度浮点则我们可以用1表单精度，2表示双精度，则可以使用2&amp;(3&lt;&lt;4)来表示） 6 位 表示是否是可收集(GC)对象 StackValue 123456789101112// 用于表示Lua栈上的值typedef union StackValue &#123; TValue val; struct &#123; TValuefields; unsigned short delta; &#125; tbclist; // to-be-closed值，表示5.4新增的一种带作用域的变量，比如在lua中这样定义： local a&lt;close&gt; = xxx 当a超出作用范围时，则会调用a元表中的__close元方法&#125; StackValue;// 栈元素的索引(通过将两个栈值的地址先减就能计算出两个栈值的相对位置，比如当前栈值地址-栈基地址，就能求出在栈中的索引)typedef StackValue *StkId; 核心函数 State相关 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165// 线程状态+额外空间typedef struct LX &#123; lu_byte extra_[LUA_EXTRASPACE]; lua_State l;&#125; LX;// 主线程和全局状态对象typedef struct LG &#123; LX l; global_State g;&#125; LG;// 创建一个StateLUA_API lua_State *lua_newstate (lua_Alloc f, void *ud) &#123; int i; lua_State *L; global_State *g; LG *l = cast(LG *, (*f)(ud, NULL, LUA_TTHREAD, sizeof(LG))); // 分配一个LG对象 if (l == NULL) return NULL; L = &amp;l-&gt;l.l; // 线程状态对象 g = &amp;l-&gt;g; // 全局状态对象 L-&gt;tt = LUA_VTHREAD; // 初始化类型为线程 g-&gt;currentwhite = bitmask(WHITE0BIT); // 垃圾收集器相关的，后续单独分析 L-&gt;marked = luaC_white(g); preinit_thread(L, g); // 初始化一个线程状态，并关联线程状态对应的全局状态 g-&gt;allgc = obj2gco(L); // 将线程状态放入GC列表 L-&gt;next = NULL; // 下一个GC为空（当前它是最有一个GC对象，因为当前只分配了这一个线程状态对象） incnny(L); ///* main thread is always non yieldable */ g-&gt;frealloc = f; // 保持内存分配函数到全局状态对象上 g-&gt;ud = ud; // NULL g-&gt;warnf = NULL; g-&gt;ud_warn = NULL; g-&gt;mainthread = L; // 将当前线程状态设置为主线程 g-&gt;seed = luai_makeseed(L); // 产生随机数种子 g-&gt;gcrunning = 0; g-&gt;strt.size = g-&gt;strt.nuse = 0; // 初始化短字符串缓存表 g-&gt;strt.hash = NULL; setnilvalue(&amp;g-&gt;l_registry); // 注册表设为空 g-&gt;panic = NULL; g-&gt;gcstate = GCSpause; // 初始化时GC处于暂停状态 g-&gt;gckind = KGC_INC; // 默认的GC类型是增量GC g-&gt;gcstopem = 0; g-&gt;gcemergency = 0; g-&gt;finobj = g-&gt;tobefnz = g-&gt;fixedgc = NULL; g-&gt;firstold1 = g-&gt;survival = g-&gt;old1 = g-&gt;reallyold = NULL; g-&gt;finobjsur = g-&gt;finobjold1 = g-&gt;finobjrold = NULL; g-&gt;sweepgc = NULL; g-&gt;gray = g-&gt;grayagain = NULL; g-&gt;weak = g-&gt;ephemeron = g-&gt;allweak = NULL; g-&gt;twups = NULL; g-&gt;totalbytes = sizeof(LG); g-&gt;GCdebt = 0; g-&gt;lastatomic = 0; setivalue(&amp;g-&gt;nilvalue, 0); setgcparam(g-&gt;gcpause, LUAI_GCPAUSE); setgcparam(g-&gt;gcstepmul, LUAI_GCMUL); g-&gt;gcstepsize = LUAI_GCSTEPSIZE; setgcparam(g-&gt;genmajormul, LUAI_GENMAJORMUL); g-&gt;genminormul = LUAI_GENMINORMUL; for (i=0; i &lt; LUA_NUMTAGS; i++) g-&gt;mt[i] = NULL; // 基础类型的元表设置为空 if (luaD_rawrunprotected(L, f_luaopen, NULL) != LUA_OK) &#123; // 调用线程状态打开函数 // 不成功，则关闭线程状态并释放已经分配的内存 close_state(L); L = NULL; &#125; return L;&#125;// 线程状态初始化函数static void f_luaopen (lua_State *L, void *ud) &#123; global_State *g = G(L); UNUSED(ud); stack_init(L, L); // 初始化线程状态的栈 init_registry(L, g); // 初始化注册表 luaS_init(L); // 初始化短字符串缓存表 luaT_init(L); // 初始化元方法字符串，并缓存到全局状态的tmname中 luaX_init(L); // 初始化保留关键字字符串 g-&gt;gcrunning = 1; // 开启GC setnilvalue(&amp;g-&gt;nilvalue); luai_userstateopen(L);&#125;// 初始化一个栈static void stack_init (lua_State *L1, lua_State *L) &#123; int i; CallInfo *ci; // 分配一个大小为(40+5)的栈 L1-&gt;stack = luaM_newvector(L, BASIC_STACK_SIZE + EXTRA_STACK, StackValue); L1-&gt;tbclist = L1-&gt;stack; // 将栈中的值设置为nil for (i = 0; i &lt; BASIC_STACK_SIZE + EXTRA_STACK; i++) setnilvalue(s2v(L1-&gt;stack + i)); L1-&gt;top = L1-&gt;stack; // 将栈顶设置到栈的初始位置 L1-&gt;stack_last = L1-&gt;stack + BASIC_STACK_SIZE; // 标记栈的最后一个栈值（不包含额外空间） // 初始化首个调用信息 ci = &amp;L1-&gt;base_ci; ci-&gt;next = ci-&gt;previous = NULL; ci-&gt;callstatus = CIST_C; // C调用 ci-&gt;func = L1-&gt;top; ci-&gt;u.c.k = NULL; ci-&gt;nresults = 0; setnilvalue(s2v(L1-&gt;top)); L1-&gt;top++; ci-&gt;top = L1-&gt;top + LUA_MINSTACK; // 设置当前调用信息的栈顶为当前线程状态的栈顶+20 L1-&gt;ci = ci; // 设置base_ci调用信息为当前的ci&#125;// 创建注册表并初始化static void init_registry (lua_State *L, global_State *g) &#123; // 创建注册表 Table *registry = luaH_new(L); // 保持注册表到全局状态中去 sethvalue(L, &amp;g-&gt;l_registry, registry); // 扩展注册表大小为2 luaH_resize(L, registry, LUA_RIDX_LAST, 0); // 将主线程保存到注册表中 setthvalue(L, &amp;registry-&gt;array[LUA_RIDX_MAINTHREAD - 1], L); // 创建一个全局表到注册表中 sethvalue(L, &amp;registry-&gt;array[LUA_RIDX_GLOBALS - 1], luaH_new(L));&#125;// 初始化stringtablevoid luaS_init (lua_State *L) &#123; global_State *g = G(L); int i, j; stringtable *tb = &amp;G(L)-&gt;strt; tb-&gt;hash = luaM_newvector(L, MINSTRTABSIZE, TString*); // 初始化一个大小为128的TString指针数组 tablerehash(tb-&gt;hash, 0, MINSTRTABSIZE); // 清空tb-&gt;hash tb-&gt;size = MINSTRTABSIZE; g-&gt;memerrmsg = luaS_newliteral(L, MEMERRMSG); // 创建一个\"not enough memory\"消息 luaC_fix(L, obj2gco(g-&gt;memerrmsg)); // 不让其被GC for (i = 0; i &lt; STRCACHE_N; i++) // 给strcache填充一个有效值 for (j = 0; j &lt; STRCACHE_M; j++) g-&gt;strcache[i][j] = g-&gt;memerrmsg;&#125;// 初始化元方法字符串void luaT_init (lua_State *L) &#123; static const char *const luaT_eventname[] = &#123; \"__index\", \"__newindex\", \"__gc\", \"__mode\", \"__len\", \"__eq\", \"__add\", \"__sub\", \"__mul\", \"__mod\", \"__pow\", \"__div\", \"__idiv\", \"__band\", \"__bor\", \"__bxor\", \"__shl\", \"__shr\", \"__unm\", \"__bnot\", \"__lt\", \"__le\", \"__concat\", \"__call\", \"__close\" &#125;; int i; for (i=0; i&lt;TM_N; i++) &#123; G(L)-&gt;tmname[i] = luaS_new(L, luaT_eventname[i]); luaC_fix(L, obj2gco(G(L)-&gt;tmname[i])); // 不被GC回收 &#125;&#125;// 初始化Lua中的关键字字符串void luaX_init (lua_State *L) &#123; int i; TString *e = luaS_newliteral(L, LUA_ENV); // 创建\"_ENV\"字符串 luaC_fix(L, obj2gco(e)); for (i=0; i&lt;NUM_RESERVED; i++) &#123; TString *ts = luaS_new(L, luaX_tokens[i]); // 创建关键字字符串 luaC_fix(L, obj2gco(ts)); ts-&gt;extra = cast_byte(i+1); // 记录关键字枚举值 &#125;&#125; Proto和Closure相关 1234567891011121314151617181920212223242526272829303132333435// 分配一个Proto对象LUAI_FUNC Proto *luaF_newproto (lua_State *L);// 分配C闭包LUAI_FUNC CClosure *luaF_newCclosure (lua_State *L, int nupvals);// 分配一个Lua闭包 LUAI_FUNC LClosure *luaF_newLclosure (lua_State *L, int nupvals);// 初始化上值void luaF_initupvals (lua_State *L, LClosure *cl) &#123; int i; for (i = 0; i &lt; cl-&gt;nupvalues; i++) &#123; GCObject *o = luaC_newobj(L, LUA_VUPVAL, sizeof(UpVal)); UpVal *uv = gco2upv(o); uv-&gt;v = &amp;uv-&gt;u.value; /* make it closed */ setnilvalue(uv-&gt;v); cl-&gt;upvals[i] = uv; luaC_objbarrier(L, cl, uv); &#125;&#125;// 查找或创建一个上值UpVal *luaF_findupval (lua_State *L, StkId level) &#123; UpVal **pp = &amp;L-&gt;openupval; UpVal *p; lua_assert(isintwups(L) || L-&gt;openupval == NULL); while ((p = *pp) != NULL &amp;&amp; uplevel(p) &gt;= level) &#123; /* search for it */ lua_assert(!isdead(G(L), p)); if (uplevel(p) == level) /* corresponding upvalue? */ return p; /* return it */ pp = &amp;p-&gt;u.open.next; &#125; /* not found: create a new upvalue after 'pp' */ return newupval(L, 0, level, pp);&#125; CallInfo相关 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293// 调用lua解析器生成Lua闭包和函数原型int luaD_protectedparser (lua_State *L, ZIO *z, const char *name, const char *mode) &#123; struct SParser p; int status; incnny(L); /* cannot yield during parsing */ p.z = z; p.name = name; p.mode = mode; p.dyd.actvar.arr = NULL; p.dyd.actvar.size = 0; p.dyd.gt.arr = NULL; p.dyd.gt.size = 0; p.dyd.label.arr = NULL; p.dyd.label.size = 0; luaZ_initbuffer(L, &amp;p.buff); status = luaD_pcall(L, f_parser, &amp;p, savestack(L, L-&gt;top), L-&gt;errfunc); luaZ_freebuffer(L, &amp;p.buff); luaM_freearray(L, p.dyd.actvar.arr, p.dyd.actvar.size); luaM_freearray(L, p.dyd.gt.arr, p.dyd.gt.size); luaM_freearray(L, p.dyd.label.arr, p.dyd.label.size); decnny(L); return status;&#125;// 准备函数的调用信息CallInfo *luaD_precall (lua_State *L, StkId func, int nresults) &#123; lua_CFunction f; retry: switch (ttypetag(s2v(func))) &#123; case LUA_VCCL: /* C closure */ f = clCvalue(s2v(func))-&gt;f; goto Cfunc; case LUA_VLCF: /* light C function */ f = fvalue(s2v(func)); Cfunc: &#123; int n; /* number of returns */ CallInfo *ci; checkstackGCp(L, LUA_MINSTACK, func); /* ensure minimum stack size */ L-&gt;ci = ci = next_ci(L); ci-&gt;nresults = nresults; ci-&gt;callstatus = CIST_C; ci-&gt;top = L-&gt;top + LUA_MINSTACK; ci-&gt;func = func; lua_assert(ci-&gt;top &lt;= L-&gt;stack_last); if (l_unlikely(L-&gt;hookmask &amp; LUA_MASKCALL)) &#123; int narg = cast_int(L-&gt;top - func) - 1; luaD_hook(L, LUA_HOOKCALL, -1, 1, narg); &#125; lua_unlock(L); n = (*f)(L); /* do the actual call */ lua_lock(L); api_checknelems(L, n); luaD_poscall(L, ci, n); return NULL; &#125; case LUA_VLCL: &#123; /* Lua function */ CallInfo *ci; Proto *p = clLvalue(s2v(func))-&gt;p; int narg = cast_int(L-&gt;top - func) - 1; /* number of real arguments */ int nfixparams = p-&gt;numparams; int fsize = p-&gt;maxstacksize; /* frame size */ checkstackGCp(L, fsize, func); L-&gt;ci = ci = next_ci(L); ci-&gt;nresults = nresults; ci-&gt;u.l.savedpc = p-&gt;code; /* starting point */ ci-&gt;top = func + 1 + fsize; ci-&gt;func = func; L-&gt;ci = ci; for (; narg &lt; nfixparams; narg++) setnilvalue(s2v(L-&gt;top++)); /* complete missing arguments */ lua_assert(ci-&gt;top &lt;= L-&gt;stack_last); return ci; &#125; default: &#123; /* not a function */ checkstackGCp(L, 1, func); /* space for metamethod */ luaD_tryfuncTM(L, func); /* try to get '__call' metamethod */ goto retry; /* try again with metamethod */ &#125; &#125;&#125;// 调用函数static void ccall (lua_State *L, StkId func, int nResults, int inc) &#123; CallInfo *ci; L-&gt;nCcalls += inc; if (l_unlikely(getCcalls(L) &gt;= LUAI_MAXCCALLS)) luaE_checkcstack(L); if ((ci = luaD_precall(L, func, nResults)) != NULL) &#123; /* Lua function? */ ci-&gt;callstatus = CIST_FRESH; /* mark that it is a \"fresh\" execute */ luaV_execute(L, ci); /* call it */ &#125; L-&gt;nCcalls -= inc;&#125;void luaD_call (lua_State *L, StkId func, int nResults) &#123; ccall(L, func, nResults, 1);&#125; Table相关 创建和释放： 123456789101112131415161718// 创建一个TablewTable *luaH_new (lua_State *L) &#123; GCObject *o = luaC_newobj(L, LUA_VTABLE, sizeof(Table)); // 分配一个Table对象 Table *t = gco2t(o); t-&gt;metatable = NULL; t-&gt;flags = cast_byte(maskflags); /* table has no metamethod fields */ t-&gt;array = NULL; t-&gt;alimit = 0; setnodevector(L, t, 0); // 初始化一个带虚拟节点的空table return t;&#125;// 释放void luaH_free (lua_State *L, Table *t) &#123; freehash(L, t); luaM_freearray(L, t-&gt;array, luaH_realasize(t)); luaM_free(L, t);&#125; 获取： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131// 获取函数const TValue *luaH_get (Table *t, const TValue *key) &#123; switch (ttypetag(key)) &#123; case LUA_VSHRSTR: return luaH_getshortstr(t, tsvalue(key)); // 短字符串key case LUA_VNUMINT: return luaH_getint(t, ivalue(key)); // 整型key case LUA_VNIL: return &amp;absentkey; // Nil返回未找到static const TValue absentkey = &#123; value_: &#123;NULL&#125;, tt_: makevariant(LUA_TNIL, 2)&#125;; case LUA_VNUMFLT: &#123; // 浮点型key lua_Integer k; if (luaV_flttointeger(fltvalue(key), &amp;k, F2Ieq)) // 能转换成整形吗 return luaH_getint(t, k); // 能则调用获取整形key的函数，否则直接到default &#125; default: return getgeneric(t, key, 0); // 通用的get方法 &#125;&#125;// 整型key时，如果key的小于等于alimit则直接访问数组部分，当等于alimit + 1时则增加数组部分的大小并访问，否则就访问hash部分const TValue *luaH_getint (Table *t, lua_Integer key) &#123; if (l_castS2U(key) - 1u &lt; t-&gt;alimit) return &amp;t-&gt;array[key - 1]; else if (!limitequalsasize(t) &amp;&amp; (l_castS2U(key) == t-&gt;alimit + 1 || l_castS2U(key) - 1u &lt; luaH_realasize(t))) &#123; t-&gt;alimit = cast_uint(key); return &amp;t-&gt;array[key - 1]; &#125; else &#123; Node *n = hashint(t, key); 获取hash的slot（主节点） for (;;) &#123; if (keyisinteger(n) &amp;&amp; keyival(n) == key) return gval(n); else &#123; int nx = gnext(n); if (nx == 0) break; n += nx; &#125; &#125; return &amp;absentkey; &#125;&#125;// “通用\"的get(不是通用的：对于可能在数组部分的整数无效，对于具有整数值的浮点数也无效。)static const TValue *getgeneric (Table *t, const TValue *key, int deadok) &#123; Node *n = mainpositionTV(t, key); for (;;) &#123; // 检查key是否在链表中的某处 if (equalkey(key, n, deadok)) return gval(n); 匹配到key就返回，否则需要在hash的冲突链中查找 else &#123; int nx = gnext(n); // 获取冲突链中的下一个 if (nx == 0) // 已经到最后一个了，则返回没有找到 return &amp;absentkey; n += nx; &#125; &#125;&#125;// 获取key值对应的hash槽（slot）也就是hash值对应的主节点static Node *mainpositionTV (const Table *t, const TValue *key) &#123; return mainposition(t, rawtt(key), valraw(key));&#125;static Node *mainposition (const Table *t, int ktt, const Value *kvl) &#123; switch (withvariant(ktt)) &#123; case LUA_VNUMINT: &#123; // 整形的hash函数 lua_Integer key = ivalueraw(*kvl); return hashint(t, key); &#125; case LUA_VNUMFLT: &#123; // 浮点数的hash 函数 lua_Number n = fltvalueraw(*kvl); return hashmod(t, l_hashfloat(n)); &#125; case LUA_VSHRSTR: &#123; // 字符串的哈希函数， TString *ts = tsvalueraw(*kvl); return hashstr(t, ts); &#125; case LUA_VLNGSTR: &#123; // 字符串的函数函数 TString *ts = tsvalueraw(*kvl); return hashpow2(t, luaS_hashlongstr(ts)); &#125; case LUA_VFALSE: // 布尔值的hash函数 return hashboolean(t, 0); case LUA_VTRUE: return hashboolean(t, 1); case LUA_VLIGHTUSERDATA: &#123; // 轻量级用户数据的hash函数 void *p = pvalueraw(*kvl); return hashpointer(t, p); &#125; case LUA_VLCF: &#123; // C函数的hash函数 lua_CFunction f = fvalueraw(*kvl); return hashpointer(t, f); &#125; default: &#123; // 其他类型的hash函数 GCObject *o = gcvalueraw(*kvl); return hashpointer(t, o); &#125; &#125;&#125;// Hash函数说明：// 整形：直接用整形值对表的长度求模。// 浮点数：#define hashmod(t,n) (gnode(t, ((n) % ((sizenode(t)-1)|1))))static int l_hashfloat (lua_Number n) &#123; int i; lua_Integer ni; n = l_mathop(frexp)(n, &amp;i) * -cast_num(INT_MIN); if (!lua_numbertointeger(n, &amp;ni)) &#123; /* is 'n' inf/-inf/NaN? */ lua_assert(luai_numisnan(n) || l_mathop(fabs)(n) == cast_num(HUGE_VAL)); return 0; &#125; else &#123; /* normal case */ unsigned int u = cast_uint(i) + cast_uint(ni); return cast_int(u &lt;= cast_uint(INT_MAX) ? u : ~u); &#125;&#125;// 字符串：#define hashstr(t,str) hashpow2(t, (str)-&gt;hash)unsigned int luaS_hash (const char *str, size_t l, unsigned int seed) &#123; unsigned int h = seed ^ cast_uint(l); for (; l &gt; 0; l--) h ^= ((h&lt;&lt;5) + (h&gt;&gt;2) + cast_byte(str[l - 1])); return h;&#125;// 布尔值：#define hashboolean(t,p) hashpow2(t, p) // 轻量级用户数据, C函数和其他类型： // 都是将地址转换为整型 #define hashpointer(t,p) hashmod(t, point2uint(p)) 设置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// 设置值void luaH_set (lua_State *L, Table *t, const TValue *key, TValue *value) &#123; const TValue *slot = luaH_get(t, key); luaH_finishset(L, t, key, slot, value);&#125;// 完成设置void luaH_finishset (lua_State *L, Table *t, const TValue *key, const TValue *slot, TValue *value) &#123; // 没有发现key，则新建一个，否则直接设置值 if (isabstkey(slot)) luaH_newkey(L, t, key, value); else setobj2t(L, cast(TValue *, slot), value);&#125;// 在Hash表中添加一个新的keyvoid luaH_newkey (lua_State *L, Table *t, const TValue *key, TValue *value) &#123; Node *mp; TValue aux; // 检查key if (l_unlikely(ttisnil(key))) // key为nil luaG_runerror(L, \"table index is nil\"); else if (ttisfloat(key)) &#123; // key为浮点数 lua_Number f = fltvalue(key); lua_Integer k; if (luaV_flttointeger(f, &amp;k, F2Ieq)) &#123; // 将浮点数转为整型 setivalue(&amp;aux, k); key = &amp;aux; // 将其作为一个整数key插入 &#125; else if (l_unlikely(luai_numisnan(f))) luaG_runerror(L, \"table index is NaN\"); &#125; // 检查值 if (ttisnil(value)) return; // 值为nil就不插入了 mp = mainpositionTV(t, key); // 获取key的在hash槽的主位置 if (!isempty(gval(mp)) || isdummy(t)) &#123; // 获取到主位置 Node *othern; Node *f = getfreepos(t); // 获取一个空的Node（倒着查找） if (f == NULL) &#123; // 没有空余位置时，则需要进行扩容 rehash(L, t, key); // 重新计算hash并扩容 luaH_set(L, t, key, value); 插入值到成长后的表 return; &#125; lua_assert(!isdummy(t)); othern = mainposition(t, keytt(mp), &amp;keyval(mp)); // 获取主位置上key值对应的主位置 if (othern != mp) &#123; // 冲突节点在主位置外部（当前key对应的槽已经被其他的key占用了） // 移动冲突节点的空位置，并将其重新连接上 while (othern + gnext(othern) != mp) othern += gnext(othern); gnext(othern) = cast_int(f - othern); *f = *mp; // 拷贝冲突节点到空的位置上（next也一并拷贝了） if (gnext(mp) != 0) &#123; gnext(f) += cast_int(mp - f); // 重新修正next的位置 gnext(mp) = 0; &#125; setempty(gval(mp)); // 现在主位置空出来了，将其设置为空 &#125; else &#123; // 冲突节点在主位置上，这说明了是相同的key // 将空节点连接到冲突链上 if (gnext(mp) != 0) gnext(f) = cast_int((mp + gnext(mp)) - f); /* chain new position */ else lua_assert(gnext(f) == 0); gnext(mp) = cast_int(f - mp); mp = f; &#125; &#125; // 设置节点的key setnodekey(L, mp, key); luaC_barrierback(L, obj2gco(t), key); lua_assert(isempty(gval(mp))); // 设置节点的值 setobj2t(L, gval(mp), value);&#125; HelloWorld 通过HelloWorld函数程序来说明Lua程序是如何运行起来的。 创建LuaState并注册标注库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990int main()&#123; // 先构建一个lua state lua_State* L = luaL_newstate(); // 注册lua的标注库 luaL_openlibs(L); // 构造打印HelloWorld的Lua代码 std::string str = \"print('hello world')\"; // 加载并执行 luaL_dostring(L, str.c_str());&#125;// 创建一个Lua StateLUALIB_API lua_State *luaL_newstate (void) &#123; lua_State *L = lua_newstate(l_alloc, NULL); // 注册全局内存分配函数l_alloc，辅助库里的内存分配函数使用的是realloc if (l_likely(L)) &#123; lua_atpanic(L, &amp;panic); // 注册全局错误回调函数 lua_setwarnf(L, warnfoff, L); // 设置全局的默认警告函数，默认警告关闭 &#125; return L;&#125;// 需要注册的库static const luaL_Reg loadedlibs[] = &#123; &#123;LUA_GNAME, luaopen_base&#125;, &#123;LUA_LOADLIBNAME, luaopen_package&#125;, &#123;LUA_COLIBNAME, luaopen_coroutine&#125;, &#123;LUA_TABLIBNAME, luaopen_table&#125;, &#123;LUA_IOLIBNAME, luaopen_io&#125;, &#123;LUA_OSLIBNAME, luaopen_os&#125;, &#123;LUA_STRLIBNAME, luaopen_string&#125;, &#123;LUA_MATHLIBNAME, luaopen_math&#125;, &#123;LUA_UTF8LIBNAME, luaopen_utf8&#125;, &#123;LUA_DBLIBNAME, luaopen_debug&#125;, &#123;\"mylib\", luaopen_mylib&#125;, &#123;NULL, NULL&#125;&#125;;// 注册所有的库LUALIB_API void luaL_openlibs (lua_State *L) &#123; const luaL_Reg *lib; for (lib = loadedlibs; lib-&gt;func; lib++) &#123; luaL_requiref(L, lib-&gt;name, lib-&gt;func, 1); lua_pop(L, 1); &#125;&#125;// 调用注册函数LUALIB_API void luaL_requiref (lua_State *L, const char *modname, lua_CFunction openf, int glb) &#123; luaL_getsubtable(L, LUA_REGISTRYINDEX, LUA_LOADED_TABLE); // 从注册表中获取_LOADED对应的table lua_getfield(L, -1, modname); // 获取table中对应的模块（modname） if (!lua_toboolean(L, -1)) &#123; // 检查获取结果,如果没有就加载 lua_pop(L, 1); // 弹出刚lua_getfield获取的结果 lua_pushcfunction(L, openf); // 将要调用的c函数压入栈中 lua_pushstring(L, modname); // 传递到open函数的参数 lua_call(L, 1, 1); // 调用‘openf’函数去打开库（1:一个传入参数， 1：一个返回值） lua_pushvalue(L, -1); // 将调用的结果（一个table）再次压入栈中 lua_setfield(L, -3, modname); // 将返回的table保存到key(modname)中 &#125; lua_remove(L, -2); // 移除栈中的LOADED table if (glb) &#123; // 需要将其保存到全局表中？ lua_pushvalue(L, -1); // 先将lua_call返回的table压入栈顶 lua_setglobal(L, modname); // 将栈顶的table设置到全局表中的modname字段 &#125;&#125;// 看一个简单的table库的打开函数// 辅助库定义的一个创建库的函数，就是创建了一个table, 然后将tab_funcs中定义的字段名和函数绑定到table中#define luaL_newlib(L,l) \\ (luaL_checkversion(L), luaL_newlibtable(L,l), luaL_setfuncs(L,l,0))// 需要注册的函数static const luaL_Reg tab_funcs[] = &#123; &#123;\"concat\", tconcat&#125;, &#123;\"insert\", tinsert&#125;, &#123;\"pack\", tpack&#125;, &#123;\"unpack\", tunpack&#125;, &#123;\"remove\", tremove&#125;, &#123;\"move\", tmove&#125;, &#123;\"sort\", sort&#125;, &#123;NULL, NULL&#125;&#125;;// table库的打开函数LUAMOD_API int luaopen_table (lua_State *L) &#123; luaL_newlib(L, tab_funcs); return 1;&#125; 解析Lua代码生产指令 Lua解释器将Lua源码转换成Lua指令的过程。核心代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// 加载解析并执行Lua代码#define luaL_dostring(L, s) \\ (luaL_loadstring(L, s) || lua_pcall(L, 0, LUA_MULTRET, 0))// 加载并解析字符串LUALIB_API int luaL_loadstring (lua_State *L, const char *s) &#123; return luaL_loadbuffer(L, s, strlen(s), s);&#125;#define luaL_loadbuffer(L,s,sz,n) luaL_loadbufferx(L,s,sz,n,NULL)LUALIB_API int luaL_loadbufferx (lua_State *L, const char *buff, size_t size, const char *name, const char *mode) &#123; LoadS ls; // 将Lua代码放在ls中 ls.s = buff; ls.size = size; return lua_load(L, getS, &amp;ls, name, mode); // 加载&#125;LUA_API int lua_load (lua_State *L, lua_Reader reader, void *data, const char *chunkname, const char *mode) &#123; ZIO z; // 数据流缓存，用于解析Lua代码 int status; lua_lock(L); if (!chunkname) chunkname = \"?\"; luaZ_init(L, &amp;z, reader, data); // 初始化数据流缓存 status = luaD_protectedparser(L, &amp;z, chunkname, mode); if (status == LUA_OK) &#123; /* no errors? */ LClosure *f = clLvalue(s2v(L-&gt;top - 1)); /* get newly created function */ if (f-&gt;nupvalues &gt;= 1) &#123; /* does it have an upvalue? */ /* get global table from registry */ const TValue *gt = getGtable(L); /* set global table as 1st upvalue of 'f' (may be LUA_ENV) */ setobj(L, f-&gt;upvals[0]-&gt;v, gt); luaC_barrier(L, f-&gt;upvals[0], gt); &#125; &#125; lua_unlock(L); return status;&#125;// 解析（词法，语法，语义分析）Lua代码int luaD_protectedparser (lua_State *L, ZIO *z, const char *name, const char *mode) &#123; struct SParser p; // 创建一个解析器 int status; incnny(L); p.z = z; p.name = name; p.mode = mode; p.dyd.actvar.arr = NULL; p.dyd.actvar.size = 0; p.dyd.gt.arr = NULL; p.dyd.gt.size = 0; p.dyd.label.arr = NULL; p.dyd.label.size = 0; luaZ_initbuffer(L, &amp;p.buff); status = luaD_pcall(L, f_parser, &amp;p, savestack(L, L-&gt;top), L-&gt;errfunc);// 调用解析函数 luaZ_freebuffer(L, &amp;p.buff); luaM_freearray(L, p.dyd.actvar.arr, p.dyd.actvar.size); luaM_freearray(L, p.dyd.gt.arr, p.dyd.gt.size); luaM_freearray(L, p.dyd.label.arr, p.dyd.label.size); decnny(L); return status;&#125;// 解析函数static void f_parser (lua_State *L, void *ud) &#123; LClosure *cl; struct SParser *p = cast(struct SParser *, ud); int c = zgetc(p-&gt;z); // 读取第一个字符 if (c == LUA_SIGNATURE[0]) &#123; checkmode(L, p-&gt;mode, \"binary\"); cl = luaU_undump(L, p-&gt;z, p-&gt;name); //解析编译好的二进制格式 &#125; else &#123; checkmode(L, p-&gt;mode, \"text\"); cl = luaY_parser(L, p-&gt;z, &amp;p-&gt;buff, &amp;p-&gt;dyd, p-&gt;name, c); // 解析源码 &#125; lua_assert(cl-&gt;nupvalues == cl-&gt;p-&gt;sizeupvalues); luaF_initupvals(L, cl); // 初始化闭包的上值&#125; 解析完后会生成一个原型（Proto）对象，对象的code字段存放的就是对应的指令码，指令码有如下5种格式： 3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 9 8 7 6 5 4 3 2 1 0 iABC C(8) | B(8) |k| A(8) | Op(7) | iABx Bx(17) | A(8) | Op(7) | iAsBx sBx (signed)(17) | A(8) | Op(7) | iAx Ax(25) | Op(7) | isJ sJ(25) | Op(7) | 开头的7位表示操作码，后面的表示操作码对应的参数。 执行指令 123456789101112131415161718192021222324252627282930void luaV_execute (lua_State *L, CallInfo *ci) &#123; LClosure *cl; TValue *k; StkId base; const Instruction *pc; ...... base = ci-&gt;func + 1; // 函数在栈上的地址 // 主解释循环 for (;;) &#123; Instruction i; // 当前被执行的指令 StkId ra; // 寄存器A(就是一个栈值的地址) vmfetch(); // pc指针加1并给ra寄存器赋值 lua_assert(base == ci-&gt;func + 1); lua_assert(base &lt;= L-&gt;top &amp;&amp; L-&gt;top &lt; L-&gt;stack_last); lua_assert(isIT(i) || (cast_void(L-&gt;top = base), 1)); // 获取操作码 vmdispatch (GET_OPCODE(i)) &#123; vmcase(OP_MOVE) &#123; // 移动指令OP_MOVE将寄存器B的值赋值个寄存A, /* A B R[A] := R[B] */ setobjs2s(L, ra, RB(i)); vmbreak; &#125; vmcase(OP_LOADI) &#123; // 装载一个整形常量值到ra lua_Integer b = GETARG_sBx(i); setivalue(s2v(ra), b); vmbreak; &#125; ...... &#125; &#125;&#125; 总结 通过对源码的阅读，对Lua虚拟机的内部结构以及运行机制有更深入的理解。但是并不是所有细节和模块都进行了阅读，比如像Lua词法，语法和语义分析以及GC相关的代码就是直接跳过的。Lua本身也不是一篇文章就能理解完的，对于Lua虚拟机的学习先暂时告一段落。后续闲了会再对GC相关的代码进行阅读，至于词法，语法和语义分析这块的代码暂时不打算去学习，主要是在实际项目中暂时不需要这方面的内容。","categories":[],"tags":[{"name":"Lua","slug":"Lua","permalink":"http://yoursite.com/tags/Lua/"},{"name":"源码","slug":"源码","permalink":"http://yoursite.com/tags/%E6%BA%90%E7%A0%81/"}]},{"title":"Unity渲染管线介绍","slug":"Unity/Graphics/Unity渲染管线介绍","date":"2021-11-05T04:20:02.000Z","updated":"2025-04-26T11:06:24.130Z","comments":true,"path":"2021/11/05/Unity/Graphics/Unity渲染管线介绍/","link":"","permalink":"http://yoursite.com/2021/11/05/Unity/Graphics/Unity%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF%E4%BB%8B%E7%BB%8D/","excerpt":"什么渲染管线 渲染管线跟工厂的流水线一样，工厂的流水线为了生成最终的一个产品，每个流水线的工人只干一件特定的事情，最后将一个产品完成。计算机的图形渲染管线和工厂的流水线一样，只是图形渲染管线最终生成的是一张二维图片。 我们将一个渲染流程分成3个阶段：应用阶段、几何阶段、光栅化阶段。注意这仅仅是概念性阶段，每个阶段本身通常也是一个流水线系统，即包含了子流水线阶段。如下图： 应用阶段 从名字我们可以看出，这个阶段是由我们的应用主导的，因此通常由CPU负责实现。换句话说，我们这些开发者具有这个阶段的绝对控制权。 在这阶段中，开发者有3个主要任务： 1. 我们要准备好场景数据，例如摄像机的位置、视椎体、场景中包含了哪些模型、使用了哪些光源等等； 2. 为了渲染性能，我们往往需要做一个粗粒度剔除（culling）工作,以把哪些不可见的物体踢出去，这样就不需要再移交给几何阶段进行处理； 3. 最后我们要设置好每个模型的渲染状态，这些渲染状态包括但不限于它们使用的材质（漫反射颜色、高光反色颜色）、使用的纹理、使用的Shader等。 这一阶段最重要的输出是渲染所需的几何信息，即渲染图元（rendering primitives）。通俗来讲，渲染图元可以是点、线、三角面等。这些渲染图元将会被传递给下一个阶段——几何阶段。 几何阶段 几何阶段用于处理所有和我们要绘制的几何相关的事情。例如，决定需要绘制的图元是什么，怎么绘制它们，在哪里绘制它们。这一阶段通常在GPU上进行。 几何阶段负责和每个渲染图元打交道，进行逐顶点、逐多边形的操作。这个阶段可以进一步分成更小的流水阶段，这在下一章中会讲到。几何阶段的一个重要任务就是把顶点坐标变换到屏幕空间中，再交给光栅器进行处理。通过对输入的渲染图元进行多步处理后，这一阶段将会输出屏幕空间的二维顶点坐标、每个顶点对应的深度值，着色相等相关信息**，并传递给下一个阶段。 光栅化阶段 这一阶段将会使用上个阶段传递的数据来生成屏幕上的像素，并渲染出最终的图像。这个阶段也是在GPU上运行。光栅化的任务主要是决定每个渲染图元中的哪些像素应该被绘制在屏幕上。它需要对上一个阶段得到的逐顶点数据（例如纹理坐标，顶点颜色等）进行插值，然后在进行逐像素处理。和上一个阶段相似，光栅化阶段也可以分成更小的流水线阶段。","text":"什么渲染管线 渲染管线跟工厂的流水线一样，工厂的流水线为了生成最终的一个产品，每个流水线的工人只干一件特定的事情，最后将一个产品完成。计算机的图形渲染管线和工厂的流水线一样，只是图形渲染管线最终生成的是一张二维图片。 我们将一个渲染流程分成3个阶段：应用阶段、几何阶段、光栅化阶段。注意这仅仅是概念性阶段，每个阶段本身通常也是一个流水线系统，即包含了子流水线阶段。如下图： 应用阶段 从名字我们可以看出，这个阶段是由我们的应用主导的，因此通常由CPU负责实现。换句话说，我们这些开发者具有这个阶段的绝对控制权。 在这阶段中，开发者有3个主要任务： 1. 我们要准备好场景数据，例如摄像机的位置、视椎体、场景中包含了哪些模型、使用了哪些光源等等； 2. 为了渲染性能，我们往往需要做一个粗粒度剔除（culling）工作,以把哪些不可见的物体踢出去，这样就不需要再移交给几何阶段进行处理； 3. 最后我们要设置好每个模型的渲染状态，这些渲染状态包括但不限于它们使用的材质（漫反射颜色、高光反色颜色）、使用的纹理、使用的Shader等。 这一阶段最重要的输出是渲染所需的几何信息，即渲染图元（rendering primitives）。通俗来讲，渲染图元可以是点、线、三角面等。这些渲染图元将会被传递给下一个阶段——几何阶段。 几何阶段 几何阶段用于处理所有和我们要绘制的几何相关的事情。例如，决定需要绘制的图元是什么，怎么绘制它们，在哪里绘制它们。这一阶段通常在GPU上进行。 几何阶段负责和每个渲染图元打交道，进行逐顶点、逐多边形的操作。这个阶段可以进一步分成更小的流水阶段，这在下一章中会讲到。几何阶段的一个重要任务就是把顶点坐标变换到屏幕空间中，再交给光栅器进行处理。通过对输入的渲染图元进行多步处理后，这一阶段将会输出屏幕空间的二维顶点坐标、每个顶点对应的深度值，着色相等相关信息**，并传递给下一个阶段。 光栅化阶段 这一阶段将会使用上个阶段传递的数据来生成屏幕上的像素，并渲染出最终的图像。这个阶段也是在GPU上运行。光栅化的任务主要是决定每个渲染图元中的哪些像素应该被绘制在屏幕上。它需要对上一个阶段得到的逐顶点数据（例如纹理坐标，顶点颜色等）进行插值，然后在进行逐像素处理。和上一个阶段相似，光栅化阶段也可以分成更小的流水线阶段。 在Unity中，我们能够选择不同的渲染管线。Unity提供了3种(内建，SRP（URP和HDRP）)预定义的管线对于不同的兼容性和性能特性，我们也可以创建自己的渲染管线。每种类型的管线有这不同的兼容性适配不同类型的游戏，应用和平台，并且每种管线之间切换是非常困难的，因为不同的管线使用的shader不同。所以项目初期选择管线是非常重要的。Unity为我们提供了以下的渲染管线： 内建管线是Unity的默认渲染管线。它被定义为通用的渲染管线，限制了它的自定义。 URP是一个快速、可自定义的可编程渲染管线，让我们可以在各种平台上创建优化的图形。 HDRP是一个让您在高端平台上创建尖端的高保真图形的可编程管线。 自定义可以通过Unity的可编程渲染管线API自己创建渲染管线 各种渲染管线之间支持的特性对比 内建管线 内建渲染管线是Unity的传统的管线，它不是基于可编程渲染管线的。我们能够配置它不同的渲染路径，以及通过命令buffer(command buffer)和回调(callback)来对其进行简单的扩展。这个章节将包含以下的内容： 渲染路径 使用命令buffer扩展内建渲染管线 Shader实例 渲染路径 内建渲染管线支持不同的渲染路径，渲染路径被定义为对光照和着色的一些列操作(策略)。不同的渲染路径有不同的能力和性能表现。使用什么样的渲染路径取决于你项目的类型和目标硬件。我们能够在项目的Graphics窗口和Camera面板上去设置使用不同的渲染路径。如果在你运行的设备上不支持选择的渲染路径Unity则会给切换到兼容性更好的渲染路径，比如：在运行的设备上不支持延迟渲染，Unity则会切换到前向渲染路径。Unity总共为我们提供了4种渲染路径：前向渲染路径，延迟渲染路径, 旧版延迟渲染路劲和旧版顶点光照渲染路径，现在内建渲染管线的渲染路径主要是：前向渲染路径和延迟渲染路径。不同渲染路径的对比如下图： 不同渲染路径对比 前向渲染路径 前向渲染路径是Unity内建渲染管线的默认渲染渲染路径，它是一个通用的渲染路径。在前向渲染路径中实时光照是非常耗的。为了降低这种消耗,你可以选择在任意时刻有多少灯光是需要逐像素光照，其他的灯光则可以使用更低消耗的逐顶点或逐对象光照。如果你的项目没有大量的实时灯光，或者灯光效果不那么重要，前向渲染路径是一个不错的选择。 前向渲染渲染每个对象时使用一个或多个pass进行着色，使用pass的数量取决于对象被多少灯光影响。前向渲染也会对灯光本身进行不同的处理，具体取决于它们的设置和光照强度。 在前向渲染中，场景中一些（最多4个）最亮的灯光将进行逐像素渲染，然后最大4个点光源进行逐顶点光照，其他的灯光则使用球谐函数(Spherical Harmonics)进行计算。一个灯光是否进行逐像素光照或其他的光照方式取决于一下几点： - 灯光的渲染模式设置为“Not Important”时都是以逐顶点或SH进行光照的。 - 最亮的方向光总是逐像素的。 - 灯光的渲染模式设置为“Important”时都是逐像素光照的。 - 如果上面的灯光数量还没达到QualitySetting中设置的逐像素灯光的上限时，则其他灯光以亮度优先的方式优先作为逐像素灯光使用。 每个渲染对象在渲染时则使用以下规则： - 基础pass只用于一个逐像素方向光和所有的逐顶点/SH光。 - 其他逐像素灯光则在附加pass中着色，每个灯光执行一次附加pass。 例如，下图的一个对象被8个灯光（A-H）所影响 注意灯组重叠； 例如，最后一个逐像素光照混合到逐顶点光照模式中，因此当物体和光照四处移动时，“光爆”会减少。 基础pass 基础pass渲染对象时，使用逐像素方向光和所有的逐顶点/SH光，基础pass也添加任意的光照图，环境和自发光。方向光在基础pass上能够产生阴影。注意光照图对象不能从SH灯光中获取照明信息。当pass的flag设置为“OnlyDirectional”时， forward base pass仅作用于主方向光，环境/光照探针和光照图（SH和逐顶点灯光数据则不会被包含到pass的数据中）。 附加pass 附加pass对于每个影响此对象的额外（除基础pass中使用的逐像素灯光外）逐像素灯光执行渲染。这些灯光在附加pass默认是不会有阴影（因此，前向渲染支持一个方向光带阴影）。除非multi_compile_fwdadd_fullshadows被使用。 前向渲染性能注意事项 每个被动态逐像素灯光影响的像素在渲染此像素时将增加大量的工作，并且这也会导致一个对象的渲染pass被执行多次。在低端设备（像手机或低端的PC）上应该避免出现超过一个的逐像素灯管照明到任意单个对象上，并且对于静态对象应该使用光照图而不每帧去计算它们的光照信息。逐顶点动态灯光在顶点变换的时候将增加大量的工作，所以尽量避免多个灯光同时照明一个对象。 避免将两个距离很远的mesh进行合并，因为当有多个逐像素灯光时，每个mesh不得不被渲染多次，两个间隔比较远的mesh合并就不能被单独计算光照只有mesh有一个三角形被灯光覆盖整个mesh都将参与光照计算。 在渲染期间，Unity将查找一个mesh周围的多有灯光并且计算哪些灯光对其影响最大。在Quality窗口去设置逐像素灯光数量和逐定点灯光数量的上限。每个灯光基于其到mesh的远近以及照明强度确定其对mehs的重要性，纯粹根据游戏的环境有些灯光是比其他灯光更重要的。基于这个原因，每个灯光上有个Render Mode的设置，我们能够将其设置为Important或Not Important,灯光标记为Not Important有更低的渲染开销。 例如：一个赛车游戏，在昏暗的环境下开着前大灯，此时此灯光就应当将其设置为Improtant，像尾灯则可以设置为Not Important。 优化逐素光照可以节省 CPU 和 GPU 的工作：CPU 需要执行的DrawCall调用更少，GPU 需要处理的顶点和要栅格化的所有额外对象渲染的像素更少。 球谐函数(Spherical Harmonics) 灯光的渲染速度非常快。 它们在CPU上的开销很小，实际上对 GPU 来说是无额外开销的（也就是说，base pass 总是计算 SH 光照；但由于SH光照的工作方式，无论多少SH光照，成本都是完全相同的）。 SH灯光的缺点： - 由于SH是在逐顶点计算的，不是逐像素，这也意味着它们不支持灯光Cookies或法线贴图。 - SH照明的频率非常低。 使用SH灯无法实现锐利的照明过渡。它们也只影响漫反射照明。 - SH照明不是局部的； 靠近某个表面的点或点 SH 灯会“看起来不对”。 总而言之，SH灯通常对于小的动态物体来说已经足够了。 延迟渲染路径 延迟着色是内建渲染管线中光照和阴影保真度最高的渲染路径。延迟渲染需要GPU的支持，并且还有一些限制。它不支持半透明对象，正交投影和硬件抗锯齿（可以同通过后处理解决-边缘查找做模糊）。它对剔除蒙版的支持有限(最多只能使用四个剔除蒙版。也就是你的剔除图层蒙版必须至少包含所有层减去四个任意层，因此必须设置32层中的28层。否则你会得到不正确的图形)，并将Renderer.receiveShadows标志视为始终为真。 如果你的项目有大量的实时灯光并且需要更好的光照效果，并且你的目标硬件平台支持延迟渲染，那么这渲染路径将是个不错的选择。 在使用延迟着色的时候，对象是没有灯光数量限制的。所有的灯光都将是逐像素的，这意味着它们都与法线贴图正确交互，另外所有的灯光都能有cookies和阴影。 延迟着色的优点是光照的处理开销与灯光照亮的像素数成正比。光照的开销是由场景中的光量大小决定的不管它照亮了多少游戏对象。因此，可以通过让灯光照明范围更小来改善性能。延迟着色还具有高度一致和可预测的行为。每个灯光的效果都是逐像素计算的。 延迟着色的需求 延迟着色它需要显卡支持多目标渲染（MRT），着色模型（Shader Model）3.0并且支持深度渲染纹理。大多数在2006年以后生产的PC显卡都支持延迟着色。在移动端，延迟着色至少需要是OpenGL ES3.0。 前向渲染性能注意事项 在延迟渲染中实时灯光的性能消耗跟这个灯光所照明的像素是成正比的，不依赖于场景的复杂度。所以小的Point光源或Spot光源的性能消耗是非常低的，如果照明对象是被其他对象部分或完全遮挡的那么它的性能开销将更小。 当然，带阴影的灯光是比不带阴影的灯光更耗的。在延迟着色中，对于每个投射阴影的灯光，阴影投射对象还是需要被渲染一次或多次。 实现细节 在延迟渲染完成后，再使用前向渲染路径渲染那些不支持延迟着色的Shader对象。 在G-buffer中渲染目标（RT0-RT4）的默认布局如下，数据类型存储在每个渲染目标的不同通道中： RT0, ARGB32格式：漫反射颜色（RGB）, 遮挡（A）。 RT1, ARGB32格式：高光颜色（RGB）, 粗糙度(A)。 RT2, ARGB2101010格式：世界空间下的法线，未被使用（A）。 RT3, ARGB2101010(非HDR)或ARGBHalf(HDR每个通道16bits)格式：自发光+光照+lightmap+反射探针+深度缓冲和模板缓冲 所以默认的G-buffer布局每个像素是160bits/pixel(非HDR)或192bits/pixel(HDR)。 如果对于混合光照使用了Shadowmask或Distance Shadowmask模式,那么第5个渲染目标（RT）将被使用： RT5, ARGB32 格式：灯光遮挡值（RGBA） 因此G-buffer布局大小将增至每个192bits/pixel(非HDR)或224bits/pixel(HDR)。 如果硬件不支持五个并发渲染目标，则使用阴影遮罩的对象将回退到前向渲染路径。 当摄像机不使用 HDR 时，发射+光照缓冲区 (RT3) 采用对数编码，因此提供的动态范围高于 ARGB32 纹理通常可能提供的范围。 请注意，当摄像机使用 HDR 渲染时，不会为发射 + 光照缓冲区 (RT3) 创建单独的渲染目标；而是将摄像机渲染到的渲染目标（即传递给图像效果的渲染目标）用作 RT3。 G-Buffer pass G-Buffer pass 将每个游戏对象渲染一次。漫射和镜面反射颜色、表面平滑度、世界空间法线和自发光+环境光+反射光+光照贴图都将渲染到G-Buffer纹理中。G-Buffer纹理设置为全局着色器属性供着色器以后访问（_CameraGBufferTexture0 .._CameraGBufferTexture3 指定）。 光照pass 光照pass根据G-Buffer和深度来计算光照。光照是在屏幕空间内计算的，因此处理所需的时间与场景复杂性无关。光照将添加到发射缓冲区。 不穿过相机近平面的点光源和聚光灯被渲染为3D形状，并启用了Z缓冲区针对场景的测试。 这使得部分或完全遮挡的点光源和聚光灯的渲染成本非常低。穿过近平面的方向光和点或聚光灯被渲染为全屏四边形。 如果光源启用了阴影，那么也会在此通道中渲染并应用阴影。请注意，阴影并非是“无成本”的；需要渲染阴影投射物，并且必须应用更复杂的光照着色器。 唯一可用的光照模型是标准 (Standard) 光照模型。如果需要不同的模型，可修改光照pass着色器，方法是将内置着色器中的 Internal-DeferredShading.shader 文件的修改版本放入“Assets”文件夹中名为“Resources”的文件夹内。然后打开 Graphics 设置（菜单：Edit &gt; Project Settings，然后单击 Graphics 类别）。将“Deferred”下拉选单改为“Custom Shader”。然后，更改当前使用的着色器对应的着色器 (Shader) 选项。 扩展内建渲染管线 Unity为我们提供了命令缓冲区(CommandBuffers)来扩展内建渲染管线。一个命令缓冲区(CommandBuffers)是一系列渲染命令列表（例如：渲染目标设置，渲染某个mesh）。你能够安排这些命令在内建渲染管线的某些点上执行，这也允许你自定义并扩展Unity的渲染功能。你能使用Graphics.ExecuteCommandBuffer立即执行CommandBuffers，也可以在渲染管线的某个点执行。通过使用Camera.AddCommandBuffer并传入CameraEvent枚举, 以及Light.AddCommandBuffer并且传入LightEvent枚举来安排CommandBuffers的执行时机。 对于全部可以执行的命令，参见CommandBuffer API。有些命令仅在特定平台才支持，比如射线追踪就只在DX12上支持。 extending-unity-5-rendering-pipeline-command-buffers Shader基础模板 前向渲染路径Shader模板 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960Shader &quot;Unlit&#x2F;NewUnlitShader&quot;&#123; Properties &#123; _MainTex (&quot;Texture&quot;, 2D) &#x3D; &quot;white&quot; &#123;&#125; &#125; SubShader &#123; Tags &#123; &quot;RenderType&quot;&#x3D;&quot;Opaque&quot; &#125; LOD 100 Pass &#123; CGPROGRAM #pragma vertex vert #pragma fragment frag &#x2F;&#x2F; make fog work #pragma multi_compile_fog #include &quot;UnityCG.cginc&quot; struct appdata &#123; float4 vertex : POSITION; float2 uv : TEXCOORD0; &#125;; struct v2f &#123; float2 uv : TEXCOORD0; UNITY_FOG_COORDS(1) float4 vertex : SV_POSITION; &#125;; sampler2D _MainTex; float4 _MainTex_ST; v2f vert (appdata v) &#123; v2f o; o.vertex &#x3D; UnityObjectToClipPos(v.vertex); o.uv &#x3D; TRANSFORM_TEX(v.uv, _MainTex); UNITY_TRANSFER_FOG(o,o.vertex); return o; &#125; fixed4 frag (v2f i) : SV_Target &#123; &#x2F;&#x2F; sample the texture fixed4 col &#x3D; tex2D(_MainTex, i.uv); &#x2F;&#x2F; apply fog UNITY_APPLY_FOG(i.fogCoord, col); return col; &#125; ENDCG &#125; &#125; Fallback &quot;ExampleOtherShader&quot;&#125; 延迟渲染路径Shader模板 GBuffer Shader （每个Mesh执行一次） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108Shader &quot;Custom&#x2F;deferred&quot;&#123; &#x2F;&#x2F;unity参数入口 Properties &#123; _MainTex(&quot;贴图&quot;,2D)&#x3D;&quot;white&quot;&#123;&#125; _Diffuse(&quot;漫反射&quot;,Color) &#x3D; (1,1,1,1) _Specular(&quot;高光色&quot;,Color) &#x3D; (1,1,1,1) _Gloss(&quot;平滑度&quot;,Range(1,100)) &#x3D; 50 &#125; SubShader &#123; &#x2F;&#x2F;非透明队列 Tags &#123; &quot;RenderType&quot; &#x3D; &quot;Opaque&quot; &#125; LOD 100 &#x2F;&#x2F;延迟渲染 Pass &#123; &#x2F;&#x2F;设置 光照模式为延迟渲染 Tags&#123;&quot;LightMode&quot; &#x3D; &quot;Deferred&quot;&#125; CGPROGRAM &#x2F;&#x2F; 声明顶点着色器、片元着色器和输出目标 #pragma target 3.0 #pragma vertex vert #pragma fragment frag &#x2F;&#x2F;排除不支持MRT的硬件 &#x2F;&#x2F;#pragma exclude_renderers norm &#x2F;&#x2F; unity 函数库 #include&quot;UnityCG.cginc&quot; &#x2F;&#x2F;定义UNITY_HDR_ON关键字 &#x2F;&#x2F;在c# 中 Shader.EnableKeyword(&quot;UNITY_HDR_ON&quot;); Shader.DisableKeyword(&quot;UNITY_HDR_ON&quot;); &#x2F;&#x2F; 设定hdr是否开启 #pragma multi_compile __ UNITY_HDR_ON &#x2F;&#x2F; 贴图 sampler2D _MainTex; &#x2F;&#x2F; 题图uv处理 float4 _MainTex_ST; &#x2F;&#x2F; 漫反射光 float4 _Diffuse; &#x2F;&#x2F; 高光 float4 _Specular; &#x2F;&#x2F; 平滑度 float _Gloss; &#x2F;&#x2F; 顶点渲染器所传入的参数结构，分别是顶点位置、法线信息、uv坐标 struct a2v &#123; float4 pos:POSITION; float3 normal:NORMAL; float2 uv:TEXCOORD0; &#125;; &#x2F;&#x2F; 片元渲染器所需的传入参数结构，分别是像素位置、uv坐标、像素世界位置、像素世界法线 struct v2f &#123; float4 pos:SV_POSITION; float2 uv : TEXCOORD0; float3 worldPos:TEXCOORD1; float3 worldNormal:TEXCOORD2; &#125;; &#x2F;&#x2F; 延迟渲染所需的输出结构。前向渲染只需要输出1个Target，而延迟渲染的片元需要输出4个Target struct DeferredOutput &#123; &#x2F;&#x2F; RGB存储漫反射颜色，A通道存储遮罩 float4 gBuffer0:SV_TARGET0; &#x2F;&#x2F; RGB存储高光（镜面）反射颜色，A通道存储高光反射的指数部分，也就是平滑度 float4 gBuffer1:SV_TARGET1; &#x2F;&#x2F; RGB通道存储世界空间法线，A通道没用 float4 gBuffer2:SV_TARGET2; &#x2F;&#x2F; Emission + lighting + lightmaps + reflection probes (高动态光照渲染&#x2F;低动态光照渲染)用于存储自发光+lightmap+反射探针深度缓冲和模板缓冲 float4 gBuffer3:SV_TARGET3; &#125;; &#x2F;&#x2F; 顶点渲染器 v2f vert(a2v v) &#123; v2f o; &#x2F;&#x2F; 获取裁剪空间下的顶点坐标 o.pos &#x3D; UnityObjectToClipPos(v.pos); &#x2F;&#x2F; 应用uv设置，获取正确的uv o.uv &#x3D; TRANSFORM_TEX(v.uv, _MainTex); &#x2F;&#x2F; 获取顶点的世界坐标 o.worldPos &#x3D; mul(unity_ObjectToWorld, v.pos).xyz; &#x2F;&#x2F; 获取世界坐标下的法线 o.worldNormal &#x3D; UnityObjectToWorldNormal(v.normal); return o; &#125; &#x2F;&#x2F; 片元着色器 DeferredOutput frag(v2f i) &#123; DeferredOutput o; &#x2F;&#x2F; 像素颜色 &#x3D; 贴图颜色 * 漫反射颜色 fixed3 color &#x3D; tex2D(_MainTex, i.uv).rgb * _Diffuse.rgb; &#x2F;&#x2F; 默认使用高光反射输出！！ o.gBuffer0.rgb &#x3D; color; &#x2F;&#x2F; RGB存储漫反射颜色，A通道存储遮罩 o.gBuffer0.a &#x3D; 1; &#x2F;&#x2F; 漫反射的透明度 o.gBuffer1.rgb &#x3D; _Specular.rgb; &#x2F;&#x2F; RGB存储高光（镜面）反射颜色， o.gBuffer1.a &#x3D; _Gloss &#x2F; 100; &#x2F;&#x2F; 高光（镜面）反射颜色 的 o.gBuffer2 &#x3D; float4(i.worldNormal * 0.5 + 0.5, 1); &#x2F;&#x2F; RGB通道存储世界空间法线，A通道没用 &#x2F;&#x2F; 如果没开启HDR，要给颜色编码转换一下数据exp2，后面在lightpass2里则是进行解码log2 #if !defined(UNITY_HDR_ON) color.rgb &#x3D; exp2(-color.rgb); #endif &#x2F;&#x2F; Emission + lighting + lightmaps + reflection probes (高动态光照渲染&#x2F;低动态光照渲染)用于存储自发光+lightmap+反射探针深度缓冲和模板缓冲 o.gBuffer3 &#x3D; fixed4(color, 1); return o; &#125; ENDCG &#125; &#125;&#125; Light Shader (每个光源执行一次) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173Shader &quot;Unlit&#x2F;deferredLight&quot;&#123; SubShader &#123; &#x2F;&#x2F; 第一个pass用于合成灯光 Pass &#123; &#x2F;&#x2F; 由于像素信息已经经过深度测试，所以可以关闭深度写入 ZWrite Off &#x2F;&#x2F; 如果开启了LDR混合方案就是DstColor zero（当前像素 + 0）， &#x2F;&#x2F; 如果开启了HDR混合方案就是One One（当前像素 + 缓冲像素），由于延迟渲染就是等于把灯光渲染到已存在的gbuffer上，所以使用one one Blend [_SrcBlend] [_DstBlend] CGPROGRAM &#x2F;&#x2F; 定义运行平台 #pragma target 3.0 &#x2F;&#x2F; 我们需要所有的关于灯光的变体，使用multi_compile_lightpass #pragma multi_compile_lightpass &#x2F;&#x2F; 不使用nomrt着色器 #pragma exclude_renderers nomrt &#x2F;&#x2F;定义UNITY_HDR_ON关键字 &#x2F;&#x2F;在c# 中 Shader.EnableKeyword(&quot;UNITY_HDR_ON&quot;); Shader.DisableKeyword(&quot;UNITY_HDR_ON&quot;); &#x2F;&#x2F; 设定hdr是否开启 #pragma multi_compile __ UNITY_HDR_ON &#x2F;&#x2F; 定义顶点渲染器和片元渲染器的输入参数 #pragma vertex vert #pragma fragment frag &#x2F;&#x2F; 引入shader 相关宏宏 #include &quot;UnityCG.cginc&quot; #include &quot;UnityDeferredLibrary.cginc&quot; #include &quot;UnityGBuffer.cginc&quot; &#x2F;&#x2F;定义从 Deferred模型对象输入的屏幕像素数据 sampler2D _CameraGBufferTexture0;&#x2F;&#x2F; 漫反射颜色 sampler2D _CameraGBufferTexture1;&#x2F;&#x2F; 高光、平滑度 sampler2D _CameraGBufferTexture2;&#x2F;&#x2F; 世界法线 &#x2F;&#x2F;顶点渲染器输出参数结构，包含顶点坐标、法线 struct a2v &#123; float4 pos:POSITION; float3 normal:NORMAL; &#125;; &#x2F;&#x2F;片元渲染器输出结构，包含像素坐标、uv坐标 struct Deffred_v2f &#123; float4 pos: SV_POSITION; float4 uv:TEXCOORD; float3 ray : TEXCOORD1; &#125;; &#x2F;&#x2F; 顶点渲染器 Deffred_v2f vert(a2v v) &#123; Deffred_v2f o; &#x2F;&#x2F;将顶点坐标从模型坐标转化为裁剪坐标 o.pos &#x3D; UnityObjectToClipPos(v.pos); &#x2F;&#x2F; 获取屏幕上的顶点坐标 o.uv &#x3D; ComputeScreenPos(o.pos); &#x2F;&#x2F; 模型空间转 视角空间做i奥 o.ray &#x3D; UnityObjectToViewPos(v.pos) * float3(-1,-1,1); &#x2F;&#x2F; 插值 o.ray &#x3D; lerp(o.ray, v.normal, _LightAsQuad); return o; &#125; &#x2F;&#x2F;片段渲染器 &#x2F;&#x2F;设置片段渲染器输出结果的数据格式。如果开始hdr就使用half4,否则使用fixed4 #ifdef UNITY_HDR_ON half4 #else fixed4 #endif frag(Deffred_v2f i) : SV_Target &#123; &#x2F;&#x2F; 定义光照属性 float3 worldPos;&#x2F;&#x2F;像素的世界位置 float2 uv;&#x2F;&#x2F;uv half3 lightDir;&#x2F;&#x2F;灯光方向 float atten;&#x2F;&#x2F; 衰减 float fadeDist;&#x2F;&#x2F; 衰减距离 &#x2F;&#x2F;计算灯光数据，并填充光照属性数据，返回灯光的坐标，uv、方向衰减等等 UnityDeferredCalculateLightParams(i, worldPos, uv, lightDir, atten, fadeDist); &#x2F;&#x2F; 灯光颜色 half3 lightColor &#x3D; _LightColor.rgb * atten; &#x2F;&#x2F;gbuffer与灯光合成后的像素数据 half4 diffuseColor &#x3D; tex2D(_CameraGBufferTexture0, uv);&#x2F;&#x2F; 漫反射颜色 half4 specularColor &#x3D; tex2D(_CameraGBufferTexture1, uv);&#x2F;&#x2F; 高光颜色 float gloss &#x3D; specularColor.a * 100;&#x2F;&#x2F;平滑度 half4 gbuffer2 &#x3D; tex2D(_CameraGBufferTexture2, uv);&#x2F;&#x2F; 法线 float3 worldNormal &#x3D; normalize(gbuffer2.xyz * 2 - 1);&#x2F;&#x2F; 世界法线 &#x2F;&#x2F; 视角方向 &#x3D; 世界空间的摄像机位置 - 像素的位置 fixed3 viewDir &#x3D; normalize(_WorldSpaceCameraPos - worldPos); &#x2F;&#x2F; 计算高光的方向 &#x3D; 灯光方向与视角方向中间的点 fixed3 halfDir &#x3D; normalize(lightDir + viewDir); &#x2F;&#x2F; 漫反射 &#x3D; 灯光颜色 * 漫反射颜色 * max（dot（像素世界法线， 灯光方向）） half3 diffuse &#x3D; lightColor * diffuseColor.rgb * max(0,dot(worldNormal, lightDir)); &#x2F;&#x2F; 高光 &#x3D; 灯光颜色 * 高光色 * pow(max(0,dot(像素世界法线，计算高光的方向)), 平滑度); half3 specular &#x3D; lightColor * specularColor.rgb * pow(max(0,dot(worldNormal, halfDir)),gloss); &#x2F;&#x2F; 像素颜色 &#x3D; 漫反射+高光，透明度为1 half4 color &#x3D; float4(diffuse + specular,1); &#x2F;&#x2F;如果开启了hdr则使用exp2处理颜色 #ifdef UNITY_HDR_ON return color; #else return exp2(-color); #endif &#125; ENDCG &#125; &#x2F;&#x2F;转码pass,主要用于LDR转码 Pass &#123; &#x2F;&#x2F;使用深度测试，关闭剔除 ZTest Always Cull Off ZWrite Off &#x2F;&#x2F;模板测试 Stencil &#123; ref[_StencilNonBackground] readMask[_StencilNonBackground] compback equal compfront equal &#125; CGPROGRAM &#x2F;&#x2F;输出平台 #pragma target 3.0 #pragma vertex vert #pragma fragment frag &#x2F;&#x2F; 剔除渲染器 #pragma exclude_renderers nomrt &#x2F;&#x2F; #include &quot;UnityCG.cginc&quot; &#x2F;&#x2F;缓冲区颜色 sampler2D _LightBuffer; struct a2v &#123; float4 pos:POSITION; float2 uv:TEXCOORD0; &#125;; struct v2f &#123; float4 pos:SV_POSITION; float2 uv:TEXCOORD0; &#125;; &#x2F;&#x2F;顶点渲染器 v2f vert(a2v v) &#123; v2f o; &#x2F;&#x2F; 坐标转为裁剪空间 o.pos &#x3D; UnityObjectToClipPos(v.pos); o.uv &#x3D; v.uv; &#x2F;&#x2F; 通常用于判断D3D平台，在开启抗锯齿的时候图片采样会用到 #ifdef UNITY_SINGLE_PASS_STEREO o.uv &#x3D; TransformStereoScreenSpaceTex(o.uv,1.0); #endif return o; &#125; &#x2F;&#x2F;片段渲染器 fixed4 frag(v2f i): SV_Target &#123; return -log2(tex2D(_LightBuffer,i.uv)); &#125; ENDCG &#125; &#125;&#125; SRP 可编程渲染管线（SRP）是一种可以通过C#脚本控制渲染的技术。URP和HDRP是基于SRP技术开发的。SRP是一个轻量级的API层，我们可以通过这些C# API来安排和配置渲染命令。Unity传递这些命令到底层的图形架构，然后发送指令到图形API。我们可以基于SRP创建自定义的渲染管线。 渲染管线实例和渲染管线资源 每个基于SRP的渲染管线都有两个关键的元素： 渲染管线实例， 这是一个实例类定义渲染管线的功能。它继承至RenderPipeline, 并且覆写它的Render()方法。 渲染管线资源， 这是一个存储渲染管线实例使用的数据的资源，这个脚本继承至RenderPipelineAsset类并且覆写其中的CreatePipeline方法。 ScriptableRenderContext ScriptableRenderContext是自定义C#代码和Unity底层图形代码之间沟通的接口类。 入口点和回调 当工作在SRP时，使用以下这些确保Unity在特定的时候嗲用你的C#代码 RenderPipeline.Render 是SRP的主入口函数，Unity将自动的调用此函数。如果你写一个自定渲染管线这就你代码开始的地方。 RenderPipelineManager 渲染管线的管理类，此类提供了一些事件，可以订阅这些事件然后执行你的特定代码。 beginFrameRendering 将产生GC 使用beginContextRendering代替 endFrameRendering 将产生GC 使用endContextRendering代替 beginContextRendering endContextRendering beginCameraRendering endCameraRendering 安排和执行渲染命令 在SRP中，我们可以通过CommandBuffer或直接调用ScriptableRenderContext中的API来安排和执行渲染命令。 使用ScriptableRenderContext APIs 在SRP中，ScriptableRenderContext做为与底层图形沟通的接口。SRP的渲染工作是延迟执行的；你可以使用ScriptableRenderContext去构建一个渲染命令列表，然后告诉Unity去执行它们，最后Unity图形架构发送命令到图形API。 安排渲染命令，有如下方式： 传递一个CommandBuffer到ScriptableRenderContext, 通过使用ScriptableRenderContext.ExecuteCommandBuffer. 直接调用ScriptableRenderContext中的API，比如：ScriptableRenderContext.Cull或ScriptableRenderContext.DrawRenderers. 当安排好命令后，通过调用ScriptableRenderContext.Submit方法告诉Unity去执行你的命令。注意：不论你是使用的CommandBuffer还是直接调用的API, 在ScriptableRenderContext中都同样的，并且也只有调用了Submit才会去执行。 下面这个实例说明了如何通过CommandBuffer安排和执行命令去清除渲染目标： 1234567891011121314151617181920using UnityEngine;using UnityEngine.Rendering;public class ExampleRenderPipeline : RenderPipeline&#123; public ExampleRenderPipeline() &#123; &#125; protected override void Render(ScriptableRenderContext context, Camera[] cameras) &#123; // 创建并安排命令 var cmd = new CommandBuffer(); cmd.ClearRenderTarget(true, true, Color.red); context.ExecuteCommandBuffer(cmd); cmd.Release(); // 告诉Unity执行命令 context.Submit(); &#125;&#125; 自定义渲染管线 Unity提供了两个内置的SRP渲染管线:HDRP和URP。虽然这个两个渲染管线给我们提供了自定义的扩展选项，但是如果你想获取的更多的控制，则可以创建一个自定义的渲染管线。 基于SRP创建自定渲染管线 此节将介绍如何基于SRP创建自定义的渲染管线。 创建一个项目并安装依赖包 这些说明向您展示如何使用SRP Core包创建自定义渲染管线。 SRP Core是由Unity提供的一个包，其中包含可重用的代码，可帮助您创建自己的渲染管线，包括用于使用特定于平台的图形API的样板代码、用于常见渲染操作的实用程序函数和着色器库这些着色器也被URP和HDRP使用的。有关SRP Core的更多信息，请参阅SRP Core软件包文档。 创建一个新的Unity项目 在SRP源码仓库中下载对应Unity版本的SRP包。 将下载的包放到工程中 com.unity.render-pipelines.core : SRP的核心包 com.unity.render-pipelines.shadergraph ：一个可视化的shader编辑器 com.unity.render-pipelines.visualeffectgraph ：视觉效果图像编辑器 如果你觉得完全去自定一个渲染管线过于复杂，那么你可以基于URP或HDRP进行修改，扩展和自定义你需要修改的部分,需要安装的包如下： URP： com.unity.render-pipelines.core com.unity.render-pipelines.shadergraph com.unity.render-pipelines.universal HDRP: com.unity.render-pipelines.core com.unity.render-pipelines.shadergraph com.unity.render-pipelines.high-defintion 创建渲染管线资源和渲染管线实例 创建自定义的渲染管线，在项目中必须包含： 一个继承至RenderPipelineAsset类并覆写了CreatePipeline方法的脚本。这个脚本定义了渲染管线的资源。 一个继承至RenderPipeline，并且覆写了Render方法的脚步，这个脚本定义了渲染管线的实例。 因为这些元素都是紧密相关的，必须同时创建。 创建基础的渲染管线资源和实例 实例代码如下： 渲染资源脚本： 1234567891011using UnityEngine;using UnityEngine.Rendering;[CreateAssetMenu(menuName = \"Rendering/ExampleRenderPipelineAsset\")]public class ExampleRenderPipelineAsset : RenderPipelineAsset&#123; // Unity在第一帧渲染前调用这个方法，如果渲染管线的设置改变，Unity将销毁当前的渲染管线实例，并调用此方法重新创建一个 protected override RenderPipeline CreatePipeline() &#123; return new ExampleRenderPipelineInstance(); &#125;&#125; 渲染实例脚本： 12345678910using UnityEngine;using UnityEngine.Rendering; public class ExampleRenderPipelineInstance : RenderPipeline&#123; public ExampleRenderPipelineInstance() &#123; &#125; protected override void Render (ScriptableRenderContext context, Camera[] cameras) &#123; // 渲染主入口 &#125;&#125; 创建可配置的渲染管线资源和实例 默认情况下，渲染管线资源存储关于用于渲染的渲染管线实例以及默认材质和着色器的信息。 在您的RenderPipelineAsset脚本中，您可以扩展您的渲染管线资源，以便它存储额外的数据，并且您可以在您的项目中拥有多个具有不同配置的不同渲染管线资源。例如，您可以使用渲染管线资源来保存每个不同硬件层的配置数据。 高清渲染管线 (HDRP) 和通用渲染管线 (URP) 包括这方面的示例。 以下示例展示了如何创建一个RenderPipelineAsset脚本，该脚本定义一个带有公共数据的渲染管线资源，您可以使用Inspector窗口为每个实例设置这些数据，以及在其构造函数中接收渲染管线资源并使用这些数据。 实例代码如下： 渲染资源脚本： 1234567891011121314using UnityEngine;using UnityEngine.Rendering;[CreateAssetMenu(menuName = \"Rendering/ExampleRenderPipelineAsset\")]public class ExampleRenderPipelineAsset : RenderPipelineAsset&#123; // 定义的数据 public Color exampleColor; public string exampleString; protected override RenderPipeline CreatePipeline() &#123; return new ExampleRenderPipelineInstance(this); &#125;&#125; 渲染实例脚本： 123456789101112131415using UnityEngine;using UnityEngine.Rendering; public class ExampleRenderPipelineInstance : RenderPipeline&#123; private ExampleRenderPipelineAsset renderPipelineAsset; public ExampleRenderPipelineInstance(ExampleRenderPipelineAsset asset) &#123; renderPipelineAsset = asset; &#125; protected override void Render(ScriptableRenderContext context, Camera[] cameras) &#123; Debug.Log(renderPipelineAsset.exampleString); &#125;&#125; 创建一个简单的渲染循环 渲染循环是在单个帧中发生的所有渲染操作的术语。此节代码示例说明使用SRP的基础概念。你能使用这些信息去构建自己的自定可编程渲染管线，也可以用于理解SRP是如何工作的。 准备你的项目 在开始写渲染循环之前，你必须准备你的项目，步骤如下： 创建一个SRP兼容的shader。 创建一个或多个渲染的对象。 创建自定义SRP的基础结构。 可选步骤，如果你计划去扩展简单的自定SRP去添加一些更复杂的功能，安装SRP核心包。SRP核心包包含SRP的shader库（能使用这些shader去保证Batcher的兼容性），并且还包含了一些通用的操作逻辑功能。更多信息，参见SRP核心包文档。 创建一个SRP兼容shader 在SRP中，使用Pass的LightMode标记确定几何对象将如何绘制。更多Pass的标记参见ShaderLab 本节的任务是使用LightMode为ExampleLightModeTag的标记来创建一个非常简单的无光照的Shader对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243&#x2F;&#x2F; 无光照，不兼容SRP Batcher的ShaderShader &quot;Examples&#x2F;SimpleUnlitColor&quot;&#123; SubShader &#123; Pass &#123; &#x2F;&#x2F; 必须于ScriptableRenderContext.DrawRenderer中的ShaderTagId匹配 Tags &#123; &quot;LightMode&quot; &#x3D; &quot;ExampleLightModeTag&quot;&#125; HLSLPROGRAM #pragma vertex vert #pragma fragment frag float4x4 unity_MatrixVP; float4x4 unity_ObjectToWorld; struct Attributes &#123; float4 positionOS : POSITION; &#125;; struct Varyings &#123; float4 positionCS : SV_POSITION; &#125;; Varyings vert (Attributes IN) &#123; Varyings OUT; float4 worldPos &#x3D; mul(unity_ObjectToWorld, IN.positionOS); OUT.positionCS &#x3D; mul(unity_MatrixVP, worldPos); return OUT; &#125; float4 frag (Varyings IN) : SV_TARGET &#123; return float4(0.5,1,0.5,1); &#125; ENDHLSL &#125; &#125;&#125; 创建渲染对象 为了测试渲染循环，必须创建一些东西进行渲染。Unity标准流程。 创建自定义SRP的基础结构 前面章节所描述的步骤： 创建渲染管线资源和渲染管线实例脚本 将当前创建的管线资源设置为激活的渲染管线 创建渲染循环 在一个简单的渲染循环中，基础的操作是： 清理渲染目标，主要的目的是移除上一帧渲染的对象。 剔除，主要作用是过滤掉摄像机完全看不见的对象。 绘制，告诉GPU那些对象要绘制，以及如何绘制。 清理渲染目标 清理意味着清楚上一帧绘制的所有对象。渲染目标一般是屏幕，当然也能是RT。 12345678910111213141516using UnityEngine;using UnityEngine.Rendering;public class ExampleRenderPipeline : RenderPipeline &#123; public ExampleRenderPipeline() &#123; &#125; protected override void Render (ScriptableRenderContext context, Camera[] cameras) &#123; var cmd = new CommandBuffer(); cmd.ClearRenderTarget(true, true, Color.black); context.ExecuteCommandBuffer(cmd); cmd.Release(); context.Submit(); &#125;&#125; 剔除 剔除是过滤掉相机不可见的对象。在SRP中，剔除需要如下步骤： 构建关于相机的ScriptableCullingParameters结构体，可以通过调用Camera.TryGetCullingParameters来获取。 可以选步骤，可以手动的方式构建ScriptableCullingParameters结构体。 调用ScriptableRenderContext.Cull, 并将结果存儲在CullingResults结构体中。 代码如下： 123456789101112131415161718192021222324252627using UnityEngine;using UnityEngine.Rendering;public class ExampleRenderPipeline : RenderPipeline &#123; public ExampleRenderPipeline() &#123; &#125; protected override void Render (ScriptableRenderContext context, Camera[] cameras) &#123; var cmd = new CommandBuffer(); cmd.ClearRenderTarget(true, true, Color.black); context.ExecuteCommandBuffer(cmd); cmd.Release(); // 遍历所有相机 foreach (Camera camera in cameras) &#123; // 获取剔除参数 camera.TryGetCullingParameters(out var cullingParameters); // 剔除并存储剔除结果 var cullingResults = context.Cull(ref cullingParameters); &#125; // 通知图像API执行命令 context.Submit(); &#125;&#125; 绘制 绘图是指示图形API以给定设置绘制一组给定几何图形的过程。在SRP中的绘制应有以下步骤： 执行剔除（上一节描述的），并存储结果在CullingResults结构体中。 创建并且配置FilteringSetting结构体，此结构描述了如果去过滤剔除结果。 创建并配置DrawingSettings结构体， 此结构描述哪些几何体将被绘制以及如何绘制它。 可以选步骤，默认Unity基于Shader对象设置渲染状态(深度，蒙版和模板等)。如果你想对一些或所有打算绘制的几何对象覆写这些渲染状态，你能使用RenderStateBlock结构体去做这件事情。 调用ScriptableRenderContext.DrawRenderers，并传递上面创建的结构体对象。Unity将根据上面的设置的信息进行绘制。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152using UnityEngine;using UnityEngine.Rendering;public class ExampleRenderPipeline : RenderPipeline &#123; public ExampleRenderPipeline() &#123; &#125; protected override void Render (ScriptableRenderContext context, Camera[] cameras) &#123; var cmd = new CommandBuffer(); cmd.ClearRenderTarget(true, true, Color.black); context.ExecuteCommandBuffer(cmd); cmd.Release(); // 遍历所有相机 foreach (Camera camera in cameras) &#123; // 获取剔除参数 camera.TryGetCullingParameters(out var cullingParameters); // 剔除并存储剔除结果 var cullingResults = context.Cull(ref cullingParameters); //设置视图、投影和剪切平面全局着色器变量。 context.SetupCameraProperties(camera); //根据 LightMode Pass 标签值，告诉Unity要绘制哪个几何图形 ShaderTagId shaderTagId = new ShaderTagId(\"ExampleLightModeTag\"); // 告诉Unity，基于当前相机该如何去排序 var sortingSettings = new SortingSettings(camera); // 创建用于描述需要绘制的几何体，以及如何绘制他们 DrawingSettings drawingSettings = new DrawingSettings(shaderTagId, sortingSettings); // 告诉Unity如何去过滤裁剪的结果，进一步指定要绘制的几何体 // 使用FilteringSettings.defaultValue指定不进行过滤 FilteringSettings filteringSettings = FilteringSettings.defaultValue; // 基于前面的设置绘制几何体 context.DrawRenderers(cullingResults, ref drawingSettings, ref filteringSettings); // 安排天空盒子的绘制 if (camera.clearFlags == CameraClearFlags.Skybox &amp;&amp; RenderSettings.skybox != null) &#123; context.DrawSkybox(camera); &#125; // 提交执行 context.Submit(); &#125; &#125;&#125; URP URP是Unity基于SRP创建的渲染管线。URP提供了艺术家友好的工作流程让你能够快速且容易的创建跨越多个平台的图形，从移动端到高端的控制台和PCs。 渲染管线概念 URP资源 任何基于URP的Unity项目都必须有URP资源去配置相关的设置。URP资源控制了各种图形和质量设置。它是继承至RenderPipelineAsset的可编程对象。当你在Graphics设置中将URP资源后，Unity将自动切换到URP管线。我们可以有多个URP资源并且可以来回切换。 具体的URP资源的配置信息参见官方文档 URP全局设置 如果你的项目安装了URP的包，Unity将在Project Setting窗口中的Graphic页签里显示URP的全局设置。 具体设置参见官方文档 渲染器 URP的渲染器实现了两个渲染路径： 前向渲染路径 延迟渲染路径 两渲染路径的对比如下： 渲染器特性 渲染器特性是一种资源，可让您将额外的渲染通道添加到URP渲染器并配置其行为。URP包含预构建的名为渲染对象 (Render Objects) 的渲染器特性。有关如何向渲染器添加渲染器特性的信息，请参阅如何向渲染器添加渲染器功能页面。 Unity自带支持3种渲染特性： 1. 渲染对象特性（Render Objects） 2. 环境光遮挡（Ambient Occlusion） 3. 贴花（Decal） 灯光 使用URP能实现适合艺术风格范围内的真实光照。Unity的所有渲染管线共享光照功能，但是不同的渲染管线也有一些重要的不同。 URP不同于Unity的通用光照功能的特性如下： 在灯光组件的Inspector窗口中，显示了一些URP特定的选项 在灯光组件上有Universal Additional Light Data组件，存储了与URP管线相关的数据。 从URP12开始支持Enlighten的实时全局光照。 内建渲染管线和URP灯光之间的特性差异，能参见灯光特性对比表 相机 URP中的相机是基于Unity的标准相机的，但是也有一些明显的区别。如下： Universal Additional Camera Data组件，扩展标准相机的功能，并允许URP存储相机相关的数据。 渲染类型（Render Type）设置，定义了URP中的两种类型的相机：Base和Overlay。 相机堆叠（Camera Stacking）系统, 允许你去层化多个相机的输出到一个合并后的输出。 Volume系统，允许基于场景的给定位置去应用后处理效果。 相机组件，在Inspector中暴露了URP特定的选项。 后处理 URP包含了一个整合的后处理效果，如果你使用的URP，将不再需要额外安装后处理包了。URP不兼容Post Processing Stack v2包。URP使用Volume系统框架来现实后处理效果。Volumes能够覆写或扩展场景属性，依赖每个Volume相对于相机的位置。URP 为Volumes实现了专用的GameObjects：Global Volume, Box Volume, Sphere Volume, Convex Mesh Volume. Volume组件引用了一个Volume Profile的文件。Volume Profile文件包含了每个属性的默认值并默认这些属性是隐藏的。 Volume Overrides让你改变或扩展这些在Volume Profile上的默认值。 在运行时，URP将便利那些启用了Volume组件的激活对象，比并且确定每个Volume组件的在最终场景设置中的贡献。URP使用相机位置和Volume组件属性去计算这些贡献值。URP将从所有有贡献的Volumes中插值得到最终的属性值。 URP支持的后处理效果有： - Bloom - Channel Mixed - Chromatic Aberration - Color Adjustments - Color Curves - Depth of Field - Film Grain - Lens Distortion - Lift Gamma Gain - Motion Blur - Panini Projection - Shadows Midtones Highlights - Split Toning - Tonemapping - Vignette - White Balance - Lens Flare 着色器和材质 URP提供了以下的一些着色器来满足大多数的使用场景： Complex Lit （包含所有Lit着色器的共功能并添加一些高级材质特性，此着色器中的某些功能可能会占用更多资源并且需要Unity Shader Model 4.5的硬件） Lit （PBS光照模型） Simple Lit （Blinn-Phong光照模型） Baked Lit （烘培光） Unlit （无光照） Terrain Lit （地形） Particles Lit （粒子PBS光照模型） Particles Simple Lit（粒子Blinn-Phong光照模型） Particles Unlit （粒子无光照） SpeedTree （SpeedTree特定着色器） Decal （贴花） 选择着色器 URP提供了PBS和非PBS的光照着色器。对于PBS，使用Lit着色器。你能够使用它在所有的平台。这个Shader的质量适应性依赖平台，但保持在所有平台都是基于物理的渲染。Unity内建渲染管线的Standard和Standard（Specular）着色器都对应于URP的Lit着色器。内建渲染管线的和URP的着色器映射表参见着色器映射。 如果您的目标是功能较弱的设备，或者你的项目只有简单的着色需求，使用Simple Lit着色器，此着色器是非PBS的光照模型。 如果你不需要实时灯光，或者宁愿只使用烘焙光照和采样全局光照，选择Baked Lit着色器。 如果你不需要光照，可以使用Unlit着色器。 SRP批处理器兼容 确保SRP批处理器兼容着色器需要满足以下要求： 声明所有材质属性在一个名为UnityPerMaterial的CBUFFER中。 声明所有的内建引擎属性（比如：unity_ObjectToWorld 或 unity_WorldTransformParams）在一个名为UnityPerDraw的CBUFFER中。 着色器剔除 Unity从单个Shader源文件编译许多Shader变体。着色器变体的数量取决于您在着色器中包含多少关键字。 在默认着色器中，通用渲染管线 (URP) 使用一组关键字用于照明和阴影。 URP可以排除某些着色器变体，具体取决于URP资源中哪些功能处于活动状态。当您禁用 URP 资源中的某些功能时，管道会从构建中“剥离”相关的着色器变体。 剥离着色器可让您获得更小的构建尺寸和更短的构建时间。 如果您的项目永远不会使用某些功能或关键字，这将非常有用。例如，您可能有一个项目，您从不将阴影用于定向灯。 如果没有着色器剥离，具有定向阴影支持的着色器变体仍保留在构建中。 如果您知道根本不会使用这些阴影，则可以取消选中 URP 资源中的投射阴影以获得主或附加方向灯。 URP 然后从构建中去除这些着色器变体。具体的剔除方法参见剔除可编程着色器变体 URP ShaderLab Pass 标记 LightMode标记让管道确定在执行渲染管道的不同部分时使用哪个Pass。如果你在Pass中不设置LightMode标记，URP将使用SRPDefaultUnlit作为LightModede的值。在URP中， LightMode标记可以有下面的值： 属性 描述 UniversalForward Pass渲染对象几何体并评估所有光的贡献。 URP在前向渲染路径中使用此标记值。 UniversalGBuffer Pass渲染对象几何体但不评估任何光贡献。 URP在延迟渲染路径中使用此标记值。 UniversalForwardOnly Pass渲染对象几何体并评估所有光贡献，类似于LightMode为UniversalForward时。与UniversalForward的不同之处在于，URP可以将Pass用于前向和延迟渲染路径。如果在URP使用延迟渲染路径时某个通道必须使用前向渲染路径渲染对象，请使用此值。 例如，如果URP使用延迟渲染路径渲染场景并且场景包含具有不适合GBuffer的着色器数据的对象（例如透明涂层法线），则使用此标记。如果着色器必须同时在前向和延迟渲染路劲中渲染，请使用UniversalForward和UniversalGBuffer标签值声明两个Pass。 如果着色器必须使用前向渲染路径进行渲染，而不管URP渲染器使用的渲染路径，请仅声明一个将LightMode标记设置为UniversalForwardOnly的Pass。 Universal2D Pass渲染对象并评估2D光贡献。 URP在2D渲染器中使用此标记值。 ShadowCaster Pass将物体深度从灯光的角度渲染到阴影贴图或深度纹理中。 DepthOnly Pass仅将来自相机视角的深度信息渲染到深度纹理中。 Meta Unity仅在Unity编辑器中烘焙光照贴图时执行此Pass。 Unity在构建Player时会从着色器中剥离此通道。 SRPDefaultUnlit 渲染对象时使用此LightMode标记值绘制额外的Pass。 应用示例：绘制对象轮廓。 此标记值对前向和延迟渲染路径均有效。当Pass没有LightMode标记时，URP使用此标记值作为默认值。 2D图形特性 URP包含的2D功能是2D Lighting图形管道，可让您创建2D灯光和2D照明效果;并且用于在您的项目中实现像素化视觉风格的 2D 像素完美相机。 以下是包的 Light 2D 组件中包含的不同 2D 灯光类型： Freeform Sprite Spot Global URP附带的2D光照系统由一组艺术家友好的工具和运行时组件组成，可帮助您通过核心Unity组件（例如 Sprite Renderer）和2D光照组件快速创建光照2D场景，这些组件充当熟悉的3D光照的2D对应物。这些工具旨在与2D渲染器无缝集成，例如 Sprite Renderer、Tilemap Renderer 和 Sprite Shape Renderer。 该工具和组件系统针对移动系统和在多个平台上运行进行了优化。 2D灯光实现细节 2D Lighting图形管线渲染过程可以分为2个不同的阶段： 1. 绘制光照渲染纹理 2. 绘制渲染器 灯光渲染纹理是包含有关屏幕空间中灯光颜色和形状信息的渲染纹理。这两个阶段仅针对每组明显照明的 Light Layers 重复。 换句话说，如果 Sorting Layers 1 到 4 具有完全相同的一组 Lights，则它只会执行上述一组操作。默认设置允许在绘制渲染器之前提前绘制多个批次，以减少目标切换。 理想的设置将允许管道为所有批次渲染光渲染纹理，然后才继续绘制渲染器。 这可以防止加载和卸载颜色目标。 有关更多详细信息，请参阅优化。 前期：计算排序层批处理 在继续渲染阶段之前，2D Lighting 图形管道首先分析场景，以评估哪些图层可以在单个绘制操作中批量处理。 以下是确定图层是否一起批处理的标准： 它们是连续的层。 他们共享完全相同的一组灯光。 强烈建议批处理尽可能多的图层，以最大限度地减少光渲染纹理绘制操作的数量并提高性能。 阶段 1：绘制光渲染纹理 在预阶段批处理之后，管道然后为该批处理绘制光照纹理。这基本上将灯光的形状绘制到渲染纹理上。 根据灯光的设置，可以使用Additive或Alpha Blended将灯光的颜色和形状混合到目标灯管的渲染纹理上。 阶段一 值得注意的是，仅当至少有一个2D光以它为目标时才会创建光渲染纹理。 例如，如果一个图层的所有灯光只使用 Blendstyle #1，那么只会创建一个灯光渲染纹理。 第 2 阶段：绘制渲染器 绘制完所有光照渲染纹理后，管道将继续绘制渲染器。 系统将跟踪哪组渲染器是由哪组光渲染纹理绘制的。 它们在前期阶段的批处理过程中关联。绘制渲染器时，它将可以访问所有（每种混合样式一个）可用的光渲染纹理。 在着色器中，通过使用指定操作将输入颜色与来自灯光的渲染纹理的颜色组合来计算最终颜色。 阶段二 具有四种活动混合样式的设置示例，说明了多种混合样式如何组合在一起。 在大多数情况下，您通常只需要两种混合样式即可获得所需的效果。 2D Pixel Perfect 2D Pixel Perfect套件包含 Pixel Perfect相机组件，可确保您的像素艺术在不同分辨率下保持清晰，并且在运动情况下也能保持清晰。它是一个单一的组件，可以进行Unity所需的所有计算，以随分辨率变化缩放视口，因此您无需手动进行。您可以使用此组件设置来调整相机视口中渲染像素艺术的定义，并且可以使用“在编辑模式下运行”功能在游戏视图中立即预览任何更改。 URP管线的具体的实现细节，打算后续对URP的源码进行阅读，待续... HDRP 现阶段手游开发不会用到此渲染管线。 原文 [1] Render pipelines https://docs.unity3d.com/Manual/render-pipelines.html [2] Universal Render Pipeline https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@14.0/manual/index.html [3] High Definition Render Pipeline https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@14.0/manual/index.html [4] unity shader 实现延迟渲染代码加注释 https://blog.csdn.net/lengyoumo/article/details/104489830","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"Rendering","slug":"Rendering","permalink":"http://yoursite.com/tags/Rendering/"}]},{"title":"UGUI基础","slug":"Unity/UI/UGUI基础","date":"2021-08-04T09:52:28.000Z","updated":"2025-04-26T11:06:24.132Z","comments":true,"path":"2021/08/04/Unity/UI/UGUI基础/","link":"","permalink":"http://yoursite.com/2021/08/04/Unity/UI/UGUI%E5%9F%BA%E7%A1%80/","excerpt":"Canvas的渲染 渲染基本层次 基本渲染层次是根据可视化对象在继承窗口中的显示顺序来渲染的，在此有两种方式可以调节对象的顺序。第一种：直接在继承窗口拖拽。第二种：通过Transform的SetAsFirstSibling, SetAsLastSibling, and SetSiblingIndex函数来进行调节。 渲染模式 屏幕覆盖模式 直接屏幕映射方式显示，永远在屏幕的最上面，跟相机无关即使没有相机也可以显示 屏幕相机模式 此模式和屏幕模式相似但是受到相机的限制，但是始终都显示在相机的前面，如果有3D物体在UI的前面也是会挡住UI。 世界模式 把Canvas当成普通的对象在世界坐标系中定位渲染。","text":"Canvas的渲染 渲染基本层次 基本渲染层次是根据可视化对象在继承窗口中的显示顺序来渲染的，在此有两种方式可以调节对象的顺序。第一种：直接在继承窗口拖拽。第二种：通过Transform的SetAsFirstSibling, SetAsLastSibling, and SetSiblingIndex函数来进行调节。 渲染模式 屏幕覆盖模式 直接屏幕映射方式显示，永远在屏幕的最上面，跟相机无关即使没有相机也可以显示 屏幕相机模式 此模式和屏幕模式相似但是受到相机的限制，但是始终都显示在相机的前面，如果有3D物体在UI的前面也是会挡住UI。 世界模式 把Canvas当成普通的对象在世界坐标系中定位渲染。 Canvas的屏幕适配策略 UGUI中的屏幕适配只需要在Canvas下挂一个CanvasScaler组件就行了。 适配模式分为3种： 固定像素 忽略屏幕的大小根据UI元素的实际像素显示 根据屏幕大小进行缩放（也是最常用的一项） 此项会根据设备真实分辨率与设计分辨率来对Canvas进行缩放。有三种模式： 1.Match Width or Height 根据宽或者高来对Canvas进行缩放，比如设备分辨率为1920x900，设计分辨率为1280x720，此时，如果采用宽进行匹配那么可以通过公式计算出此时应该缩放多少倍，公式如下： 缩放因子：1920/1280 = 1.5 缩放后的Canvas的宽为：1920（刚好能够完全显示） 缩放后的Canvas的高为：720x1.5 = 1080 由于设备的高为900所以会导致高度上的一部分不会被显示出来 2.Expand 适配的计算公式同上，只是在此模式下会保证设计分辨率下的东西能够全部显示出来，及选择设备分辨率和设计分辨率的宽、高比中选择最小值作为缩放因子。 3.Shrink 和Expand恰好相反，在此模式下不会留黑边但是会导致显示不完全。及选择设备分辨率和设计分辨率的宽、高比中选择最大值作为缩放因子。 固定物理大小 忽略屏幕大小和分辨率根据UI的实际物理大小来显示。 可视化对象 Text 同NGUI的Label.富文本格式参见：http://docs.unity3d.com/Manual/StyledText.html Image 同NGUI的Sprite RawImage 同NGUI的Texture Mask 此可视化组件是不可见的，只是用于定义一个子对象的显示区域不能超过父对象的显示区域，超出部分将被剪切掉。同NGUI Panel的剪切功能。在Unity5.3里有两个Mask组件 1.Mask 2.RectMask2D Effects 1.Shadow 2.Outline 3.PositionAsUI1(未知功能) 布局系统 基础布局 Rect Tool 选中此工具后，我们就可以在场景视图中编辑它的大小，位置，缩放，轴心点和锚点。 Toolbar buttons with Rect Tool selected Rect Transform 功能同Rect Tool只是此组件是需要自己手动输入参数。 Pivot(轴心点) 旋转，大小和缩放的修改都依赖于Pivo的位置。当ToolTar设置了Pivot时，Rect Transform的Pivot能够在场景中被移动。 Toolbar buttons set to Pivot and Local Anchors 有两种情况，在每种情况下Rect Transform显示是不同的如下： 1.四个锚点都在一起的情况 在此种状态可视化组件可以根据屏幕的任意位置来定进行定位，大小不会根据屏幕的大小的改变而调整。 2.四个锚点分开的情况 在此种状态下可视化组件的大小会根据屏幕的大小改变而调整可视化组件的大小。下图中Left,Right,Top和Bottom表示与这四个锚点构成的矩形的边的距离。 更详细的说明参见官方文档：http://docs.unity3d.com/Manual/UIBasicLayout.html 自动布局 自动布局系统依赖于基础布局系统。自动布局系统有两部分组成：布局元素和布局控制器。 布局元素 布局元素定义了如下尺寸： Minimum width Minimum height Preferred width Preferred height Flexible width Flexible height 可以通过ILayoutElement接口查看他们的定义： 123456789101112public interface ILayoutElement &#123; float flexibleHeight &#123; get; &#125; float flexibleWidth &#123; get; &#125; int layoutPriority &#123; get; &#125; float minHeight &#123; get; &#125; float minWidth &#123; get; &#125; float preferredHeight &#123; get; &#125; float preferredWidth &#123; get; &#125; void CalculateLayoutInputHorizontal(); void CalculateLayoutInputVertical(); &#125; 每个可视化对象都继承了ILayoutElement来控制自己的大小。当然也可以通过LayoutElement组件去重写这些属性。 布局控制器 布局控制器是根据布局元素定义的大小属性来确定自己或子对象的大小和位置的。 布局控制器有两种类型的控制器：控制自己和控制子对象 控制自己 ContentSizeFitter:内容填充 Aspect Ratio Fitter：比例控制 控制子对象 Horizontal Layout Group：让子对象水平排布 Vertical Layout Group：让子对象垂直排布 Grid Layout Group：让子对象按网格排布 局控计算顺序 1.计算自己的宽（子在父之前计算），通过调用ILayoutElement的CalculateLayoutInputHorizontal 2.控制器计算宽，通过调用ILayoutController的SetLayoutHorizontal 3.计算自己的高（子在父之前计算）， 通过调用ILayoutElement 的CalculateLayoutInputVertical 4.控制器计算高，通过调用ILayoutController的SetLayoutVertical 布局控重建触发时机 通过调用LayoutRebuilder.MarkLayoutForRebuild后在下一帧进行重建。 在设置了能够改变布局属性的时候 在一下回调被调用的时候 OnEnable OnDisable OnRectTransformDimensionsChange OnValidate （仅在编辑器有效） OnDidApplyAnimationProperties 交互式组件 交互式组件是用于处理鼠标，键盘，Touches和控制器的交互的组件，此组件是不可见的必须与一个或多个可视化组件一起使用。 交互式组件的公共功能能被放在了Selectable中，其中包括了交互组件的状态（normal, highlighted, pressed, disabled）切换，以及交换式组件的导航。 Button 按钮组件，通过OnClick来相应单击事件。 Toggle Toggle是一个复选框，OnValueChanged监听它的值改变。 Toggle Group Toggle Group是一个单选框 Slider 滑动器控件 Scrollbar 滚动条控件，通常联合Scroll Rect 和 Mask制作滚动视图。 Input Field 输入字段控件 Scroll Rect 同NGUI的ScrollView.需要配合Mask一起制作。 注意：一般在一个对象上不会挂两个交互式组件。 事件系统 事件系统是一中发送输入信息（比如像鼠标，键盘和Touches等的事件）到对象上的一种方法。 在事件系统中包括了两个主要模块： 输入模块 输入模块决定了事件系统的将有怎么样的行为。主要作用如下： 处理输入 管理事件状态 发送事件到场景对象 同一时刻只有一个输入模块作用于事件系统，输入模块必须和EventSystem组件在同一个对象上。 Raycasters（射线器） Raycasters主要被用于检查发送指针进入，离开和悬浮在对象。它通常用于输入模块去计算设备指针的进入，离开和悬浮等 Unity提供了3种类型的Raycasters： Graphic Raycaster 用于UI Physics 2D Raycaster 用于2D 物理元素 Physics Raycaster 用于3D物理元素 扩展自定义消息 从IEventSystemHandler借口派生一个接口出来就定义了一个消息 123456public interface ICustomMessageTarget : IEventSystemHandler&#123; // functions that can be called via the messaging system void Message1(); void Message2();&#125; 通过ExecuteEvents类可以发送一个消息，此类是消息的的帮助类，可以查看，验证一个对象上的消息接口。 1ExecuteEvents.Execute&lt;ICustomMessageTarget&gt;(target, null, (x,y)=&gt;x.Message1()); 支持的事件 IPointerEnterHandler - OnPointerEnter - Called when a pointer enters the object IPointerExitHandler - OnPointerExit - Called when a pointer exits the object IPointerDownHandler - OnPointerDown - Called when a pointer is pressed on the object IPointerUpHandler - OnPointerUp - Called when a pointer is released (called on the original the pressed object) IPointerClickHandler - OnPointerClick - Called when a pointer is pressed and released on the same object IInitializePotentialDragHandler - OnInitializePotentialDrag - Called when a drag target is found, can be used to initialise values IBeginDragHandler - OnBeginDrag - Called on the drag object when dragging is about to begin IDragHandler - OnDrag - Called on the drag object when a drag is happening IEndDragHandler - OnEndDrag - Called on the drag object when a drag finishes IDropHandler - OnDrop - Called on the object where a drag finishes IScrollHandler - OnScroll - Called when a mouse wheel scrolls IUpdateSelectedHandler - OnUpdateSelected - Called on the selected object each tick ISelectHandler - OnSelect - Called when the object becomes the selected object IDeselectHandler - OnDeselect - Called on the selected object becomes deselected IMoveHandler - OnMove - Called when a move event occurs (left, right, up, down, ect) ISubmitHandler - OnSubmit - Called when the submit button is pressed ICancelHandler - OnCancel - Called when the cancel button is pressed 参考文献 Unity官网-UIhttp://docs.unity3d.com/Manual/EventSystem.html Unity官网-EventSystemhttp://docs.unity3d.com/Manual/EventSystem.html 附录-组成结构图 UGUI组成结构图","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"UI","slug":"UI","permalink":"http://yoursite.com/tags/UI/"}]},{"title":"UGUI源码分析","slug":"Unity/UI/UGUI源码分析","date":"2021-08-04T09:52:28.000Z","updated":"2025-04-26T11:06:24.132Z","comments":true,"path":"2021/08/04/Unity/UI/UGUI源码分析/","link":"","permalink":"http://yoursite.com/2021/08/04/Unity/UI/UGUI%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"基础回顾 在正式分析UGUI源码前，让我们回顾一下，模型的组成，以及渲染。 模型组成 模型是由一系列的三角形组成，三角形又是由三个顶点构成的，每个顶点上保存了相关的顶点属性（顶点位置，顶点颜色，顶点UV，顶点法线，顶点切线）在Unity中使用Mesh类来表示。 模型渲染 模型筛选： 首先根据相机的Culling Mask剔除那些不需要此相机渲染的模型。 然后根据摄像机的视锥体大小，剔除完全在视锥体外的模型。 模型分类: Unity将根据需要渲染的模型Shader中RenderQueue对模型进行分类，分为不透明模型(小于2500)和透明模型(大于等于2500)。 排序渲染模型: 首先通过sortLayer和sortingOrder对不透明模型进行进行排序，越低的越先渲染；然后再通过Shader中的RenderQueue进行排序；再然后就通过包围盒中心点的深度（距离摄像机的位置）进行排序；最后如果深度相同则还可以通过ZBias进行深度调节。 透明物体同样的使用上述规则进行排序。","text":"基础回顾 在正式分析UGUI源码前，让我们回顾一下，模型的组成，以及渲染。 模型组成 模型是由一系列的三角形组成，三角形又是由三个顶点构成的，每个顶点上保存了相关的顶点属性（顶点位置，顶点颜色，顶点UV，顶点法线，顶点切线）在Unity中使用Mesh类来表示。 模型渲染 模型筛选： 首先根据相机的Culling Mask剔除那些不需要此相机渲染的模型。 然后根据摄像机的视锥体大小，剔除完全在视锥体外的模型。 模型分类: Unity将根据需要渲染的模型Shader中RenderQueue对模型进行分类，分为不透明模型(小于2500)和透明模型(大于等于2500)。 排序渲染模型: 首先通过sortLayer和sortingOrder对不透明模型进行进行排序，越低的越先渲染；然后再通过Shader中的RenderQueue进行排序；再然后就通过包围盒中心点的深度（距离摄像机的位置）进行排序；最后如果深度相同则还可以通过ZBias进行深度调节。 透明物体同样的使用上述规则进行排序。 根据排序的结果逐个渲染模型: 渲染一个模型主要分为以下2步： 设置渲染状态（提交模型数据，贴图，设置深度,模板和混合参数等 ） 调用绘制命令 当然这个只是它的渲染顺序，并不是说最后渲染的物体就一定会进入颜色缓冲，最后还的看深度和模板测试的结果。 显示组件（渲染） 设计结构 UGUI主要由显示组件，布局系统，事件系统，交互组件组成。这里主要介绍UGUI显示组件相关的几个核心类 UIBehaviour, 此类继承至MonoBehaviour，主要定义了UI组件的生命周期以及RectTransform的一些事件函数（比如：RectTransform的大小或层次结构的改变等） Canvas, 主要定义了渲染相关的参数，比如渲染模式，渲染顺序，使用的相机以及缩放因子等。此类也用来进行组织UI渲染和Mesh合并。Canvas的渲染， 在有相机(ScreenSpaceCamera和WorldSpace渲染模式)的情况下是在渲染透明物体阶段进行渲染的 在没有相机（ScreenSpaceOverlay渲染模式）的情况下是在所有渲染完成后在进行单独渲染的，所以能够保证Canvas在最上面。 ICanvasElement, 此接口是所有Canvas元素需要实现的接口，接口中最重要的函数就是Rebuild函数和其关联的transform字段，当一个Canvas元素需要重建时则会调用此函数。Canvas元素包含显示组件和交互组件。 CanvasRenderer, 定义了显示组件需要用到的Mesh，材质，纹理，是否剔除，裁剪区域以及深度信息等。Canvas则会根据这些信息进行排序、动态合并Mesh和最终的渲染。 CanvasUpdateRegistry, 此用用于管理需要更新的Canvas元素，更新主要分为两类，一类是：布局更新，另一类是：Graphic更新（包含UpdateGeometry和UpdateMaterial）Canvas元素的更新主要包含以下5个阶段： 123456789101112131415public enum CanvasUpdate&#123; //布局前 Prelayout = 0, //布局中 Layout = 1, //布局后 PostLayout = 2, //渲染前 PreRender = 3, //渲染后 LatePreRender = 4, //定义的最大值 MaxUpdateValue = 5&#125; CanvasUpdateRegistry类在其构造函数中监听了Canvas将要渲染的事件，当收到此事件后则会进行上述几个阶段的更新，核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// 此类是个单例protected CanvasUpdateRegistry()&#123; Canvas.willRenderCanvases += PerformUpdate;&#125;//核心的更新函数，所有需要更新的Canvas元素都不被添加到m_LayoutRebuildQueue或m_GraphicRebuildQueue列表中，如果没有Canvas元素Dirty则队列里面是空private void PerformUpdate()&#123; //在Profile里可以查看完成一次完整的更新需要多少时间 UISystemProfilerApi.BeginSample(UISystemProfilerApi.SampleType.Layout); CleanInvalidItems(); m_PerformingLayoutUpdate = true; //根据父节点数量进行排序 m_LayoutRebuildQueue.Sort(s_SortLayoutFunction); //开始更新布局 for (int i = 0; i &lt;= (int)CanvasUpdate.PostLayout; i++) &#123; for (int j = 0; j &lt; m_LayoutRebuildQueue.Count; j++) &#123; var rebuild = instance.m_LayoutRebuildQueue[j]; try &#123; if (ObjectValidForUpdate(rebuild)) rebuild.Rebuild((CanvasUpdate)i); &#125; catch (Exception e) &#123; Debug.LogException(e, rebuild.transform); &#125; &#125; &#125; //布局更新完成 for (int i = 0; i &lt; m_LayoutRebuildQueue.Count; ++i) m_LayoutRebuildQueue[i].LayoutComplete(); instance.m_LayoutRebuildQueue.Clear(); m_PerformingLayoutUpdate = false; //布局完后进行裁剪剔除 RectMask2D的方式，与传统的Mask使用模板(Stencil)缓冲不同，RectMask2D是直接在应用层级根据Mask的矩形区域直接进行裁剪 ClipperRegistry.instance.Cull(); //开始Graphic更新 m_PerformingGraphicUpdate = true; for (var i = (int)CanvasUpdate.PreRender; i &lt; (int)CanvasUpdate.MaxUpdateValue; i++) &#123; for (var k = 0; k &lt; instance.m_GraphicRebuildQueue.Count; k++) &#123; try &#123; var element = instance.m_GraphicRebuildQueue[k]; if (ObjectValidForUpdate(element)) element.Rebuild((CanvasUpdate)i); &#125; catch (Exception e) &#123; Debug.LogException(e, instance.m_GraphicRebuildQueue[k].transform); &#125; &#125; &#125; //Graphic更新完成 for (int i = 0; i &lt; m_GraphicRebuildQueue.Count; ++i) m_GraphicRebuildQueue[i].GraphicUpdateComplete(); instance.m_GraphicRebuildQueue.Clear(); m_PerformingGraphicUpdate = false; UISystemProfilerApi.EndSample(UISystemProfilerApi.SampleType.Layout);&#125;//Canvas元素的布局Dirty会调用此函数记录一下应该对Canvas元数进行布局更新public static void RegisterCanvasElementForLayoutRebuild(ICanvasElement element)&#123; instance.InternalRegisterCanvasElementForLayoutRebuild(element);&#125;//Canvas元素的Graphic Dirty会调用此函数记录一下应该对Canvas元数进行Graphic更新public static void RegisterCanvasElementForGraphicRebuild(ICanvasElement element)&#123; instance.InternalRegisterCanvasElementForGraphicRebuild(element);&#125; 总结：设计的主要结构就是Canvas元素负责更新CanvasRenderer中的Mesh,材质等，然后Canvas则管理CanvasRenderer中Mesh的合并，并根据其渲染模式进行渲染。接下来我们将继续探索Canvas元素的具体更新流程。 显示组件RawImage，Image和Text 显示组件关联的几个核心类： Graphic, 继承至UIBehaviour并实现了ICanvasElement是所有显示组件的基类，此类维护了显示组件的生命周期，管理了布局，顶点和材质改变，一旦顶点或材质改变则会将自己添加到CanvasUpdateRegistry的更新列表里，在下帧则会进行重建（调用Rebuild函数）。如果布局改变则会将自己添加到LayoutRebuilder的队列里，在下一帧进行构建。 Graphic的核心函数： Graphic.Rebuild, 此函数会负责具体的材质更新和Mesh的构建； 1234567891011121314151617181920212223public virtual void Rebuild(CanvasUpdate update)&#123; if (canvasRenderer == null || canvasRenderer.cull) return; switch (update) &#123; case CanvasUpdate.PreRender: if (m_VertsDirty) &#123; // 此函数会调用DoMeshGeneration进行Mesh的构建 UpdateGeometry(); m_VertsDirty = false; &#125; if (m_MaterialDirty) &#123; // 更新材质到CanvasRenderer UpdateMaterial(); m_MaterialDirty = false; &#125; break; &#125;&#125; Graphic.DoMeshGeneration, 此函数负责具体的Mesh构建（基础Mesh和附加Mesh（描边和阴影Mesh的生成））； 123456789101112131415161718192021private void DoMeshGeneration()&#123; // 矩形有大小的大小的时候，才去构建Mesh，否则就清空顶点 if (rectTransform != null &amp;&amp; rectTransform.rect.width &gt;= 0 &amp;&amp; rectTransform.rect.height &gt;= 0) OnPopulateMesh(s_VertexHelper); else s_VertexHelper.Clear(); // clear the vertex helper so invalid graphics dont draw. // 这里是获取修改顶点的组件 这类组件的基类是class BaseMeshEffect：UIBehaviour, IMeshModifier var components = ListPool&lt;Component&gt;.Get(); GetComponents(typeof(IMeshModifier), components); // 具体的顶点修改在BaseMeshEffect的子类的ModifyMesh函数里进行，具体有哪些Mesh效果的子类，在下一节进行介绍 for (var i = 0; i &lt; components.Count; i++) ((IMeshModifier)components[i]).ModifyMesh(s_VertexHelper); ListPool&lt;Component&gt;.Release(components); // 将顶点信息，填充到Mesh里， 并设置个CanvasRenderer对象 s_VertexHelper.FillMesh(workerMesh); canvasRenderer.SetMesh(workerMesh);&#125; Graphic.OnPopulateMesh, 此函数主要是用于派生类去覆写Mesh的基础构建； 123456789101112131415161718protected virtual void OnPopulateMesh(VertexHelper vh)&#123; // 将矩形调整为像素对其的 var r = GetPixelAdjustedRect(); var v = new Vector4(r.x, r.y, r.x + r.width, r.y + r.height); // 开始构建顶点 Color32 color32 = color; vh.Clear(); vh.AddVert(new Vector3(v.x, v.y), color32, new Vector2(0f, 0f)); vh.AddVert(new Vector3(v.x, v.w), color32, new Vector2(0f, 1f)); vh.AddVert(new Vector3(v.z, v.w), color32, new Vector2(1f, 1f)); vh.AddVert(new Vector3(v.z, v.y), color32, new Vector2(1f, 0f)); // 定义顶点构成的三角形，这里构建了2个三角形，刚好构成一个Quad vh.AddTriangle(0, 1, 2); vh.AddTriangle(2, 3, 0);&#125; Graphic.materialForRendering，此字段用于获取渲染时使用的材质，这主要做了一个提供修改材质的机制，当对象上有IMaterialModifier接口的组件对象时，则会使用它的材质进行渲染，主要用于Mask的材质修改，当然你也可以新建一个自定义的组件用于修改渲染时的材质。 12345678910111213141516public virtual Material materialForRendering&#123; get &#123; // 获取一个Component的列表， 并回去自己对象上实现了IMaterialModifier接口的组件 var components = ListPool&lt;Component&gt;.Get(); GetComponents(typeof(IMaterialModifier), components); // 调用IMaterialModifier接口的GetModifiedMaterial的函数，此函数返回修改后的材质 var currentMat = material; for (var i = 0; i &lt; components.Count; i++) currentMat = (components[i] as IMaterialModifier).GetModifiedMaterial(currentMat); ListPool&lt;Component&gt;.Release(components); return currentMat; &#125;&#125; Graphic.Raycast, 此函数由于检查是否能被击中 MaskableGraphic, 继承至Graphic新增了Mask的支持，Mask相关的内容，在下一节去学习。 VertexHelper, 此类是一个辅助产生Mesh的类，记录每个顶点以及顶点的相关属性（顶点位置，顶点颜色，顶点UV，顶点法线，顶点切线和三角形索引）。 RawImage, 直接使用一张纹理显示一张图片，一个RawImage就是一个Drawcall，此类一版用于背景。核心函数就是覆写的OnPopulateMesh支持了uv的修改。 Imge, 此类也是显示一张图片，但是显示的不是一张完整的纹理而是显示一个图集中的一个Sprite。支持Simple, Sliced, Tiled和Filled类型。在OnPopulateMesh函数中根据不同的类型进行顶点的生成。 Text, 此类是用来显示文字的。此类在OnPopulateMesh函数中，通过TextGenerator类进行顶点的生成，并将每4个顶点组成一个Quad。 类图： Mask实现原理 UGUI在实现Mask的时候有两种方式： 使用模板(Stencil)缓冲进行裁剪，缺点是在Mask下没有任何显示对象的情况下都需要2个Drawcall，一个Drawcall绘制蒙版，另一个Drawcall清除绘制的蒙版，并且还会打断合批。 直接在应用层级根据Mask的矩形区域进行裁剪剔除，这样保证在绘制前，那些超出Mask区域的顶点已经被裁剪，优点是不需要额外的2个Drawcall，缺点是需要CPU进行额外的裁剪操作，对象越多CPU执行裁剪的消耗就越多，并且也只能进行2D对象的裁剪。 所用我们在选用蒙版的时候也要综合考虑两种方式的优缺点，在子对象比较多，CPU成为了性能瓶颈的时候，可以考虑使用第1种方式，如果子对象比较少则可以使用第2种方式；还的综合考虑合批的影响。 模板(Stencil)缓冲实现Mask IMaskable, 实现这个接口的元素是可以被蒙版裁剪剔除的，现在只有MaskableGraphic实现了这个接口，也就是说，上一节讨论的所有显示组件（RawImage，Image和Text）都能被蒙版裁剪剔除。就一个接口RecalculateMasking。 Mask, 此类主要实现了IMaterialModifier接口中的GetModifiedMaterial函数，用于修改显示组件使用的材质，参见上一节的Graphic.materialForRendering。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// Mask类的GetModifiedMaterial函数public virtual Material GetModifiedMaterial(Material baseMaterial)&#123; // 检查是否启用了Mask组件 if (!MaskEnabled()) return baseMaterial; // 在父节点中获取最近的overringsort的Canvas如果没则获取最顶层的Canvas var rootSortCanvas = MaskUtilities.FindRootSortOverrideCanvas(transform); // 获取自己在继承结构中的深度，用于后续的Stencil计算 var stencilDepth = MaskUtilities.GetStencilDepth(transform, rootSortCanvas); // 蒙版不能嵌套超过8层（因为stencil buffer每个像素只有8个bit） if (stencilDepth &gt;= 8) &#123; Debug.LogWarning(\"Attempting to use a stencil mask with depth &gt; 8\", gameObject); return baseMaterial; &#125; // 定义蒙版的Bit值，用于后续的计算，每个深度的Mask占用一位bit int desiredStencilBit = 1 &lt;&lt; stencilDepth; // 如果是最外层的Mask，我们则直接替换stencil buffer中的值为1 if (desiredStencilBit == 1) &#123; // 根据基础材质，创建带stencil参数的材质，直接替换stencil的值为1，并且根据如果要显示模板对应的显示对象，那么就让让所有颜色通道通过，否则就不通过任何颜色通道 var maskMaterial = StencilMaterial.Add(baseMaterial, 1, StencilOp.Replace, CompareFunction.Always, m_ShowMaskGraphic ? ColorWriteMask.All : 0); StencilMaterial.Remove(m_MaskMaterial); m_MaskMaterial = maskMaterial; // 当Mask下的子对象渲染完后，Unity会为我们再次渲染这个Mask对象，作用是为了清除设置的stencil buffer中的对应值。最外层的Mask的子对象绘制完成后，直接就将stencil buffer的值清零了。 var unmaskMaterial = StencilMaterial.Add(baseMaterial, 1, StencilOp.Zero, CompareFunction.Always, 0); StencilMaterial.Remove(m_UnmaskMaterial); m_UnmaskMaterial = unmaskMaterial; //这表示在所有子对象渲染完后，需要使用此材质再次渲染此对象 graphic.canvasRenderer.popMaterialCount = 1; graphic.canvasRenderer.SetPopMaterial(m_UnmaskMaterial, 0); return m_MaskMaterial; &#125; // 构建写入内层Mask的材质，这里把stencil值写入后，在渲染它的子对象时，需要匹配stencil buffer的值，在MaskableGraphic.GetModifiedMaterial中根据m_StencilValue的进行模板测试，如果模板测试未通过则不绘制制定区域，如果通过则绘制 var maskMaterial2 = StencilMaterial.Add(baseMaterial, desiredStencilBit | (desiredStencilBit - 1), StencilOp.Replace, CompareFunction.Equal, m_ShowMaskGraphic ? ColorWriteMask.All : 0, desiredStencilBit - 1, desiredStencilBit | (desiredStencilBit - 1)); StencilMaterial.Remove(m_MaskMaterial); m_MaskMaterial = maskMaterial2; // 构建内层清理stencil buffer值的材质 graphic.canvasRenderer.hasPopInstruction = true; var unmaskMaterial2 = StencilMaterial.Add(baseMaterial, desiredStencilBit - 1, StencilOp.Replace, CompareFunction.Equal, 0, desiredStencilBit - 1, desiredStencilBit | (desiredStencilBit - 1)); StencilMaterial.Remove(m_UnmaskMaterial); m_UnmaskMaterial = unmaskMaterial2; graphic.canvasRenderer.popMaterialCount = 1; graphic.canvasRenderer.SetPopMaterial(m_UnmaskMaterial, 0); return m_MaskMaterial;&#125;// MaskableGraphic类的GetModifiedMaterial函数public virtual Material GetModifiedMaterial(Material baseMaterial)&#123; var toUse = baseMaterial; // 根据层次结构(Hierarchy)的深度获取m_StencilValue，如果不需要maskable这直接使用0 if (m_ShouldRecalculateStencil) &#123; if (maskable) &#123; var rootCanvas = MaskUtilities.FindRootSortOverrideCanvas(transform); m_StencilValue = MaskUtilities.GetStencilDepth(transform, rootCanvas); &#125; else m_StencilValue = 0; m_ShouldRecalculateStencil = false; &#125; // 如果需要进行模板计算则构建新的stencil材质， isMaskingGraphic表示要排除用于模板绘制的显示对象，如果stencil buffer值和参考值相等，则通过次像素，否则不通过， stencil buffer值是在Mask.GetModifiedMaterial函数中生成的材质设置的 if (m_StencilValue &gt; 0 &amp;&amp; !isMaskingGraphic) &#123; var maskMat = StencilMaterial.Add(toUse, (1 &lt;&lt; m_StencilValue) - 1, StencilOp.Keep, CompareFunction.Equal, ColorWriteMask.All, (1 &lt;&lt; m_StencilValue) - 1, 0); StencilMaterial.Remove(m_MaskMaterial); m_MaskMaterial = maskMat; toUse = m_MaskMaterial; &#125; // 如果不用使用stencil材质，则直接使用原材质进行渲染 return toUse;&#125; MaskUtilities, 两种类型的Mask都用的工具类。 StencilMaterial, 模板(Stencil)缓冲的材质生成类，主要的功能是基于基础材质添加上模板参数生成新的材质给Mask使用，也使用了引用计数的方式对相同的材质进行了缓存，避免重复创建相同的。核心代码如下： 12345678910111213141516171819202122232425public static Material Add(Material baseMat, int stencilID, StencilOp operation, CompareFunction compareFunction, ColorWriteMask colorWriteMask, int readMask, int writeMask)&#123; if ((stencilID &lt;= 0 &amp;&amp; colorWriteMask == ColorWriteMask.All) || baseMat == null) return baseMat; //省略部分代码 //...... 缓存引用的代码省略 // 设置shader参数 newEnt.customMat.SetInt(\"_Stencil\", stencilID); newEnt.customMat.SetInt(\"_StencilOp\", (int)operation); newEnt.customMat.SetInt(\"_StencilComp\", (int)compareFunction); newEnt.customMat.SetInt(\"_StencilReadMask\", readMask); newEnt.customMat.SetInt(\"_StencilWriteMask\", writeMask); newEnt.customMat.SetInt(\"_ColorMask\", (int)colorWriteMask); newEnt.customMat.SetInt(\"_UseUIAlphaClip\", newEnt.useAlphaClip ? 1 : 0); if (newEnt.useAlphaClip) newEnt.customMat.EnableKeyword(\"UNITY_UI_ALPHACLIP\"); else newEnt.customMat.DisableKeyword(\"UNITY_UI_ALPHACLIP\"); m_List.Add(newEnt); return newEnt.customMat;&#125; 应用程序裁剪Mask实现RectMask2D IClipper, 定义了裁剪器接口，现在只有RectMask2D实现了此接口，核心函数PerformClipping。 IClippable, 定义了能够被裁剪的接口，现在只有MaskableGraphic实现了此接口，核心函数RecalculateClipping，Cull和SetClipRect。 ClipperRegistry, 此类缓存了所有的IClipper对象,核心函数Cull，在布局完后调用，对所有的裁剪器调用PerformClipping进行裁剪。 RectangularVertexClipper, 唯一一个函数GetCanvasRect获取在Canvas下的Rect信息。 Clipping, 唯一一个函数FindCullAndClipWorldRect查找所有RectMask2D的公共Rect区域. RectMask2D, 此类实现了IClipper接口，核心函数就是IClipper接口中的PerformClipping，此函数执行计算了裁剪的矩形区域，并将计算的裁剪区域设置给了IClippable接口的对象，现在只有MaskableGraphic类，MaskableGraphic又将裁剪区域设置给了CanvasRenderer，每个显示组件的裁剪是Unity底层去进行裁剪的。核心代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public virtual void PerformClipping()&#123; // 检查有没有Canvas对象 if (ReferenceEquals(Canvas, null)) &#123; return; &#125; // 当结构改变时需要重新获取RectMask2D的对象列表（存在嵌套） if (m_ShouldRecalculateClipRects) &#123; MaskUtilities.GetRectMasksForClip(this, m_Clippers); m_ShouldRecalculateClipRects = false; &#125; //获取所有RectMask2D的公共区域 bool validRect = true; Rect clipRect = Clipping.FindCullAndClipWorldRect(m_Clippers, out validRect); // 检查自己的裁剪区域是否和公共的裁剪区域有交集，如果没有则剔除这个RectMask2D RenderMode renderMode = Canvas.rootCanvas.renderMode; bool maskIsCulled = (renderMode == RenderMode.ScreenSpaceCamera || renderMode == RenderMode.ScreenSpaceOverlay) &amp;&amp; !clipRect.Overlaps(rootCanvasRect, true); if (maskIsCulled) &#123; clipRect = Rect.zero; validRect = false; &#125; // 如果公共裁剪区域变化，则更新 if (clipRect != m_LastClipRectCanvasSpace) &#123; // 更新IClippable对象，保留对其他非MaskableGraphic对象的裁剪区域设置 foreach (IClippable clipTarget in m_ClipTargets) &#123; clipTarget.SetClipRect(clipRect, validRect); &#125; // 对MaskableGraphic对象的裁剪区域设置，并将完全不在这个clipRect内的MaskableGraphic标记为剔除 foreach (MaskableGraphic maskableTarget in m_MaskableTargets) &#123; maskableTarget.SetClipRect(clipRect, validRect); maskableTarget.Cull(clipRect, validRect); &#125; &#125; else if (m_ForceClip) &#123; // 同上， foreach (IClippable clipTarget in m_ClipTargets) &#123; clipTarget.SetClipRect(clipRect, validRect); &#125; foreach (MaskableGraphic maskableTarget in m_MaskableTargets) &#123; maskableTarget.SetClipRect(clipRect, validRect); if (maskableTarget.canvasRenderer.hasMoved) maskableTarget.Cull(clipRect, validRect); &#125; &#125; else &#123; foreach (MaskableGraphic maskableTarget in m_MaskableTargets) &#123; //Case 1170399 - hasMoved is not a valid check when animating on pivot of the object maskableTarget.Cull(clipRect, validRect); &#125; &#125; m_LastClipRectCanvasSpace = clipRect; m_ForceClip = false; UpdateClipSoftness();&#125; 总结：ClipperRegistry.Cull会在每帧布局完成后调用，此函数执行所有IClipper的裁剪计算，并将计算好的裁剪矩形区域通过MaskableGraphic.SetClipRect传递给CanvasRenderer对象，最后Unity内部根据这些裁剪矩形来进行顶点的裁剪。 Mesh效果 Mesh效果是在，基础Mesh构建完后，再重修或添加新的顶点构成最终的Mesh。UGUI自带的Mesh效果有Outline，PositionAsUV1和Shadow效果。 IMeshModifier, 定义了ModifyMesh函数 BaseMeshEffect, Mesh效果的基类，核心函数是接口IMeshModifier的ModifyMesh函数。 Outline, 在ModifyMesh添加描边的顶点。 PositionAsUV1, 将顶点的位置设置为该顶点的UV1（一般是法线贴图坐标），为图片或者文字添加法线贴图效果。 类图： 布局系统 布局基础-RectTransform RectTransform组件是Transform组件对应的2D表示。 其中Transform表示单个点，RectTransform表示可以放置UI元素的矩形。如果RectTransform的父项也是RectTransform，则子项RectTransform也可以指定它相对于父矩形的位置和大小。RectTransform用于存储和操作矩形的位置、大小和锚点，并支持基于父RectTransform的各种形式的缩放。 基础属性： 轴心点(Pivot)，轴心点定义了物体自身原点的位置以及作为旋转和缩放基点。 锚点(Anchors)，子对象锚定在父对象上的点，使用RectTransform的anchorMin和anchorMax两个属性表示，anchorMin和anchorMax两个属性的范围是[0,1],anchorMin表示锚定在父对象左下角的位置，anchorMax表示锚定在父对象右上角的位置，值[0,1]表示是锚定在父对象X轴和Y轴上的百分百位置。锚定在父对象上，意思是子对象的Pivot（轴心点）到（父对象上）Anchors（锚点）的相对位置不变。 位置，子对象在父对象下的位置，有两种：一种是：localPosition，是基于轴心点的一个本地位置；另一种：anchoredPosition和anchoredPosition3D, 是基于锚点的位置(就是轴心点在锚点空间下的坐标)，如下图： 偏移(offset)，指的是RectTransform的左下角或右上角到锚点的左下角或右上角的偏移值，即RectTransform的offsetMin和offsetMax字段。如下图： sizeDelta，表示的是RectTransform的区域与Anchors区域的差值，即offsetMax - offsetMin。在锚点重合的时候，offsetMax - offsetMin刚好是RectTransform的宽度和高度。 矩形区域(rect)， RectTransform对象的矩形区域通过rect字段表示，其中x,y是RectTransform左下角到轴心点（Pivot）的相对位置，with和height是RectTransform的宽度和高度。 SetSizeWithCurrentAnchors，此函数设置大小。 SetInsetAndSizeFromParentEdge，此函数设置边距和大小（会改变锚点（Anchors））。 RectTransformUtility，此类是RectTransform的工具类，提供了一些便利的方法（像：坐标转换、坐标获取、范围测试等）。 布局的基础知识了解完了，接下来看下UGUI自带的布局器。 布局器 布局器大致很两类，一类控制自己，另一类控制子对象。 控制自己 ILayoutElement， 定义布局元素的相关属性和方法（CalculateLayoutInputHorizontal， CalculateLayoutInputVertical，minWidth/Height, preferredWidth/Height, flexibleWidth/Height）。 LayoutElement， 覆盖对象的布局元素参数，使用这个组件定义的大小。 ILayoutSelfController，ILayoutElement负责计算布局，ILayoutController负责更新布局，ILayoutSelfController直接继承至ILayoutController，其中两个核心函数（SetLayoutHorizontal和SetLayoutVertical）。 LayoutUtility， 对获取布局元素中的min, preferred和flexible便捷的获取，并且获取优先级最高的。 AspectRatioFitter，调整自身的大小以适应指定的纵横比。 ContentSizeFitter，根据自身内容调整RectTransfrom的大小。 控制子对象 ILayoutGroup，ILayoutElement负责计算布局，ILayoutController负责更新布局，ILayoutGroup直接继承至ILayoutController，其中两个核心函数（SetLayoutHorizontal和SetLayoutVertical）。 LayoutGroup，所有控制子对象布局器的基类。主要完成的工作包括：定义对其参数以及相关的操作，获取有效的布局子对象和获取布局对象总的min, preferred和flexible的值。 12345678protected float GetStartOffset(int axis, float requiredSpaceWithoutPadding)&#123; float requiredSpace = requiredSpaceWithoutPadding + (axis == 0 ? padding.horizontal : padding.vertical); float availableSpace = rectTransform.rect.size[axis]; float surplusSpace = availableSpace - requiredSpace; float alignmentOnAxis = GetAlignmentOnAxis(axis); return (axis == 0 ? padding.left : padding.top) + surplusSpace * alignmentOnAxis;&#125; HorizontalOrVerticalLayoutGroup, 水平或垂直布局的基类，核心函数CalcAlongAxis计算自己的总的大小和SetChildrenAlongAxis设置子的位置和大小。 HorizontalLayoutGroup和VerticalLayoutGroup, 继承至HorizontalOrVerticalLayoutGroup这个类，分别在计算和设置函数中调用了HorizontalOrVerticalLayoutGroup类的两个核心函数。 GridLayoutGroup， 网格布局的。 LayoutRebuilder，此类是RectTransform对应的布局重建器，核心函数MarkLayoutForRebuild关联一个需要布局重建的RectTransfrom，Rebuild函数（ICanvasElement的接口函数）负责布局重建，Rebuild函数是在Canvas更新循环（CanvasUpdateRegistry）的布局阶段进行调用的（常见前面章节）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 关联LayoutRebuilderpublic static void MarkLayoutForRebuild(RectTransform rect)&#123; // 检查输入参数 if (rect == null || rect.gameObject == null) return; // 找到顶层的布局器，如果没有就把传入的对象标记为需要重建的，如果有顶层的组布局器，子布局器就不用重新计算了，因为父布局器计算算的时候，会递归计算所有子对象（在PerformLayoutCalculation，和PerformLayoutControl函数中会进行递归处理） var comps = ListPool&lt;Component&gt;.Get(); bool validLayoutGroup = true; RectTransform layoutRoot = rect; var parent = layoutRoot.parent as RectTransform; while (validLayoutGroup &amp;&amp; !(parent == null || parent.gameObject == null)) &#123; validLayoutGroup = false; parent.GetComponents(typeof(ILayoutGroup), comps); for (int i = 0; i &lt; comps.Count; ++i) &#123; var cur = comps[i]; if (cur != null &amp;&amp; cur is Behaviour &amp;&amp; ((Behaviour)cur).isActiveAndEnabled) &#123; validLayoutGroup = true; layoutRoot = parent; break; &#125; &#125; parent = parent.parent as RectTransform; &#125; // 检查时候有效，无效的则不用添加到布局更新里了 if (layoutRoot == rect &amp;&amp; !ValidController(layoutRoot, comps)) &#123; ListPool&lt;Component&gt;.Release(comps); return; &#125; // 构建一个LayoutRebuilder并且关联传入的RectTransform对象rect，并将其添加到CanvasUpdateRegistry的m_LayoutRebuildQueue列表里，在下一次更新的时候则会计算布局，如果需要立即更新则可以调用LayoutRebuilder.ForceRebuildLayoutImmediate函数。 MarkLayoutRootForRebuild(layoutRoot); ListPool&lt;Component&gt;.Release(comps);&#125;// 进行布局计算并布局public void Rebuild(CanvasUpdate executing)&#123; switch (executing) &#123; case CanvasUpdate.Layout: // 下面的每个函数都会递归调用遍历完所有的子布局元素（ILayoutElement），在遍历每个对象时都会通过调用回调函数将当前操作的对象回传并这里调用当前对象自己的计算和控制布局 PerformLayoutCalculation(m_ToRebuild, e =&gt; (e as ILayoutElement).CalculateLayoutInputHorizontal()); PerformLayoutControl(m_ToRebuild, e =&gt; (e as ILayoutController).SetLayoutHorizontal()); PerformLayoutCalculation(m_ToRebuild, e =&gt; (e as ILayoutElement).CalculateLayoutInputVertical()); PerformLayoutControl(m_ToRebuild, e =&gt; (e as ILayoutController).SetLayoutVertical()); break; &#125;&#125; 屏幕适配 CanvasScaler, 定义几种缩放模式，此类是主要是计算Canvas的缩放因子（Canvas.scaleFactor）,每种缩放模式计算缩放因子的方式不一样，这里主要看哈ScaleWithScreenSize模式的缩放因子计算 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public enum ScaleMode&#123; // 位置和大小是直接对应屏幕上的像素的 ConstantPixelSize, // 根据设计分辨率进行缩缩放 ScaleWithScreenSize, // 位置和大小都是通过物理单位（Centimeters， Millimeters， Inches， Points和Picas）标定的 ConstantPhysicalSize&#125; protected virtual void HandleScaleWithScreenSize()&#123; // 获取屏幕大小 Vector2 screenSize = new Vector2(Screen.width, Screen.height); // 处理多显示的情况 int displayIndex = m_Canvas.targetDisplay; if (displayIndex &gt; 0 &amp;&amp; displayIndex &lt; Display.displays.Length) &#123; Display disp = Display.displays[displayIndex]; screenSize = new Vector2(disp.renderingWidth, disp.renderingHeight); &#125; // 根据不同的屏幕匹配模式进行计算 float scaleFactor = 0; switch (m_ScreenMatchMode) &#123; case ScreenMatchMode.MatchWidthOrHeight: &#123; // 使用宽，高或宽和高的中间值来缩放Canvas // We take the log of the relative width and height before taking the average. // Then we transform it back in the original space. // the reason to transform in and out of logarithmic space is to have better behavior. // If one axis has twice resolution and the other has half, it should even out if widthOrHeight value is at 0.5. // In normal space the average would be (0.5 + 2) / 2 = 1.25 // In logarithmic space the average is (-1 + 1) / 2 = 0 float logWidth = Mathf.Log(screenSize.x / m_ReferenceResolution.x, kLogBase); float logHeight = Mathf.Log(screenSize.y / m_ReferenceResolution.y, kLogBase); float logWeightedAverage = Mathf.Lerp(logWidth, logHeight, m_MatchWidthOrHeight); scaleFactor = Mathf.Pow(kLogBase, logWeightedAverage); break; &#125; case ScreenMatchMode.Expand: &#123; // 保证缩放后Canvas的大小不小于设计分辨率（保证能够铺满全屏，但是会存在裁剪） scaleFactor = Mathf.Min(screenSize.x / m_ReferenceResolution.x, screenSize.y / m_ReferenceResolution.y); break; &#125; case ScreenMatchMode.Shrink: &#123; // 保证缩放后的Canvas的大小不大于设计分辨率（保证设计的类容全部显示，但是会有黑边） scaleFactor = Mathf.Max(screenSize.x / m_ReferenceResolution.x, screenSize.y / m_ReferenceResolution.y); break; &#125; &#125; SetScaleFactor(scaleFactor); SetReferencePixelsPerUnit(m_ReferencePixelsPerUnit);&#125; 事件系统 事件系统分为主要分为三个模块输入模块，射线器和事件系统。 输入模块 输入模块负责获取处理系统的各种输入（屏幕输入，鼠标等）。下文中的“指针”是指鼠标指针或Touch的触摸点。核心类： BaseInput, 输入的基类，主要转接了Unity的Input的部分功能，当然我们也可以从此类派生一个类，用于处理自定义的输入。 BaseInputModule, 输入模块的基类，其中包括了关联的EventSystem，BaseInput和事件数据对象。其中BaseInput用于获取设备的输入情况。核心函数 Process, 此函数是一个抽象函数，主要是用来处理输入模块的更新，这个函数在EventSystem.Update函数中调用的 HandlePointerExitAndEnter, 此函数负责处理指针的进入和离开。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// 派生类都调用此函数进行处理指针的进入和退出protected void HandlePointerExitAndEnter(PointerEventData currentPointerData, GameObject newEnterTarget)&#123; // 如果没有新的指针进入对象或当前没有指针进入对象，则清除所有被追踪的对象 if (newEnterTarget == null || currentPointerData.pointerEnter == null) &#123; // 执行对象上的OnPointerExit函数 for (var i = 0; i &lt; currentPointerData.hovered.Count; ++i) ExecuteEvents.Execute(currentPointerData.hovered[i], currentPointerData, ExecuteEvents.pointerExitHandler); // 清除以前追踪的对象 currentPointerData.hovered.Clear(); // 清除当前事件数据中的当前进入的对象，并返回 if (newEnterTarget == null) &#123; currentPointerData.pointerEnter = null; return; &#125; &#125; // 如果当前指针进入的对象和新目标对象像同，则说明没有切换目标当前事件数据的hovered没有变化，直接返回，否则就需要处理进入和退出 if (currentPointerData.pointerEnter == newEnterTarget &amp;&amp; newEnterTarget) return; // 查找事件数据的当前对象和新对象是否具有相同的父节点，目的是为了减少添加移除的数量，如果有公共父加节点时，只需要处理到公共父节点就可以了 GameObject commonRoot = FindCommonRoot(currentPointerData.pointerEnter, newEnterTarget); // 如果当前事件数据对象中已经有指针进入的对象了，现在指针进入了新对象，那么就需要将当前事件数据对象中的已经标记为指针进入的对象移除并调用它们OnPointerExit if (currentPointerData.pointerEnter != null) &#123; Transform t = currentPointerData.pointerEnter.transform; while (t != null) &#123; // 处理到公共父节点，如果没有则直接处理到最顶层的父节点 if (commonRoot != null &amp;&amp; commonRoot.transform == t) break; // 执行退出函数OnPointerExit ExecuteEvents.Execute(t.gameObject, currentPointerData, ExecuteEvents.pointerExitHandler); // 移除老的对象 currentPointerData.hovered.Remove(t.gameObject); t = t.parent; &#125; &#125; // 将新的对象加入到hovered中 currentPointerData.pointerEnter = newEnterTarget; if (newEnterTarget != null) &#123; Transform t = newEnterTarget.transform; // 也是处理到公共父就不处理了，因为公共父节点上面的节点已经在hovered里了（前面移除就没有移除公共父节点以上的节点） while (t != null &amp;&amp; t.gameObject != commonRoot) &#123; // 同样的执行进入OnPointerEnter ExecuteEvents.Execute(t.gameObject, currentPointerData, ExecuteEvents.pointerEnterHandler); currentPointerData.hovered.Add(t.gameObject); t = t.parent; &#125; &#125;&#125; ActivateModule，定义的输入模块被激活时调用的函数，这函数是在EventSystem中调用的 DeactivateModule,定义的输入模块被禁用时调用的函数，这函数是在EventSystem中调用的 PointerInputModule, 派生至BaseInputModule，用于处理指针（Touch和Mouse）的输入。核心函数 GetPointerData: 此函数是从m_PointerData列表中获取或创建一个PointerEventData对象。 1234567891011121314protected bool GetPointerData(int id, out PointerEventData data, bool create)&#123; // 从m_PointerData中获取指针数据 if (!m_PointerData.TryGetValue(id, out data) &amp;&amp; create) &#123; data = new PointerEventData(eventSystem) &#123; pointerId = id, &#125;; m_PointerData.Add(id, data); return true; &#125; return false;&#125; GetTouchPointerEventData, 获取Touch指针的事件数据，并返回是否按下或释放以及通过EventSystem.RaycastAll函数获取当前指针击中的第一个射线结果，并将结果保存在指针事件数据的pointerCurrentRaycast字段中。 1234567891011121314151617181920212223242526272829303132333435363738394041protected PointerEventData GetTouchPointerEventData(Touch input, out bool pressed, out bool released)&#123; // 获取当前TouchID对应的指针数据，如果没有则创建一个指正数据 PointerEventData pointerData; var created = GetPointerData(input.fingerId, out pointerData, true); pointerData.Reset(); // 检查当前Touch的数据状态 pressed = created || (input.phase == TouchPhase.Began); released = (input.phase == TouchPhase.Canceled) || (input.phase == TouchPhase.Ended); // 记录当前按下时的屏幕位置, 以及距离上次的指针位置偏移值 if (created) pointerData.position = input.position; if (pressed) pointerData.delta = Vector2.zero; else pointerData.delta = input.position - pointerData.position; pointerData.position = input.position; // 将button类型设置为Left, 主要是兼容鼠标的（Left，Middle和Right） pointerData.button = PointerEventData.InputButton.Left; // 如果不是取消则通过EventSystem的RaycastAll检查第一个击中的对象，并记录到当前指针事件数据的pointerCurrentRaycast中 if (input.phase == TouchPhase.Canceled) &#123; pointerData.pointerCurrentRaycast = new RaycastResult(); &#125; else &#123; eventSystem.RaycastAll(pointerData, m_RaycastResultCache); var raycast = FindFirstRaycast(m_RaycastResultCache); pointerData.pointerCurrentRaycast = raycast; m_RaycastResultCache.Clear(); &#125; return pointerData;&#125; StateForMouseButton, 获取指定鼠标指针的按下或释放状态。 12345678910111213protected PointerEventData.FramePressState StateForMouseButton(int buttonId)&#123; // 根据当前的鼠标输入获取对应鼠标按键的状态 var pressed = input.GetMouseButtonDown(buttonId); var released = input.GetMouseButtonUp(buttonId); if (pressed &amp;&amp; released) return PointerEventData.FramePressState.PressedAndReleased; if (pressed) return PointerEventData.FramePressState.Pressed; if (released) return PointerEventData.FramePressState.Released; return PointerEventData.FramePressState.NotChanged;&#125; GetMousePointerEventData, 获取鼠标指针的事件数据，同GetTouchPointerEventData功能一样只是这里的输入状态是获取鼠标的。 ProcessMove, 处理以移动，根据指针事件数据的pointerCurrentRaycast进行指针进入或离开处理 ProcessDrag, 处理拖拽. 1234567891011121314151617181920212223242526272829303132protected virtual void ProcessDrag(PointerEventData pointerEvent)&#123; // 检查是否需要处理Drag，指针没有移动或者没有拖拽对象的时 if (!pointerEvent.IsPointerMoving() || Cursor.lockState == CursorLockMode.Locked || pointerEvent.pointerDrag == null) return; // 检查是否应该开始拖拽，当拖动距离达到阈值时才应该开始拖拽 if (!pointerEvent.dragging &amp;&amp; ShouldStartDrag(pointerEvent.pressPosition, pointerEvent.position, eventSystem.pixelDragThreshold, pointerEvent.useDragThreshold)) &#123; ExecuteEvents.Execute(pointerEvent.pointerDrag, pointerEvent, ExecuteEvents.beginDragHandler); pointerEvent.dragging = true; &#125; // 如果已经进入拖拽状态了 if (pointerEvent.dragging) &#123; // 将查是否应该取消拖拽了，如果按下的对象和拖拽的对象不是同一个则应该取消拖拽 if (pointerEvent.pointerPress != pointerEvent.pointerDrag) &#123; ExecuteEvents.Execute(pointerEvent.pointerPress, pointerEvent, ExecuteEvents.pointerUpHandler); pointerEvent.eligibleForClick = false; pointerEvent.pointerPress = null; pointerEvent.rawPointerPress = null; &#125; // 调用OnDrag函数 ExecuteEvents.Execute(pointerEvent.pointerDrag, pointerEvent, ExecuteEvents.dragHandler); &#125;&#125; StandaloneInputModule, 继承至PointerInputModule模块，此类处理了鼠标，键盘，控制器和Touch的输入，以前TouchInputModule模块已经合并到此类中。核心函数是Process, ProcessTouchEvents和ProcessMouseEvent。 Process, 此函数是EventSystem.Update函数中调用过来的，用于每帧处理Touch和鼠标的输入信息。 12345678910111213141516171819202122232425public override void Process()&#123; // 当APP不是当前激活程序时，且又应该忽略事件在没有聚焦时候则返回，不用进行处理 if (!eventSystem.isFocused &amp;&amp; ShouldIgnoreEventsOnNoFocus()) return; // 像当前选中的对象发送OnUpdateSelected bool usedEvent = SendUpdateEventToSelectedObject(); // 有Touch输入设备则优先处理Touch的事件，如果没有Touch设备且有鼠标则处理鼠标的事件 if (!ProcessTouchEvents() &amp;&amp; input.mousePresent) ProcessMouseEvent(); // 是否应该发送导航事件（move, submit和cancel） if (eventSystem.sendNavigationEvents) &#123; // 事件没有被吞掉时发送OnMove if (!usedEvent) usedEvent |= SendMoveEventToSelectedObject(); // 事件没有被吞掉时发送OnSubmit事件 if (!usedEvent) SendSubmitEventToSelectedObject(); &#125;&#125; ProcessTouchEvents, 此函数处理Touch的事件 12345678910111213141516171819202122232425262728293031private bool ProcessTouchEvents()&#123; // 处理Touch输入 for (int i = 0; i &lt; input.touchCount; ++i) &#123; // 遍历的当前Touch Touch touch = input.GetTouch(i); // 如果是间接(远程)的Touch，则不处理 if (touch.type == TouchType.Indirect) continue; // 获取当前Touch的指针位置的事件数据，并返回是按下还是释放 bool released; bool pressed; var pointer = GetTouchPointerEventData(touch, out pressed, out released); // 处理Touch的按下或释放 ProcessTouchPress(pointer, pressed, released); // 如果没有释放，则就处理父类定义的处理移动和处理拖拽，释放了则移除当前TouchID对应的指针事件数据 if (!released) &#123; ProcessMove(pointer); ProcessDrag(pointer); &#125; else RemovePointerData(pointer); &#125; return input.touchCount &gt; 0;&#125; ProcessTouchPress, 处理Touch的按下或释放 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109protected void ProcessTouchPress(PointerEventData pointerEvent, bool pressed, bool released)&#123; // 获取当前指针事件击中对象 var currentOverGo = pointerEvent.pointerCurrentRaycast.gameObject; // 处理Touch按下时 if (pressed) &#123; // 重置指针事件数据 pointerEvent.eligibleForClick = true; pointerEvent.delta = Vector2.zero; pointerEvent.dragging = false; pointerEvent.useDragThreshold = true; pointerEvent.pressPosition = pointerEvent.position; pointerEvent.pointerPressRaycast = pointerEvent.pointerCurrentRaycast; // 检查如果当前选中的对象是否发生改变如果发生则更改EventSystem中的当前选中对象并发送 DeselectIfSelectionChanged(currentOverGo, pointerEvent); // 当前的OnEnter对象不是当前射线击中的对象时，所以击中对象改变了，则需要更新指针事件数据的hoverd列表 if (pointerEvent.pointerEnter != currentOverGo) &#123; HandlePointerExitAndEnter(pointerEvent, currentOverGo); pointerEvent.pointerEnter = currentOverGo; &#125; // 在当前的对象继承结构中查找并执行OnPointerDown函数 var newPressed = ExecuteEvents.ExecuteHierarchy(currentOverGo, pointerEvent, ExecuteEvents.pointerDownHandler); if (newPressed == null) newPressed = ExecuteEvents.GetEventHandler&lt;IPointerClickHandler&gt;(currentOverGo); // 处理双击 float time = Time.unscaledTime; // 如果当前按下的对象和一次按下的对象相同，则需要统计点击次数，如果两次点击的间隔在0.3秒以下则可以识别为双击 if (newPressed == pointerEvent.lastPress) &#123; var diffTime = time - pointerEvent.clickTime; if (diffTime &lt; 0.3f) ++pointerEvent.clickCount; else pointerEvent.clickCount = 1; pointerEvent.clickTime = time; &#125; else &#123; pointerEvent.clickCount = 1; &#125; // 记录当前按下的对象对象和按下时的时间 pointerEvent.pointerPress = newPressed; pointerEvent.rawPointerPress = currentOverGo; pointerEvent.clickTime = time; // 缓存拖拽的对象 pointerEvent.pointerDrag = ExecuteEvents.GetEventHandler&lt;IDragHandler&gt;(currentOverGo); // 如果有需要拖拽的对象，则调用OnInitializePotentialDrag函数 if (pointerEvent.pointerDrag != null) ExecuteEvents.Execute(pointerEvent.pointerDrag, pointerEvent, ExecuteEvents.initializePotentialDrag); // 缓存当前的指针事件对象 m_InputPointerEvent = pointerEvent; &#125; // 处理Touch释放时 if (released) &#123; // 执行OnPointerUp函数 ExecuteEvents.Execute(pointerEvent.pointerPress, pointerEvent, ExecuteEvents.pointerUpHandler); // 获取指针释放时有OnPointerClick的对象 var pointerUpHandler = ExecuteEvents.GetEventHandler&lt;IPointerClickHandler&gt;(currentOverGo); // 如果指针按下时的对象和当前有OnPointerClick的对象相同，则说明按下和释放在同一个对象，则需要处理点击事件。在启动拖拽的时候eligibleForClick被设置成了false，也就是拖拽和点击时不可以同时出现的 if (pointerEvent.pointerPress == pointerUpHandler &amp;&amp; pointerEvent.eligibleForClick) &#123; ExecuteEvents.Execute(pointerEvent.pointerPress, pointerEvent, ExecuteEvents.pointerClickHandler); &#125; // 如果没有点击的则检查是否有拖拽的对象，有并且在拖拽中释放的Touch则执行OnDrop else if (pointerEvent.pointerDrag != null &amp;&amp; pointerEvent.dragging) &#123; ExecuteEvents.ExecuteHierarchy(currentOverGo, pointerEvent, ExecuteEvents.dropHandler); &#125; // 释放后则重置按下的对象为空 pointerEvent.eligibleForClick = false; pointerEvent.pointerPress = null; pointerEvent.rawPointerPress = null; // 释放的时候再拖拽中，则还要执行OnEndDrag if (pointerEvent.pointerDrag != null &amp;&amp; pointerEvent.dragging) ExecuteEvents.Execute(pointerEvent.pointerDrag, pointerEvent, ExecuteEvents.endDragHandler); // 重置拖拽状态 pointerEvent.dragging = false; pointerEvent.pointerDrag = null; // 在已经指针进入的对象上执行OnPointerExit，请清除了当前指针事件数据中的pointerEnter ExecuteEvents.ExecuteHierarchy(pointerEvent.pointerEnter, pointerEvent, ExecuteEvents.pointerExitHandler); pointerEvent.pointerEnter = null; // 缓存当前指针的指针数据对象 m_InputPointerEvent = pointerEvent; &#125;&#125; ProcessMouseEvent，同ProcessTouchEvents函数一样，只是输入源是鼠标。 ProcessMousePress，同ProcessTouchPress函数一样，只是处理输入源不一样。 事件的处理流程如下： 首先EventSystem的Update处理了输入模块的切换，以及调用了当前输入模块的Process函数，然后在StandaloneInputModule输入模块中处理所有的输入，并在输入模块中通过射线器去更新当前指针击中的的对象和指针的位置，并处理了所有的事件。 射线器 射线器负责根据输入的位置发射射线或通过区域检查的方式检查对象是否被击中。 BaseRaycaster, 此类是射线发射器的基类，主要功能是定义了排序优先级，Raycast函数以及在OnEnable和OnDisable的时候将自己自己添加到RaycasterManager中或从RaycasterManager中移除。 eventCamera: 射线器使用的相机。 rootRaycaster: 缓存顶层父节点的射线器。 sortOrderPriority：排序优先级（sortingOrder） renderOrderPriority：渲染优先级(renderOrder) PhysicsRaycaster, 继承至BaseRaycaster，主要功能是负责3D射线的相关功能，其核心函数就是父类定义的抽象函数Raycast。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 通过物理射线进行击中检查，并将结果放入resultAppendList中public override void Raycast(PointerEventData eventData, List&lt;RaycastResult&gt; resultAppendList)&#123; // 获取当前屏幕点的射线 Ray ray = new Ray(); float distanceToClipPlane = 0; if (!ComputeRayAndDistance(eventData, ref ray, ref distanceToClipPlane)) return; int hitCount = 0; // 最大击中数量，0：表示发送射线时分配，即调用Physics.RaycastAll时，由此函数分配RaycastHit数组 if (m_MaxRayIntersections == 0) &#123; // 通过放射的方式获取了Physics.RaycastAll函数 if (ReflectionMethodsCache.Singleton.raycast3DAll == null) return; // 调用Physics.RaycastAll，并将结果缓存到m_Hits中 m_Hits = ReflectionMethodsCache.Singleton.raycast3DAll(ray, distanceToClipPlane, finalEventMask); hitCount = m_Hits.Length; &#125; // 使用优化的版本，事先分配好RaycastHit数组 else &#123; // 通过反射方式获取Physics.RaycastNonAlloc函数 if (ReflectionMethodsCache.Singleton.getRaycastNonAlloc == null) return; // 根据设置的大小分配RaycastHit数组 if (m_LastMaxRayIntersections != m_MaxRayIntersections) &#123; m_Hits = new RaycastHit[m_MaxRayIntersections]; m_LastMaxRayIntersections = m_MaxRayIntersections; &#125; // 调用Physics.RaycastNonAlloc函数获取击中的结果 hitCount = ReflectionMethodsCache.Singleton.getRaycastNonAlloc(ray, m_Hits, distanceToClipPlane, finalEventMask); &#125; // 根据到摄像机的距离进行排序 if (hitCount &gt; 1) System.Array.Sort(m_Hits, (r1, r2) =&gt; r1.distance.CompareTo(r2.distance)); // 生成射线结果对象RaycastResult，并添加到返回列表中 if (hitCount != 0) &#123; for (int b = 0, bmax = hitCount; b &lt; bmax; ++b) &#123; var result = new RaycastResult &#123; gameObject = m_Hits[b].collider.gameObject, module = this, distance = m_Hits[b].distance, worldPosition = m_Hits[b].point, worldNormal = m_Hits[b].normal, screenPosition = eventData.position, index = resultAppendList.Count, sortingLayer = 0, sortingOrder = 0 &#125;; resultAppendList.Add(result); &#125; &#125;&#125; Physics2DRaycaster, 继承至PhysicsRaycaster类，其核心函数也是Raycast函数，功能和基类的Raycast函数相同只是使用的是Physics2D。 RaycastResult, 此结构体是用于存储射线器击中结果的信息，具体信息包括： gameObject: 击中的对象 module:使用哪个射线器击中的 distance:距离相机的距离 index:在所用击中列表的索引值 worldPosition:击中位置的事件坐标 worldNormal:击中位置的法线 screenPosition:射线发出时的屏幕位置 GraphicRaycaster, 继承至PhysicsRaycaster类的图像射线器，通过矩形区域检查是否击中，必须挂在Canvas对象上，核心函数也是Raycast。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202public override void Raycast(PointerEventData eventData, List&lt;RaycastResult&gt; resultAppendList)&#123; // 没有Canvas或Canva下没有对象则不进行射线检查 if (canvas == null) return; var canvasGraphics = GraphicRegistry.GetGraphicsForCanvas(canvas); if (canvasGraphics == null || canvasGraphics.Count == 0) return; // 获取Canvas显示在哪个显示设备上（可能会存在多个显示设备） int displayIndex; var currentEventCamera = eventCamera; // 在屏幕覆盖模式或没有相机的时候，根据Canvas中的targetDisplay来确定显示设备索引，如果有相机则使用相机中指定的显示设备索引 if (canvas.renderMode == RenderMode.ScreenSpaceOverlay || currentEventCamera == null) displayIndex = canvas.targetDisplay; else displayIndex = currentEventCamera.targetDisplay; // 计算屏幕坐标 var eventPosition = Display.RelativeMouseAt(eventData.position); if (eventPosition != Vector3.zero) &#123; // 指针的位置不是当前显示屏幕则则不进行射线处理 int eventDisplayIndex = (int)eventPosition.z; if (eventDisplayIndex != displayIndex) return; &#125; else &#123; eventPosition = eventData.position; &#125; // 将屏幕坐标转化为视口坐标 Vector2 pos; if (currentEventCamera == null) &#123; // 多显示屏支持，更具上一步确定的显示设备索引，计算视口坐标 float w = Screen.width; float h = Screen.height; if (displayIndex &gt; 0 &amp;&amp; displayIndex &lt; Display.displays.Length) &#123; w = Display.displays[displayIndex].systemWidth; h = Display.displays[displayIndex].systemHeight; &#125; pos = new Vector2(eventPosition.x / w, eventPosition.y / h); &#125; else pos = currentEventCamera.ScreenToViewportPoint(eventPosition); // 检查是否在视口范围内 if (pos.x &lt; 0f || pos.x &gt; 1f || pos.y &lt; 0f || pos.y &gt; 1f) return; // 屏蔽2D或3D对象的最小距离, 使用的方式是只要选择屏蔽对象类型（2D，3D和ALL）,就通过射线去打，大于最近的这个击中距离的Graphic对象就会被排除，只个只对带相机的Canvas有效 float hitDistance = float.MaxValue; // 根据当前指针的屏幕位置生成射线 Ray ray = new Ray(); if (currentEventCamera != null) ray = currentEventCamera.ScreenPointToRay(eventPosition); // 通过射线去中击中检查，获取最近的击中距离 if (canvas.renderMode != RenderMode.ScreenSpaceOverlay &amp;&amp; blockingObjects != BlockingObjects.None) &#123; float distanceToClipPlane = 100.0f; if (currentEventCamera != null) &#123; float projectionDirection = ray.direction.z; distanceToClipPlane = Mathf.Approximately(0.0f, projectionDirection) ? Mathf.Infinity : Mathf.Abs((currentEventCamera.farClipPlane - currentEventCamera.nearClipPlane) / projectionDirection); &#125; if (blockingObjects == BlockingObjects.ThreeD || blockingObjects == BlockingObjects.All) &#123; if (ReflectionMethodsCache.Singleton.raycast3D != null) &#123; var hits = ReflectionMethodsCache.Singleton.raycast3DAll(ray, distanceToClipPlane, (int)m_BlockingMask); if (hits.Length &gt; 0) hitDistance = hits[0].distance; &#125; &#125; if (blockingObjects == BlockingObjects.TwoD || blockingObjects == BlockingObjects.All) &#123; if (ReflectionMethodsCache.Singleton.raycast2D != null) &#123; var hits = ReflectionMethodsCache.Singleton.getRayIntersectionAll(ray, distanceToClipPlane, (int)m_BlockingMask); if (hits.Length &gt; 0) hitDistance = hits[0].distance; &#125; &#125; &#125; // 开始进行图像射线击中检查，通过检查指针当前点是否在Graphic的矩形区域内 m_RaycastResults.Clear(); Raycast(canvas, currentEventCamera, eventPosition, canvasGraphics, m_RaycastResults); // 在图像射线击中的结果中，筛选满足条件的Graphic int totalCount = m_RaycastResults.Count; for (var index = 0; index &lt; totalCount; index++) &#123; var go = m_RaycastResults[index].gameObject; bool appendGraphic = true; // 如果忽略了背向的Graphic,则需要剔除那些背向相机的 if (ignoreReversedGraphics) &#123; if (currentEventCamera == null) &#123; var dir = go.transform.rotation * Vector3.forward; appendGraphic = Vector3.Dot(Vector3.forward, dir) &gt; 0; &#125; else &#123; var cameraFoward = currentEventCamera.transform.rotation * Vector3.forward; var dir = go.transform.rotation * Vector3.forward; appendGraphic = Vector3.Dot(cameraFoward, dir) &gt; 0; &#125; &#125; // 筛选后可以进行事件检查的Graphic if (appendGraphic) &#123; float distance = 0; Transform trans = go.transform; Vector3 transForward = trans.forward; if (currentEventCamera == null || canvas.renderMode == RenderMode.ScreenSpaceOverlay) distance = 0; else &#123; // http://geomalgorithms.com/a06-_intersect-2.html distance = (Vector3.Dot(transForward, trans.position - ray.origin) / Vector3.Dot(transForward, ray.direction)); // 距离小于零表示在摄像机后面，不可见 if (distance &lt; 0) continue; &#125; // 距离大于限定距离则丢弃 if (distance &gt;= hitDistance) continue; // 最后都通过了，则生成射线结果，并将其添加到结果列表里 var castResult = new RaycastResult &#123; gameObject = go, module = this, distance = distance, screenPosition = eventPosition, index = resultAppendList.Count, depth = m_RaycastResults[index].depth, sortingLayer = canvas.sortingLayerID, sortingOrder = canvas.sortingOrder, worldPosition = ray.origin + ray.direction * distance, worldNormal = -transForward &#125;; resultAppendList.Add(castResult); &#125; &#125;&#125;// 图像射线检查private static void Raycast(Canvas canvas, Camera eventCamera, Vector2 pointerPosition, IList&lt;Graphic&gt; foundGraphics, List&lt;Graphic&gt; results)&#123; // Canvas下的所有Graphic对象 int totalCount = foundGraphics.Count; for (int i = 0; i &lt; totalCount; ++i) &#123; Graphic graphic = foundGraphics[i]; // 剔除不需要被射线检的（Canvas已经处理的，不需要射线的，已经被剔除的） // -1 means it hasn't been processed by the canvas, which means it isn't actually drawn if (graphic.depth == -1 || !graphic.raycastTarget || graphic.canvasRenderer.cull) continue; // 做矩形区域检查，点不在这个区域的直接忽略 if (!RectTransformUtility.RectangleContainsScreenPoint(graphic.rectTransform, pointerPosition, eventCamera)) continue; // 有相机时，超出远平面的也剔除 if (eventCamera != null &amp;&amp; eventCamera.WorldToScreenPoint(graphic.rectTransform.position).z &gt; eventCamera.farClipPlane) continue; // 此函数会调用ICanvasRaycastFilter接口的IsRaycastLocationValid函数检查并过滤掉无效的对象，没有ICanvasRaycastFilter接口的对象则不需要进行过滤检查 if (graphic.Raycast(pointerPosition, eventCamera)) &#123; s_SortedGraphics.Add(graphic); &#125; &#125; // 根据CanvasRenderer.absoluteDepth的绝对深度进行排序。 s_SortedGraphics.Sort((g1, g2) =&gt; g2.depth.CompareTo(g1.depth)); // 将排序后的Graphic对象添加到结果列表中 totalCount = s_SortedGraphics.Count; for (int i = 0; i &lt; totalCount; ++i) results.Add(s_SortedGraphics[i]); s_SortedGraphics.Clear();&#125; RaycasterManager, 管理所有射线器，当射线器启动时会加到RaycasterManager中，核心函数就是AddRaycaster， RemoveRaycasters和GetRaycasters函数。 事件系统 事件系统负责处理输入，发射射线并发出事件。 IEventSystemHandler, 事件接口顶层类，派生了IPointerEnterHandler，IPointerExitHandler，IPointerDownHandler，IPointerUpHandler和IPointerClickHandler接口等。 EventTrigger, 此类实现了所有的事件接口，提供了一种在Unity对象上挂事件回调的方法。 ExecuteEvents, 此类是为了在对象上执行事件接口的一个便利类，核心函数Excute在指定的对象上执行指定的接口函数，ExecuteHierarchy函数从子一直往上执行指定的接口函数，GetEventHandler函数回去指定对象或它的上级对象上第一个能有指定事件的对象。 AbstractEventData, 事件数据的顶层基类，此类定义了一个事件是否使用的标记。 BaseEventData, 事件数据的基类，此类定义了关联的EventSystem，BaseInputModule和当前选择的对象。 AxisEventData, 输入控制器（手柄之类）或键盘关联的轴事件数据，主要包含了轴的移动向量和移动方向。 PointerEventData, 触摸或鼠标事件关联的数据类，此数据类是整个事件系统的核心数据类，包含类如下的数据： pointerId: 指针ID position: 当前指针位置 delta：最后一次指针移动的偏移量 pressPosition：指针按下时的位置 clickTime：最后一次点击事件，主要用于双击的检查 clickCount：点击次数 scrollDelta：滚轮的偏移量 useDragThreshold：是否使用拖拽阈值，如果不想使用拖拽阈值可以在IInitializePotentialDragHandler.OnInitializePotentialDrag函数里设置为false dragging：是否在拖拽中 IsPointerMoving(): 指针是否在移动中 IsScrolling(): 滚轮是否在滚动中 button: 当前指针使用的按钮（Left, Right, Middle） pointerEnter: 当前指针进入的对象（有OnPointerEnter的对象） pointerPress: 当前指针按下的对象 (有OnPointerDown的对象) lastPress: 上一次指针按下的对象（不一定有OnPointerDown的对象） rawPointerPress: 当前指针按下时的对象（不一定有OnPointerDown的对象） pointerDrag: 当前指针拖拽的对象（有OnDrag的对象） pointerCurrentRaycast: 当前事件关联的射线击中结果对象(RaycastResult) pointerPressRaycast: 与指针按下时关联的射线击中结果对象(RaycastResult) hovered: 保存指针移动时，射线击中的对象。指针进入一个对象或离开一个对象时都会向此字段中添加或移除对象 eligibleForClick: 在指针谈起时，是否有资格进行点击操作（拖拽的时候不执行点击操作） EventSystem, 是驱动整个系统更新的入口，EventSystem管理了所有的输入模块但只有当前输入模块有效，我们也可以在场景中添加多个EventSystem但也只有第一个有效。EventSystem记录当前选择的对象和首个选择的对象，可以通过SetSelectedGameObject进行当前选择对象的切换，并且会对应对象发送OnDeselect和OnSelect事件。 核心函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// 启动的时候OnEnable中，调用此函数缓存了所有的输入模块，并将没有被激活的输入模块移除掉。public void UpdateModules()&#123; GetComponents(m_SystemInputModules); for (int i = m_SystemInputModules.Count - 1; i &gt;= 0; i--) &#123; if (m_SystemInputModules[i] &amp;&amp; m_SystemInputModules[i].IsActive()) continue; m_SystemInputModules.RemoveAt(i); &#125;&#125;// 核心函数，进行事件系统主循环的更新protected virtual void Update()&#123; // 虽然可以有多个事件系统，但保证只有当前的事件系统能够被更新 if (current != this) return; // 此函数遍历所有的输入模块，并调用输入模块的UpdateModule函数，UpdateModule函数主要处理应用程序失去焦点时释放按下的指针，并更新输入模块中记录的当前指针位置 TickModules(); // 检查是否需要切换当前输入模块 bool changedModule = false; for (var i = 0; i &lt; m_SystemInputModules.Count; i++) &#123; var module = m_SystemInputModules[i]; if (module.IsModuleSupported() &amp;&amp; module.ShouldActivateModule()) &#123; if (m_CurrentInputModule != module) &#123; ChangeEventModule(module); changedModule = true; &#125; break; &#125; &#125; // 如果还是没有需要切换为当前的输入模块，则直接使用第一个作为输入模块 if (m_CurrentInputModule == null) &#123; for (var i = 0; i &lt; m_SystemInputModules.Count; i++) &#123; var module = m_SystemInputModules[i]; if (module.IsModuleSupported()) &#123; ChangeEventModule(module); changedModule = true; break; &#125; &#125; &#125; // 每帧都处理输入模块中的数据数据 if (!changedModule &amp;&amp; m_CurrentInputModule != null) m_CurrentInputModule.Process();&#125;// 根据当前指针的事件数据，发送射线，并返回射线的击中结果public void RaycastAll(PointerEventData eventData, List&lt;RaycastResult&gt; raycastResults)&#123; // 清空结果 raycastResults.Clear(); // 在RaycasterManager中获取所有获取所有激活的射线器，并调用射线器中的Raycast函数进行击中检查 var modules = RaycasterManager.GetRaycasters(); for (int i = 0; i &lt; modules.Count; ++i) &#123; var module = modules[i]; if (module == null || !module.IsActive()) continue; module.Raycast(eventData, raycastResults); &#125; // 将所射线器击中的对象进行排序，排序规则为： // 1. 如果是不同的射线器且都有相机则优先使用相机的深度进行排序，没有相机或相机深度相同时则根据Canvas的sortOrder, 期次根据Canvas的renderOrder // 2. 如果射线器相同，则优先根据Canvas的sortingLayer排序,再根据Canvas的sortingOrder排序，再根据Graphic的depth排序，再根据对象的距离排序，最后根据击中结果列表的索引来进行排序 raycastResults.Sort(s_RaycastComparer);&#125; 总结 本文通过对UGUI源码的阅读，了解了整个UI系统的处理流程，其中包括了显示组件，布局系统和事件系统三大主要模块，UGUI还有一些控件比如像ScrollRect, InputFields, Button和Slider等，这里就没再继续阅读它们的代码了，因为这些控件都是基于三大模块的组合并额外加入了每个控件的自身逻辑，比如像ScrollRect就在三大模块的基础上加了滚动的处理。","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"UI","slug":"UI","permalink":"http://yoursite.com/tags/UI/"}]},{"title":"骨骼动画","slug":"Unity/Animation/骨骼动画","date":"2021-03-22T13:20:49.000Z","updated":"2025-04-26T11:06:24.128Z","comments":true,"path":"2021/03/22/Unity/Animation/骨骼动画/","link":"","permalink":"http://yoursite.com/2021/03/22/Unity/Animation/%E9%AA%A8%E9%AA%BC%E5%8A%A8%E7%94%BB/","excerpt":"\b本文主要基于Blender创建一个简单的机器人，并基于此机器人来了解骨骼动画的制作流程。制作骨骼动画主要包含以下步骤： 建模 构建骨骼 蒙皮和权重绘制 动画制作","text":"\b本文主要基于Blender创建一个简单的机器人，并基于此机器人来了解骨骼动画的制作流程。制作骨骼动画主要包含以下步骤： 建模 构建骨骼 蒙皮和权重绘制 动画制作 建模 \b建模就是通过最基础的几何图元点，线，面构建出各种不同形状的模型。构建好的机器人，如下图： 构建骨骼 模型建好后就可以开始构建Skeleton(骨骼)了，在构建Skeleton之前先来了解一下Bone(骨头)和Joint(关节)，其实和人体结构一样骨架也是由骨头和关节组成的有关节的地方就可以旋转，由大臂带动小臂也就是说小臂是大臂的一个子骨头，当大臂旋转时小臂也会跟着旋转。 蒙皮与权重绘制 现在骨骼构建好了，但是骨骼的旋转并没有带动模型顶点的改变，接下来要做的就是对骨骼进行蒙皮，蒙皮就是将模型的顶点附着到骨头上，也就是会记录受这个骨头所影响的顶点在这个骨头空间下的坐标位置。像关节这些地方的一个顶点可能会受多个骨骼影响所以每个顶点上还要记录每个顶点在每个骨骼下的权重，如果自动计算的权重不能满足那就需要手动绘制权重。当蒙皮和权重绘制好了，我们就可以通过骨骼的运动(主要是旋转)来带动顶点的运行。 顶点的位置计算过程大致如下： 通过父子骨骼之间的变换矩阵，计算出最终在世界坐标下的变换矩阵。 再将骨骼空间下顶点的坐标点乘以第一步计算出来的变换矩阵就计算出了顶点的世界坐标。 每个顶点的最终位置还得根据影响这个顶点的骨骼进行混合。 创建动画 骨骼和顶点绑定后，就只需要创建动画驱动骨骼进行运动，骨骼的运动又会带动骨骼关联的顶点进行运动，这样便形成了完整的动画。动画文件只需要记录每个关键帧骨骼的位置，旋转和缩放就能通过插值算法计算出平滑的动画。 参考 Beginner Blender Tutorial: How to Model &amp; Animate a Robot","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"动画","slug":"动画","permalink":"http://yoursite.com/tags/%E5%8A%A8%E7%94%BB/"}]},{"title":"Unity序列帧动画","slug":"Unity/Animation/Unity序列帧动画","date":"2021-03-21T10:04:00.000Z","updated":"2025-04-26T11:06:24.128Z","comments":true,"path":"2021/03/21/Unity/Animation/Unity序列帧动画/","link":"","permalink":"http://yoursite.com/2021/03/21/Unity/Animation/Unity%E5%BA%8F%E5%88%97%E5%B8%A7%E5%8A%A8%E7%94%BB/","excerpt":"Unity实现序列帧动画主要有两种方式，一个是通过Unity自带的Animation进行制作，二是根据序列帧动画原理通过代码按固定帧率切换显示的图片即可实现序列帧动画效果。","text":"Unity实现序列帧动画主要有两种方式，一个是通过Unity自带的Animation进行制作，二是根据序列帧动画原理通过代码按固定帧率切换显示的图片即可实现序列帧动画效果。 资源导入 序列帧图片资源可以是单帧的图片也可以将所有的序列帧图片放到一张图片上 导入到Unity工程后通过SpriteEditor进行拆分 使用Animation实现 将拆分后的SpriteFrame选中后拖入到Hierarchy窗口，如下图： 代码实现 参见： GitHub 总结 在Unity里实现序列帧动画基本上就上述两种方式， 第一种方式 优点： 完全使用的是Unity的动画系统工作流程统一； 缺点： 只能使用SpriteRender进行渲染，如果要用UI的Image或RawImage进行渲染则动画需要手动在Animation里Key帧（也可以写工具自动Key帧） 只能使用Animator不能是用Animation组件（不知道是不是bug，本机系统mac os Unity版本2019.3.4f）即使将动画改为老版的模式也不行 第二种方式 优点： 更灵活可以使用所有渲染组件 缺点： 每一帧都需要手动指定，当然可以通过规定命名规则通过工具自动完成","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"动画","slug":"动画","permalink":"http://yoursite.com/tags/%E5%8A%A8%E7%94%BB/"}]},{"title":"Unity协程","slug":"Unity/Unity协程","date":"2021-03-17T04:13:25.000Z","updated":"2025-04-26T11:06:24.132Z","comments":true,"path":"2021/03/17/Unity/Unity协程/","link":"","permalink":"http://yoursite.com/2021/03/17/Unity/Unity%E5%8D%8F%E7%A8%8B/","excerpt":"为什么要使用协程 为什么要使用协程？，先来看段代码： 123456789101112131415161718192021222324252627using System.Collections;using UnityEngine;public class TestCoroutine : MonoBehaviour&#123; void Start() &#123; StartCoroutine(CoroutineFunc()); &#125; IEnumerator CoroutineFunc() &#123; var wfeof = new WaitForEndOfFrame() //1000次循环大概1s for(int i = 0; i &lt; 1000; i++) &#123; //假设做大量的工作,大概CPU耗时:0.1s DoManyWork(); yield return wfeof; &#125; &#125; void DoManyWork() &#123; Debug.Log(\"DoManyWork is called.\"); &#125;&#125; 上面的CoroutineFunc函数里面消耗了大量的CPU时间，在没有协程的开发环境中，比如像C/C++这样的开发语言，一般会创建一个线程来执行这种耗时的操作，多线程开发也会牵扯到各种数据和资源的互斥访问和线程同步的问题，这也导致在其他线程访问Unity引擎相关的组件会出现问题，因为我们没法控制其他的线程在访问操作一个对象时Unity引擎不去访问它，所以在Unity中其他线程一旦访问Unity引擎相关的组件就可能回出错。因此为让Unity的主线程不被这种耗时的操作给卡住，就只有将这种耗时的操作分到多帧里去执行，所以就出现了这种伪线程即协程。","text":"为什么要使用协程 为什么要使用协程？，先来看段代码： 123456789101112131415161718192021222324252627using System.Collections;using UnityEngine;public class TestCoroutine : MonoBehaviour&#123; void Start() &#123; StartCoroutine(CoroutineFunc()); &#125; IEnumerator CoroutineFunc() &#123; var wfeof = new WaitForEndOfFrame() //1000次循环大概1s for(int i = 0; i &lt; 1000; i++) &#123; //假设做大量的工作,大概CPU耗时:0.1s DoManyWork(); yield return wfeof; &#125; &#125; void DoManyWork() &#123; Debug.Log(\"DoManyWork is called.\"); &#125;&#125; 上面的CoroutineFunc函数里面消耗了大量的CPU时间，在没有协程的开发环境中，比如像C/C++这样的开发语言，一般会创建一个线程来执行这种耗时的操作，多线程开发也会牵扯到各种数据和资源的互斥访问和线程同步的问题，这也导致在其他线程访问Unity引擎相关的组件会出现问题，因为我们没法控制其他的线程在访问操作一个对象时Unity引擎不去访问它，所以在Unity中其他线程一旦访问Unity引擎相关的组件就可能回出错。因此为让Unity的主线程不被这种耗时的操作给卡住，就只有将这种耗时的操作分到多帧里去执行，所以就出现了这种伪线程即协程。 上面实例代码中for循环的控制变量i，在下次调用的时候是如何保证能正常迭代的？为什么函数签名中的返回值必须是IEnumerator，可以是其他的吗？协程函数又是谁在调？WaitForEndOfFrame又是什么，可以有其他的吗，可以自定义吗？yield return又是什么呢？ 协程实现原理 yield return生成器功能 yield return是NET 2.0框架允许C＃引入一个提供生成器功能的迭代器，主要是构建类似于python中的yield的功能。使用yield return,下面的这个函数将自动的保存迭代中的状态。代码如下： 12345678910111213141516171819// 这个方法将传入一个数组// 并且返回所有even的数字namespace DotnetExample&#123; class Program &#123; public static IEnumerable&lt;int&gt; GetEven(int[] numbers) &#123; for(int i=0; i &lt; numbers.Length; i++) &#123; if (i % 2 == 0) yield return numbers[i]; &#125; &#125; static void Main(string[] args) &#123; &#125; &#125;&#125; 这里也有一个yield break声明，此语句用于无条件返回到调用者。在每个生成器方法的最后都会有一个隐式的yield break。前面我们提到了几个陌生的名词：生成器功能，迭代器和生成器方法。先不管这些晦涩的术语，我们先来看哈上面这段代码对应的IL代码，删除了一些不影响我们理解功能的代码，删除后的结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130// 生成了一个名为：&lt;GetEven&gt;d__0的类，并继承了IEnumerable和IEnumerator接口.class nested private auto ansi sealed beforefieldinit &lt;GetEven&gt;d__0 extends [System.Runtime]System.Object implements [System.Runtime]System.Collections.Generic.IEnumerable`1&lt;System.Int32&gt;, [System.Runtime]System.Collections.IEnumerable, [System.Runtime]System.Collections.Generic.IEnumerator`1&lt;System.Int32&gt;, [System.Runtime]System.Collections.IEnumerator, [System.Runtime]System.IDisposable&#123; .field private int32 '&lt;&gt;1__state' //用于保存迭代状态(光标位置) .field private int32 '&lt;&gt;2__current' //记录当前的迭代值 .field private int32 '&lt;&gt;l__initialThreadId' //记录执行的线程ID .field private int32[] numbers //记录的传入的形参numbers .field public int32[] '&lt;&gt;3__numbers' //记录的传入的形参numbers，给外部访问的 .field private int32 '&lt;i&gt;5__1' //记录的循环控制变量i //每迭代一次调用一下，将光标下移一个 .method private hidebysig newslot virtual final instance default bool MoveNext() cil managed &#123; .maxstack 3 .locals init(int32 V_0, bool V_1, int32 V_2, bool V_3) IL_0000: ldarg.0 IL_0001: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;1__state' IL_0006: stloc.0 IL_0007: ldloc.0 IL_0008: brfalse.s IL_0012 IL_000a: br.s IL_000c IL_000c: ldloc.0 IL_000d: ldc.i4.1 IL_000e: beq.s IL_0014 IL_0010: br.s IL_0016 IL_0012: br.s IL_0018 IL_0014: br.s IL_004e IL_0016: ldc.i4.0 IL_0017: ret IL_0018: ldarg.0 IL_0019: ldc.i4.m1 IL_001a: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;1__state' IL_001f: nop IL_0020: ldarg.0 IL_0021: ldc.i4.0 IL_0022: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;i&gt;5__1' IL_0027: br.s IL_0066 IL_0029: nop IL_002a: ldarg.0 IL_002b: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;i&gt;5__1' IL_0030: ldc.i4.2 IL_0031: rem IL_0032: ldc.i4.0 IL_0033: ceq IL_0035: stloc.1 IL_0036: ldloc.1 IL_0037: brfalse.s IL_0055 IL_0039: ldarg.0 IL_003a: ldarg.0 IL_003b: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;i&gt;5__1' IL_0040: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;2__current' IL_0045: ldarg.0 IL_0046: ldc.i4.1 IL_0047: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;1__state' IL_004c: ldc.i4.1 IL_004d: ret IL_004e: ldarg.0 IL_004f: ldc.i4.m1 IL_0050: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;1__state' IL_0055: nop IL_0056: ldarg.0 IL_0057: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;i&gt;5__1' IL_005c: stloc.2 IL_005d: ldarg.0 IL_005e: ldloc.2 IL_005f: ldc.i4.1 IL_0060: add IL_0061: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;i&gt;5__1' IL_0066: ldarg.0 IL_0067: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;i&gt;5__1' IL_006c: ldarg.0 IL_006d: ldfld int32[] DotnetExample.Program/&lt;GetEven&gt;d__0::numbers IL_0072: ldlen IL_0073: conv.i4 IL_0074: clt IL_0076: stloc.3 IL_0077: ldloc.3 IL_0078: brtrue.s IL_0029 IL_007a: ldc.i4.0 IL_007b: ret &#125; // End of method System.Boolean DotnetExample.Program/&lt;GetEven&gt;d__0::MoveNext() //获取当前的迭代值，内部函数，外部调用Current(),Current函数调用的就是这个函数 .method private hidebysig newslot virtual specialname final instance default int32 System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.get_Current() cil managed &#123; .custom instance void class [System.Runtime]System.Diagnostics.DebuggerHiddenAttribute::.ctor() = ( 01 00 00 00 ) // .... // Method begins at Relative Virtual Address (RVA) 0x2114 // Code size 7 (0x7) .maxstack 8 IL_0000: ldarg.0 IL_0001: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;2__current' IL_0006: ret &#125; // End of method System.Int32 DotnetExample.Program/&lt;GetEven&gt;d__0::System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.get_Current() //实现IEnumerable的GetEnumerator()，用于获取Enumerator对象，并初始化了迭代状态&lt;&gt;1__state和设置numbers形参数据 .method private hidebysig newslot virtual final instance default [System.Runtime]System.Collections.Generic.IEnumerator`1&lt;System.Int32&gt; System.Collections.Generic.IEnumerable&lt;System.Int32&gt;.GetEnumerator() cil managed &#123; .maxstack 2 .locals init(class DotnetExample.Program/&lt;GetEven&gt;d__0 V_0) IL_0000: ldarg.0 IL_0001: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;1__state' IL_0006: ldc.i4.s -2 IL_0008: bne.un.s IL_0022 IL_000a: ldarg.0 IL_000b: ldfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;l__initialThreadId' IL_0010: call int32 class [System.Runtime]System.Environment::get_CurrentManagedThreadId() IL_0015: bne.un.s IL_0022 IL_0017: ldarg.0 IL_0018: ldc.i4.0 IL_0019: stfld int32 DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;1__state' IL_001e: ldarg.0 IL_001f: stloc.0 IL_0020: br.s IL_0029 IL_0022: ldc.i4.0 IL_0023: newobj instance void class DotnetExample.Program/&lt;GetEven&gt;d__0::.ctor(int32) IL_0028: stloc.0 IL_0029: ldloc.0 IL_002a: ldarg.0 IL_002b: ldfld int32[] DotnetExample.Program/&lt;GetEven&gt;d__0::'&lt;&gt;3__numbers' IL_0030: stfld int32[] DotnetExample.Program/&lt;GetEven&gt;d__0::numbers IL_0035: ldloc.0 IL_0036: ret &#125; // End of method System.Collections.Generic.IEnumerator`1&lt;System.Int32&gt; DotnetExample.Program/&lt;GetEven&gt;d__0::System.Collections.Generic.IEnumerable&lt;System.Int32&gt;.GetEnumerator() // 获取当前的迭代对象 .property instance int32 System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.Current () &#123; .get instance default int32 DotnetExample.Program/&lt;GetEven&gt;d__0::System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.get_Current () &#125; // End of property System.Int32 DotnetExample.Program/&lt;GetEven&gt;d__0::System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.Current()&#125; // End of class DotnetExample.Program/&lt;GetEven&gt;d__0 从IL代码我们可以看出，当我们使用yield return关键字时，C#编译器会为我们生成一个实现了IEnumerable和IEnumerator接口的类，这就是生成器，生成的类中又保存了迭代状态，所以也具有迭代功能，整个函数就是生成器方法。那么我们现在就可以可以使用foreach进行遍历了，代码如下： 123456789101112static void Main(string[] args)&#123; var iterator = GetEven(new int[4] &#123; 1, 2, 3, 4 &#125;); foreach (var item in iterator) &#123; Console.WriteLine($\"Item:&#123;item&#125;\"); &#125;&#125;//将输出Item:1Item:3 IEnumerator有什么呢？ IEnumerable和IEnumerator的区别 还是先来看一个实例，如下： 12345678public static IEnumerator&lt;int&gt; GetEven(int[] numbers)&#123; for (int i = 0; i &lt; numbers.Length; i++) &#123; if (i % 2 == 0) yield return numbers[i]; &#125;&#125; 这实例代码跟上个实例的代码除了将IEnumerable改变成了IEnumerator以外其他的没有任何差别，那么来看哈IL有什么差别，删除了函数体的IL代码如下： 123456789101112131415161718192021222324// 只实现了IEnumerator接口，所有没有GetEnumerator函数.class nested private auto ansi sealed beforefieldinit &lt;GetEven&gt;d__0 extends [System.Runtime]System.Object implements [System.Runtime]System.Collections.Generic.IEnumerator`1&lt;System.Int32&gt;, [System.Runtime]System.Collections.IEnumerator, [System.Runtime]System.IDisposable&#123; .field private int32 '&lt;&gt;1__state' //用于保存迭代状态(光标位置) .field private int32 '&lt;&gt;2__current' //记录当前的迭代值 .field public int32[] numbers //记录的传入的形参numbers .field private int32 '&lt;i&gt;5__1' //记录的循环控制变量i .method public hidebysig specialname rtspecialname instance default void .ctor(int32 &lt;&gt;1__state) cil managed &#123; &#125; .method private hidebysig newslot virtual final instance default bool MoveNext() cil managed &#123; &#125; .method private hidebysig newslot virtual specialname final instance default int32 System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.get_Current() cil managed &#123; &#125; .property instance int32 System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.Current () &#123; .get instance default int32 DotnetExample.Program/&lt;GetEven&gt;d__0::System.Collections.Generic.IEnumerator&lt;System.Int32&gt;.get_Current () &#125; &#125; 那么返回IEnumerator的迭代器，将如何遍历呢？尝试用foreach遍历但编译器提示： “ CS1579: foreach statement cannot operate on variables of type 'IEnumerator' because 'IEnumerator' does not contain a public instance or extension definition for 'GetEnumerator'” 所以编译限定了能使用foreach遍历的对象必须有GetEnumerator函数，因为IEnumerator没有实现IEnumerable接口的GetEnumerator方法，所有foreach是不能遍历。但是foreach内部也是通过IEnumerator提供的接口来进行遍历的，那么我们尝试手动调用IEnumerator提供的函数来进行遍历，代码如下： 123456789101112static void Main(string[] args)&#123; var iterator = GetEven(new int[4] &#123; 1, 2, 3, 4 &#125;); while (iterator.MoveNext()) &#123; Console.WriteLine($\"Item:&#123;iterator.Current&#125;\"); &#125;&#125;//将输出Item:1Item:3 IEnumerable和IEnumerator都会生成IEnumerator里面的接口函数, 也就是说IEnumerable能够实现IEnumerator接口定义的全部功能，而IEnumerable只有一个GetEnumerator函数，这个函数只是为了给foreach提供支持，foreach对应的IL代码也会调用GetEnumerator函数来获取一个初始化了状态的Enumerator对象，最终的迭代器也是由Enumerator接口提供的支持。 从上面的分析来看，在Unity里面定义协时，我们也可定义个返回IEnumerable的函数来实现协程的功能，代码如下： 12345IEnumerable CoroutineFunc()&#123; //TODO:功能 yield return&#125; 在启动协程的时候我们手动调动GetEnumerator()函数来获取一个Enumerator对象，代码如下： 1StartCoroutine(CoroutineFunc().GetEnumerator()); 为了验证我们的假设才这样调用的，项目中不要这样去调用，底层的IL\b代码会多一些。经过测试这样的调用方式也是没有问题。 谁在调用 前面我们手动调用了Enumerator接口的MoveNext来进行迭代，那在Unity的协程中又是谁在负责调用MoveNext呢？下面聊聊\b协程的生命周期。先来看哈Unity的脚本函数生命周期图，如下： 在上图中，我们可以看出所有类型的协程都是在对象的生命周期的不同阶段执行的，从图中可以看出，主要在3个阶段执行： 1. 在物理更新完成调用注册的yield WaitForFixedUpdate协程。 2. 在游戏逻辑更新中调用注册的yield null, yield WaitForSeconds,yield WWW, yield StartCoroutine和自定义的其他的协程。 3. 在一帧结束时调用注册的yield WaitForEndOfFrame 当Unity引擎在执行每个对象对应的生命周期阶段时，都会调用生成的协程类的MoveNext函数进行下一步，如此反复直到MoveNext返回false,此时这个协程的生命周期也就结束了。如果我们想提前结束一个协程的生命周期呢?我们可以调用StopCoroutine函数来注销正在执行的协程，注销后就会从协程队列里面移除，在下次生命周期到来时便不会被执行了。 像WWW,WaitForSecondsRealtime, WaitUntil,WaitWhile这些继承至CustomYieldInstruction的Enumerator对象都是在游戏逻辑更新中检查执行的，这些“自定义”类型的协程对象不会直接调用MoveNext而是先检查keepWaiting是否为false如果为false则在这次的生命周期中调用MoveNext函数。伪代码如下： 1234567891011Update()&#123; List&lt;Enumerator&gt; coroutineList = ... foreach(var cr in coroutineList) &#123; var cyi = cr as CustomYieldInstruction if(cyi &amp;&amp; !cyi.keepWaiting)&#123; cyi.MoveNext() &#125; &#125;&#125; 理解了Unity调用协程机制后，我们来自定义个每间隔指定帧数的yield对象。代码如下： 1234567891011121314151617181920212223242526public class IntervalFrame : CustomYieldInstruction&#123; private int _curFrameCount = 0; private int _frameCount = 0; public IntervalFrame(int count) &#123; _frameCount = count; &#125; public override bool keepWaiting &#123; get &#123; _curFrameCount++; if (_curFrameCount &lt; _frameCount) &#123; return true; &#125; else &#123; _curFrameCount = 0; return false; &#125; &#125; &#125; &#125; 在写自定义yield对象时，注意重置自己内部状态，这样在使用时就不用每次都new一个新的对象了。Unity内建的yield类型都是重置了状态的。上面这个实例在keepWaiting返回false的时候重置了_curFrameCount内部字段的。 参考 C# 2.0 IEnumerable VS IEnumerator in C# Order of execution for event functions","categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"}],"tags":[{"name":"协程","slug":"协程","permalink":"http://yoursite.com/tags/%E5%8D%8F%E7%A8%8B/"}]},{"title":"委托与事件","slug":"编程语言/CSharp/委托与事件","date":"2021-03-14T10:16:09.000Z","updated":"2025-04-26T11:06:24.134Z","comments":true,"path":"2021/03/14/编程语言/CSharp/委托与事件/","link":"","permalink":"http://yoursite.com/2021/03/14/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/CSharp/%E5%A7%94%E6%89%98%E4%B8%8E%E4%BA%8B%E4%BB%B6/","excerpt":"委托 什么是委托？委托与事件有什么关系呢？先来看一下委托的定义，代码如下： 12345678910111213namespace DotnetExample&#123; //定义一个委托 public delegate void VoidCallback(); // Console必须要一个Main函数才能编译通过 class Program &#123; static void Main(string[] args) &#123; &#125; &#125;&#125;","text":"委托 什么是委托？委托与事件有什么关系呢？先来看一下委托的定义，代码如下： 12345678910111213namespace DotnetExample&#123; //定义一个委托 public delegate void VoidCallback(); // Console必须要一个Main函数才能编译通过 class Program &#123; static void Main(string[] args) &#123; &#125; &#125;&#125; 这样的就定义了一个返回值和参数都为空的委托，这个委托究竟是什么呢，先别急，我来看哈，这个委托转换成CIL是什么样的，CIL代码如下： 123456789101112131415.class public auto ansi sealed DotnetExample.VoidCallback extends [System.Runtime]System.MulticastDelegate&#123; .method public hidebysig specialname rtspecialname instance default void .ctor([System.Runtime]System.Object 'object', native int 'method') runtime managed &#123; &#125; // End of method System.Void DotnetExample.VoidCallback::.ctor(System.Object,System.IntPtr) .method public hidebysig newslot virtual instance default void Invoke() runtime managed &#123; &#125; // End of method System.Void DotnetExample.VoidCallback::Invoke() .method public hidebysig newslot virtual instance default [System.Runtime]System.IAsyncResult BeginInvoke(class [System.Runtime]System.AsyncCallback callback, [System.Runtime]System.Object 'object') runtime managed &#123; &#125; // End of method System.IAsyncResult DotnetExample.VoidCallback::BeginInvoke(System.AsyncCallback,System.Object) .method public hidebysig newslot virtual instance default void EndInvoke(class [System.Runtime]System.IAsyncResult result) runtime managed &#123; &#125; // End of method System.Void DotnetExample.VoidCallback::EndInvoke(System.IAsyncResult)&#125; // End of class DotnetExample.VoidCallback 现在我们明白了原来委托就是一个继承至System.MulticastDelegate的类，delegate关键字就是一个语法糖而已。MulticastDelegate类又继承至Delegate。看看Delegate都有哪些内容，如下图： 其中重要的几个函数： 构造函数 .ctor(Type, String), .ctor(Object String)，当更一个委托赋值(=)时，会根据方法类型选择调用哪个构造函数，静态的函数调用第一种形式的构造函数，如果是成员函数的则会调用第二种形式的构造函数 Combine函数， 当使用+=的时候则会调动Combine将一个委托追加到调用链的后面。 CreateDelegate静态函数，更具各种类型创建委托 GetInvocationList函数，获取委托链上的说有委托 Remove函数，当使用-=的时候回调用Remove函数，它跟Combine函数是一对的，一个负责添加委托，一个负责移除委托。 上面定义了一个VoidCallback委托，下面我们来使用一下这个委托，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738namespace DotnetExample&#123; class Program &#123; static void Main(string[] args) &#123; new DelegateExample().Init(); &#125; &#125; public delegate void VoidCallback(); class DelegateExample &#123; public VoidCallback voidCallback; public void Init() &#123; voidCallback = TestCallback; voidCallback += TestStaticCallback; voidCallback.Invoke(); voidCallback -= TestCallback; voidCallback -= TestStaticCallback; &#125; void TestCallback() &#123; Console.WriteLine(\"TestCallback is called.\"); &#125; static void TestStaticCallback() &#123; Console.WriteLine(\"TestStaticCallback is called.\"); &#125; &#125;&#125; 上面这些函数都是Delegate类里面的功能函数，其中“=”运算换转换成构造函数，“+=”会转换成Combine函数，“-=”会被转换成Remove函数。那么Invoke函数是哪里来的呢？在上面我们定义了一个委托，编译后给我们生成了一个类，这个类里面包含了3个函数其中一个就是Invoke,还是两个是BeginInvoke和EndInvoke函数，这几个函数有什么区别呢？Invoke是同步函数在主线程执行而BeginInvoke和EndInvoke是用于异步调用的一对函数在其他线程运行。BeginInvoke和EndInvoke如何使用,可以参考https://www.cnblogs.com/canger/p/5938591.html。 事件 我们还是通过CIL来了解事件吧！先定义个事件看看转换后的CIL代码是什么样的，事件定义代码如下： 12345678910111213namespace DotnetExample&#123; public delegate void VoidCallback(); class Program &#123; public static event VoidCallback eventFields; static void Main(string[] args) &#123; &#125; &#125;&#125; 删除了无关代码后，IL代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566.class private auto ansi beforefieldinit DotnetExample.Program extends [System.Runtime]System.Object&#123; // eventFields这个字段被转换成里private的访问权限 .field private static class DotnetExample.VoidCallback eventFields // 生成了一个add_xxxx(字段名)的函数 .method public hidebysig specialname static default void add_eventFields(class DotnetExample.VoidCallback 'value') cil managed &#123; // Method begins at Relative Virtual Address (RVA) 0x2050 // Code size 39 (0x27) .maxstack 3 .locals init(class DotnetExample.VoidCallback V_0, class DotnetExample.VoidCallback V_1, class DotnetExample.VoidCallback V_2) IL_0000: ldsfld class DotnetExample.VoidCallback DotnetExample.Program::eventFields IL_0005: stloc.0 IL_0006: ldloc.0 IL_0007: stloc.1 IL_0008: ldloc.1 IL_0009: ldarg.0 IL_000a: call [System.Runtime]System.Delegate class [System.Runtime]System.Delegate::Combine([System.Runtime]System.Delegate, [System.Runtime]System.Delegate) IL_000f: castclass class DotnetExample.VoidCallback IL_0014: stloc.2 IL_0015: ldsflda class DotnetExample.VoidCallback DotnetExample.Program::eventFields IL_001a: ldloc.2 IL_001b: ldloc.1 IL_001c: call !!0 class [System.Threading]System.Threading.Interlocked::CompareExchange&lt;class DotnetExample.VoidCallback&gt;(!!0&amp;, !!0, !!0) IL_0021: stloc.0 IL_0022: ldloc.0 IL_0023: ldloc.1 IL_0024: bne.un.s IL_0006 IL_0026: ret &#125; // End of method System.Void DotnetExample.Program::add_eventFields(DotnetExample.VoidCallback) // 生成了一个remove_xxxx(字段名)的函数 .method public hidebysig specialname static default void remove_eventFields(class DotnetExample.VoidCallback 'value') cil managed &#123; // Method begins at Relative Virtual Address (RVA) 0x2084 // Code size 39 (0x27) .maxstack 3 .locals init(class DotnetExample.VoidCallback V_0, class DotnetExample.VoidCallback V_1, class DotnetExample.VoidCallback V_2) IL_0000: ldsfld class DotnetExample.VoidCallback DotnetExample.Program::eventFields IL_0005: stloc.0 IL_0006: ldloc.0 IL_0007: stloc.1 IL_0008: ldloc.1 IL_0009: ldarg.0 IL_000a: call [System.Runtime]System.Delegate class [System.Runtime]System.Delegate::Remove([System.Runtime]System.Delegate, [System.Runtime]System.Delegate) IL_000f: castclass class DotnetExample.VoidCallback IL_0014: stloc.2 IL_0015: ldsflda class DotnetExample.VoidCallback DotnetExample.Program::eventFields IL_001a: ldloc.2 IL_001b: ldloc.1 IL_001c: call !!0 class [System.Threading]System.Threading.Interlocked::CompareExchange&lt;class DotnetExample.VoidCallback&gt;(!!0&amp;, !!0, !!0) IL_0021: stloc.0 IL_0022: ldloc.0 IL_0023: ldloc.1 IL_0024: bne.un.s IL_0006 IL_0026: ret &#125; // End of method System.Void DotnetExample.Program::remove_eventFields(DotnetExample.VoidCallback) // eventFields事件定义了+= addon 和-= removeon两个函数 .event class DotnetExample.VoidCallback eventFields &#123; .addon default void DotnetExample.Program::add_eventFields (class DotnetExample.VoidCallback 'value') .removeon default void DotnetExample.Program::remove_eventFields (class DotnetExample.VoidCallback 'value') &#125; // End of property DotnetExample.VoidCallback DotnetExample.Program::eventFields&#125; // End of class DotnetExample.Program 根据IL代码发现，我们定义的public的事件字段eventFields被转换成了private的访问权限，并且eventFields也就是只是委托类型的对象，还生成了两个public的函数add_xxxx和remove_xxxx函数，add_xxxx函数里直接调用了委托的Combine函数，remove_xxxx函数了调用了委托的Remove函数，以及一个event类型，里面包含了两个函数分别对应+=和-=两个运算符。 委托与事件的区别 根据上两节的分析，总结委托与事件的区别。 相同点 事件本质也是委托 不同点 事件访问权限都是private的，要触发事件调用只能在类的内部。 在外部，事件只能通过+=或-=进行回调函数的添加或删除","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"C#","slug":"编程语言/C","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://yoursite.com/tags/C/"}]},{"title":"聊聊.Net开发平台","slug":"编程语言/CSharp/聊聊.Net开发平台","date":"2021-03-04T09:30:21.000Z","updated":"2025-04-26T11:06:24.134Z","comments":true,"path":"2021/03/04/编程语言/CSharp/聊聊.Net开发平台/","link":"","permalink":"http://yoursite.com/2021/03/04/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/CSharp/%E8%81%8A%E8%81%8A.Net%E5%BC%80%E5%8F%91%E5%B9%B3%E5%8F%B0/","excerpt":"介绍 .Net是什么呢？.Net是1998年微软剑桥研究院的技术人员研究的下一代开发技术，并将其制定的规范(CLI)提交到了ECMA，形成了ECMA335规范, 随后被ISO采纳为国际标准ISO/IEC 23271:2012。到2002微软正式发布了.NET Framework 1.0。.NET Framework是.Net开发技术规范的第一个实现，所以在初期.Net和.Net Framework指的是同一个东西，但它们本质上是完全不同的。由于当时只有微软的.Net Framework实现了.Net标准所以也仅限于Windows开发栈。.Net Framework平台的内容包含很多组件和库，像WinForm, WebForm, WPF，Asp.net等。直到2004年Mono让其可以在Linux上运行了，慢慢的Mono也开始支持Android, iOS，MacOSx让.Net实现跨平台。","text":"介绍 .Net是什么呢？.Net是1998年微软剑桥研究院的技术人员研究的下一代开发技术，并将其制定的规范(CLI)提交到了ECMA，形成了ECMA335规范, 随后被ISO采纳为国际标准ISO/IEC 23271:2012。到2002微软正式发布了.NET Framework 1.0。.NET Framework是.Net开发技术规范的第一个实现，所以在初期.Net和.Net Framework指的是同一个东西，但它们本质上是完全不同的。由于当时只有微软的.Net Framework实现了.Net标准所以也仅限于Windows开发栈。.Net Framework平台的内容包含很多组件和库，像WinForm, WebForm, WPF，Asp.net等。直到2004年Mono让其可以在Linux上运行了，慢慢的Mono也开始支持Android, iOS，MacOSx让.Net实现跨平台。微软为了让.Net能够在多个平台上运行，在2014公布了.Net Core计划，并于2016年发布了第一个.Net开源版本.Net Core1.0该版本主要实现了Asp.net, 后陆续加入了WinForm和WPF的支持（仅在Windows平台下），意在将.Net Framework的功能移植成开源跨平台的版本，当然.Net Core 也是.Net的未来。为了快速的整合.Net的生态，微软在2016年收购了Xamarin（Mono）, 并将Xamarin的开发工具集整合到了Vistual Studio中，至此微软统一了.Net生态。\b\b废话说完，下面进入正题。聊一聊.Net中的相关概念和术语，相信大家也经见过一些，比如：CLI、CIL、CTS、CLS、CLR、JIT、BCL、FCL、Module、Assembly 等，本文不会安字典顺序来一一讲解，因为这样大家很难理解也很难记住，本文将通过大家熟悉的HelloWorld程序(基于.Net Core 5.0)来进行解释，欢迎主角登场！ HelloWorld粉墨登场 本文实例基于 .Net core 5.0运行，我们先来创建一个控制台工程命名为HelloWorld，命令如下： 1dotnet new console -o ./HelloWorld 创建完成后将创建如下目录结构，如下图： dotnet初始项目结构 HelloWorld.csproj 项目文件 Program.cs 默认入口代码Main函数 obj 临时文件 至此HelloWorld程序就完成了，代码如下： 12345678910111213// Program.csusing System;namespace HelloWorld&#123; class Program &#123; static void Main(string[] args) &#123; Console.WriteLine(\"Hello World!\"); &#125; &#125;&#125; 这个代码虽然简单，就在控制台打印了一个“Hello World!”. 那这个Hello World有什么怎么被打印出来的呢？里面的Console.WriteLine有哪里来的呢？这个代码编译后会什么内容呢？会和C/C++编译出来的东西一样吗？这个代码又是如何运行起来的呢？这里面就要涉及到.Net规范的核心内容了，下面我们就通过这个Hello World程序来进行介绍。 在HelloWorld目录里，运行如下命令将HelloWorld程序编译出来，命令如下： 1dotnet build -c \"Release\" 编译后将生产一个HelloWorld.dll的文件, 此文件就是一个程序集(Assembly)。那么这个程序集的结构是什么样的呢？ Assembly Assembly包含了哪些内容呢？Assembly是一个自诉型程序集，主要有下面2类 Manifest 清单部分描述了Assembly自身的一些基础信息，包括版本信息，\b以及对模块和其他程序集的引用关系等；Assembly至少包含一个Module，Module又是代码(CIL)的集合。 元数据 元数据描述了程序集拥有哪些类型，类型的成员，以及成员的可见性等。 CIL代码，详细信息参见下一节 我们可以通过安装ILDASM工具来查看Assembly里的信息，命令如下： 安装 1dotnet tool install -g dotnet-ildasm 查看程序集 1dotnet ildasm HelloWorld.dll -o ./HelloWorld.il 我们先目睹一下HelloWorld Assembly里的Manifest信息： 12345678910111213141516171819202122232425262728293031323334.assembly extern System.Runtime&#123; .publickeytoken &#x3D; ( B0 3F 5F 7F 11 D5 0A 3A ) &#x2F;&#x2F; .._..... .ver 5:0:0:0&#125;.assembly extern System.Console&#123; .publickeytoken &#x3D; ( B0 3F 5F 7F 11 D5 0A 3A ) &#x2F;&#x2F; .._..... .ver 5:0:0:0&#125;.assembly &#39;HelloWorld&#39;&#123; .custom instance void class [System.Runtime]System.Runtime.CompilerServices.CompilationRelaxationsAttribute::.ctor(int32) &#x3D; ( 01 00 08 00 00 00 00 00 ) &#x2F;&#x2F; ........ .custom instance void class [System.Runtime]System.Runtime.CompilerServices.RuntimeCompatibilityAttribute::.ctor() &#x3D; ( 01 00 01 00 54 02 16 57 72 61 70 4E 6F 6E 45 78 63 65 70 74 69 6F 6E 54 68 72 6F 77 73 01 ) &#x2F;&#x2F; ....T..WrapNonExceptionThrows. .custom instance void class [System.Runtime]System.Runtime.Versioning.TargetFrameworkAttribute::.ctor(string) &#x3D; ( 01 00 18 2E 4E 45 54 43 6F 72 65 41 70 70 2C 56 65 72 73 69 6F 6E 3D 76 35 2E 30 01 00 54 0E 14 46 72 61 6D 65 77 6F 72 6B 44 69 73 70 6C 61 79 4E 61 6D 65 00 ) &#x2F;&#x2F; ....NETCoreApp.Version.v5.0..T..FrameworkDisplayName. .custom instance void class [System.Runtime]System.Reflection.AssemblyCompanyAttribute::.ctor(string) &#x3D; ( 01 00 0A 48 65 6C 6C 6F 57 6F 72 6C 64 00 00 ) &#x2F;&#x2F; ...HelloWorld.. .custom instance void class [System.Runtime]System.Reflection.AssemblyConfigurationAttribute::.ctor(string) &#x3D; ( 01 00 07 72 65 6C 65 61 73 65 00 00 ) &#x2F;&#x2F; ...release.. .custom instance void class [System.Runtime]System.Reflection.AssemblyFileVersionAttribute::.ctor(string) &#x3D; ( 01 00 07 31 2E 30 2E 30 2E 30 00 00 ) &#x2F;&#x2F; ...1.0.0.0.. .custom instance void class [System.Runtime]System.Reflection.AssemblyInformationalVersionAttribute::.ctor(string) &#x3D; ( 01 00 05 31 2E 30 2E 30 00 00 ) &#x2F;&#x2F; ...1.0.0.. .custom instance void class [System.Runtime]System.Reflection.AssemblyProductAttribute::.ctor(string) &#x3D; ( 01 00 0A 48 65 6C 6C 6F 57 6F 72 6C 64 00 00 ) &#x2F;&#x2F; ...HelloWorld.. .custom instance void class [System.Runtime]System.Reflection.AssemblyTitleAttribute::.ctor(string) &#x3D; ( 01 00 0A 48 65 6C 6C 6F 57 6F 72 6C 64 00 00 ) &#x2F;&#x2F; ...HelloWorld.. .hash algorithm 0x00008004 .ver 1:0:0:0&#125;.module &#39;HelloWorld.dll&#39;&#x2F;&#x2F; MVID: &#123;70604f0b-74ab-4028-87d6-2026571d0897&#125;.imagebase 0x00400000.file alignment 0x00000200.stackreserve 0x00100000.subsystem 0x0003 &#x2F;&#x2F; WindowsCui.corflags 0x00000001 &#x2F;&#x2F; ILOnly CIL——公共中间语言 C#编译器将C#代码编译成包含了CIL的程序集(Assembly),其他的编程语言比如VB,F#等,通过自己的编译器将源码编译成CIL，那些能够被编译成CIL的编程语言我们统称为面向.Net的语言，正因为有CIL中间语言的存在才使得.Net能够实现跨语言编程。 接下来我们看哈上面的C#的HelloWorld程序的CIL代码，如下： 12345678910111213141516171819202122.class private auto ansi beforefieldinit HelloWorld.Program extends [System.Runtime]System.Object&#123; .method private hidebysig static default void Main(string[] args) cil managed &#123; &#x2F;&#x2F; Method begins at Relative Virtual Address (RVA) 0x2050 .entrypoint &#x2F;&#x2F; Code size 11 (0xB) .maxstack 8 IL_0000: ldstr &quot;Hello World!&quot; IL_0005: call void class [System.Console]System.Console::WriteLine(string) IL_000a: ret &#125; &#x2F;&#x2F; End of method System.Void HelloWorld.Program::Main(System.String[]) .method public hidebysig specialname rtspecialname instance default void .ctor() cil managed &#123; &#x2F;&#x2F; Method begins at Relative Virtual Address (RVA) 0x205C &#x2F;&#x2F; Code size 7 (0x7) .maxstack 8 IL_0000: ldarg.0 IL_0001: call instance void class [System.Runtime]System.Object::.ctor() IL_0006: ret &#125; &#x2F;&#x2F; End of method System.Void HelloWorld.Program::.ctor()&#125; &#x2F;&#x2F; End of class HelloWorld.Program 下面我们来分析一下“公共中间语言”的“公共”，“中间”和“语言”都代表的什么 公共 公共指的是通用的意思，也就是说不论你用哪种语言编写的代码最终编译出的都是通过CIL来描述的，我们新建一个VB的HelloWorld工程，命令如下： 1dotnet new console -lang VB -o ./HelloWorldVB VB代码如下： 1234567Imports SystemModule Program Sub Main(args As String()) Console.WriteLine(\"Hello World!\") End SubEnd Module 编译后将HelloWorldVB.dll通过ildasm工具反编译成CIL,代码如下： 123456789101112131415.class private auto ansi sealed HelloWorldVB.Program extends [System.Runtime]System.Object&#123; .custom instance void class [Microsoft.VisualBasic.Core]Microsoft.VisualBasic.CompilerServices.StandardModuleAttribute::.ctor() &#x3D; ( 01 00 00 00 ) &#x2F;&#x2F; .... .method public static default void Main(string[] args) cil managed &#123; .custom instance void class [System.Runtime]System.STAThreadAttribute::.ctor() &#x3D; ( 01 00 00 00 ) &#x2F;&#x2F; .... &#x2F;&#x2F; Method begins at Relative Virtual Address (RVA) 0x2050 .entrypoint &#x2F;&#x2F; Code size 11 (0xB) .maxstack 8 IL_0000: ldstr &quot;Hello World!&quot; IL_0005: call void class [System.Console]System.Console::WriteLine(string) IL_000a: ret &#125; &#x2F;&#x2F; End of method System.Void HelloWorldVB.Program::Main(System.String[])&#125; &#x2F;&#x2F; End of class HelloWorldVB.Program 可以看出C#和VB编译出来的CIL代码基本上一样只是C#多生成了一个类的构造函数。因为VB是面向过程的编程所以不存在类和类的构造函数这一说。现在我们应该明白公共代表什么意思了吧！ 中间 为什么是“中间”呢？CIL不像C/C++静态语言会直接编译链接成CPU能够直接执行的机器码，而是需要一个公共语言运行时(CLR)来进行及时的编译（JIT）。所以将其称为中间语言。JIT编译器的根据执行编译阶段大致可以分为3类： Pre-JIT 编译器，在程序集部署时进行编译，就是通过一个Ngen.exe(本地代码生成器)将CIL转换为本地机器码。 Normal JIT 编译器，在程序集首次调用的时候将其编译成本地机器码并缓存。 Econo JIT 编译器，在方法执行前进行编译，执行完后删除，dotnet2.0后就使用的此类型的JIT 语言 CIL其实本身也是一门基于堆栈的编程语言，只是相比C#要稍微低级（并不是很low的意思）一些，我们也可以直接写CIL代码然后使用ILASM工具将其转换成程序集在CLR中运行。 我们在回头看看HelloWorld程序，里面调用了一个Console.WriteLine函数，那这个函数又是哪里来的呢？ BCL和FCL BCL-基础类库 我们在开发一个程序时，不可能任何基础功能都功能都从零开始，所以各个.Net的实现平台都为我们定义一个基础功能类库BCL, 比如像组数，链表，字典, Console,包括基本原类型byte，short, long这些都是基于System下的System.Byte,System.Int16和System.Int64,当然还包含了些系统相关的功能比如线程,安全性等。但是各个平台的对这些接口的实现也有所差异，所以为了实现各个平台的通用性，微软在自家产品中率先制定了一个可移植类库PCL，取各个平台的公用部分形成，结构如下图： 随着实现.Net平台的增多，为了更好跨平台性微软制定了基础库的标准.Net Standard，所有.Net实现平台上的BCL接口都必须遵守.Net Standard中制定的标准，这样便可以让在各个平台开发时调用的基础库的接口都是统一的。虽然BCL构成了我们编程的基础库，但是比如我们要开发一个Windows的应用，那么界面该如何搭建呢，界面上又有哪些控件呢？接下来就要介绍框架类库-FCL了。FCL和.Net Standard关系图如下： FCL-框架类库 BCL只是框架类库的一个子集而已，FCL包含的功能巨多，也是我们经常使用的类库，每个FCL的子库都够写一本书的了，我们根据功能可以将FCL大致分为一下3层。 最内一层，由BCL的大部分组成，主要作用是对.NET框架，.NET运行时及CIL语言本身进行支持，例如基元类型、集合类型、线程处理、应用程序域、运行时、安全性、互操作等。 中间一层，包含了对操作系统功能的封装，例如文件系统、网络连接、图形图像、XML操作等。 最外一层，包含各种类型的应用程序，例如Windows Forms、Asp.NET、WPF等。 CTS-公共类型系统 如果我们要开发一门像C#或VB一样的语言，在编译后也生成CIL语言，在.Net环境中运行，那么我们这个语言具有哪些特性就不是我们语言所能决定的了，而是有CIL所定义的规则决定的，而这些定义的规则就是CTS,CTS中定义了类，接口，结构体也定义了类里能包含属性，字段，函数，事件等，也定义了类只能继承一个父类，可以实现多个接口。C#和VB就是微软定义的符合CTS规则的语言。 CLS-公共语言规范 假设我们有3门面向.Net的语言，如果3门语言公开（Public）部分需要相互调用,那么他们必定需要遵循一定规则，这个规则就是CLS,CLS是CTS的一个子集，各个面向.Net的语言需要准守这个规范，否则就不就会存在互调的兼容性问题。CLS具体有哪些规则呢？是否区分大小写，标识符的命名规则如何，可以使用的基本类型有哪些，构造函数的调用方式（是否会调用基类构造函数），支持的访问修饰符等。 CLR-公共语言运行时 前面我们了解了.Net SDK编译出来的程序集（Assembly）的相关信息，下面我们将接着讨论，编程出来的程序集是怎么运行起来的呢？这就的归功于CLR，CLR其实就是一个能够执行CIL的虚拟机，其主要功能包括：管理应用程序域、加载和运行程序集、安全检查、JIT(将CIL代码即时编译为机器代码)、异常处理、对象析构和垃圾回收等。现在我们了解了CLR的功能，但是还是不知道它是怎么运行起来的。要说清楚这个问题我们必须先看看在系统中可执行文件的格式，在MacOs中是Mach-O格式，在Windows中是PE/COFF格式，在Linux是ELF格式，我们详细看下Mach-O文件吧，其他的格式都大同小异。Mach-O格式如下： Mach-O 的组成结构如图所示包括了： Header 包含该二进制文件的一般信息 字节顺序、架构类型、加载指令的数量等。 使得可以快速确认一些信息，比如当前文件用于 32 位还是 64 位，对应的处理器是什么、文件类型是什么 Load commands 一张包含很多内容的表 内容包括区域的位置、符号表、动态符号表等。 Data 通常是对象文件中最大的部分 包含 Segement 的具体数据 每个段的具体信息可以通过otool工具进行查看。\b 系统运行Mach-O文件大致步骤： 系统把Mach-O文件加载进入内存， 检查头看是否是相同CPU架构是否和符合当前的系统需求 链接动态库 找到入口程序(Main函数)的地址开始执行 我们发布一下HelloWorld程序，代码如下： 1dotnet publish -r osx-x64 -c Release --self-contained 我们通过自包含的方式发布，这样就不需要目标机器上安装.Net运行时了。我们来看一下发布后的文件有哪些,如下图： 重点是HelloWorld文件，这个文件是启动文件，这是一个Mach-O格式的文件，就想普通的MacOs系统的可以执行文件一样，他会链接像libclrjit.dylib,libcoreclr.dylib的CLR相关动态库文件，这样当HelloWorld通过系统调起的时候便会启动CLR, 并执行HelloWorld.dll中的主函数。我也可以直接使用dotnet HelloWorld.dll执行我们的程序集，因为dotnet这个Mach-o格式文件会去链接CLR相关的库。 总结 此篇文章主要讲了CLI规范中的相关术语，并通过一个HelloWorld的程序了解了一个CIL程序集时如何被编译生成的，也了解了一个程序集的内部结构，对.Net平台也有更深入的理解。","categories":[{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"C#","slug":"编程语言/C","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/C/"}],"tags":[{"name":"C#","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":".Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"}]},{"title":"安装CentOS 8","slug":"Linux/安装CentOS8","date":"2020-07-17T09:52:05.000Z","updated":"2025-04-26T11:06:24.128Z","comments":true,"path":"2020/07/17/Linux/安装CentOS8/","link":"","permalink":"http://yoursite.com/2020/07/17/Linux/%E5%AE%89%E8%A3%85CentOS8/","excerpt":"下载相关工具 下载CentOS8 系统镜像 官方下载地址：https://www.centos.org/download/，根据自己的情况选择下载什么镜像，阿里云和网易的镜像地址下载速度很快。本文下载的是CentOS-8.2.2004-x86_64最小化版，如果需要带GUI的系统可以下载完整版，各个版本的下载地址： 最小版本：http://mirrors.aliyun.com/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-minimal.iso 完整版：http://mirrors.aliyun.com/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-dvd1.iso","text":"下载相关工具 下载CentOS8 系统镜像 官方下载地址：https://www.centos.org/download/，根据自己的情况选择下载什么镜像，阿里云和网易的镜像地址下载速度很快。本文下载的是CentOS-8.2.2004-x86_64最小化版，如果需要带GUI的系统可以下载完整版，各个版本的下载地址： 最小版本：http://mirrors.aliyun.com/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-minimal.iso 完整版：http://mirrors.aliyun.com/centos/8.2.2004/isos/x86_64/CentOS-8.2.2004-x86_64-dvd1.iso 下载制作U盘启动盘工具 官方检测刻入U盘无问题工具：Win32diskimager。 注意：使用win32diskimager准备一个新的U盘，写入系统盘之后是无法再添加其它数据，当写入成功之后，再次插入发现U盘不能显示的，若需要恢复可以参考这里 制作U盘启动盘 当下载了CentOS8镜像和安装了Win32diskimager工具后，我们就可以开始制作启动盘了。以下是具体步骤： 启动Win32diskimager 选择\b刚下载的CentOS8的镜像文件，注意在打开的“文件选择对话框”的文件过滤下拉框中选择显示所有的文件，默认是显示.IMG的文件 选择写入的盘，注意不要选错了。 点击“写入”按钮，等待写入完成 安装系统 当系统盘写入完成后，就可以选择你的电脑进行安装了，具体步骤如下： 1. 进入BIOS设置，将你刚写入了系统的U盘设置为第一启动盘 等待启动选择安装（白色为选中）,如下图： 跟着后续的步骤进行安装即可 配置网络 参见另一篇文章","categories":[],"tags":[]},{"title":"游戏AI介绍","slug":"游戏/AI/游戏AI介绍","date":"2020-07-11T10:22:45.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2020/07/11/游戏/AI/游戏AI介绍/","link":"","permalink":"http://yoursite.com/2020/07/11/%E6%B8%B8%E6%88%8F/AI/%E6%B8%B8%E6%88%8FAI%E4%BB%8B%E7%BB%8D/","excerpt":"什么是游戏AI AI（artificial intelligence）即人工智能，游戏中的AI是让游戏里的对象具有一定的智能化，能够思考和决策，比如RPG游戏中的怪物知道有玩家攻击自己时进行反击，当玩家逃跑时会进行追击，当玩家远离自己时则放弃追击，这一系的决策和动作就是游戏中的AI要做的。和现下流行的人工智能不一样的是游戏里的AI还不能那么智能，不过现在已经在尝试将机器学习和游戏结合，让游戏对象更智能。\b 自治智能体 正如其命，有自治动作的智能体称之为“自治智能体”。在《看门狗》《GTA》里，街上走路的人群或者开动的汽车 就是一种自治智能体。主角开车如果冲向它们，这些自治智能体能够自行判断并做出躲开的动作。 群体智能体 如其名，与自治智能体相对，“群体智能”一般用于编队的AI或者集群的AI。例如足球游戏里，AI操控一方所有球员互相配合传球踢球。又或者射击游戏里，AI操控一支小队通过战术进攻据点。","text":"什么是游戏AI AI（artificial intelligence）即人工智能，游戏中的AI是让游戏里的对象具有一定的智能化，能够思考和决策，比如RPG游戏中的怪物知道有玩家攻击自己时进行反击，当玩家逃跑时会进行追击，当玩家远离自己时则放弃追击，这一系的决策和动作就是游戏中的AI要做的。和现下流行的人工智能不一样的是游戏里的AI还不能那么智能，不过现在已经在尝试将机器学习和游戏结合，让游戏对象更智能。\b 自治智能体 正如其命，有自治动作的智能体称之为“自治智能体”。在《看门狗》《GTA》里，街上走路的人群或者开动的汽车 就是一种自治智能体。主角开车如果冲向它们，这些自治智能体能够自行判断并做出躲开的动作。 群体智能体 如其名，与自治智能体相对，“群体智能”一般用于编队的AI或者集群的AI。例如足球游戏里，AI操控一方所有球员互相配合传球踢球。又或者射击游戏里，AI操控一支小队通过战术进攻据点。 游戏AI相关的知识点 游戏中的对象想要智能必须要具备以下3点： 1. 对当前环境数据的搜集即感知， 比如当前自己所在的位置，血量以及前后左右有什么等； 2. 对当前环境的认识进行决策，比如检查自己的血量，比较少则逃跑，多则可以进行攻击； 3. 对决策执行相应的行为，比如使用技能，逃跑等； 感知相关知识点 \b为了让游戏里面的对象具有向人一样的智能，那么人所具有的感知系统：听觉、视觉和触觉都要在游戏进行模拟，才能让游戏里的智能对象具有类似人一样的的感知，当然在游戏里不能完全模拟人的的听觉、视觉和触觉，有的时候游戏里为了提高关卡难度也能让智能体感知到在真实世界里的人不能获取的感知，比如：能感知到自己背后的怪物，预知敌方的下一步行动，或者获得其他系统的相关信息。在这里就要引入一个叫做黑板概念，“黑板”简单来说就是可访问的共享数据，用于多模块间的数据共享。 听觉 视觉 触觉 黑板 决策相关知识点 行为相关知识点 参考文献 [1] 游戏AI研究（一）：感知AI [2] 游戏AI之初步介绍","categories":[],"tags":[]},{"title":"向量","slug":"数学物理/向量","date":"2020-04-01T03:02:05.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2020/04/01/数学物理/向量/","link":"","permalink":"http://yoursite.com/2020/04/01/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86/%E5%90%91%E9%87%8F/","excerpt":"向量的基本定义 数学定义 向量就是一个数列，是一个只有大小和方向的一个数学量，和标量不同，标量是一个只有大小的量。向量的记法如下： 行向量:a=\\(\\begin{bmatrix}1&amp;2&amp;3\\end{bmatrix}\\) 列向量:a=\\(\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\) 几何定义 向量是有大小和方向的有向线段。向量和点不同点表示了一个固定的位置然而向量是没有位置的，你画在任何地方都行，因为向量表示它在各个方向上的位移关系。向量的几何表示如下图：","text":"向量的基本定义 数学定义 向量就是一个数列，是一个只有大小和方向的一个数学量，和标量不同，标量是一个只有大小的量。向量的记法如下： 行向量:a=\\(\\begin{bmatrix}1&amp;2&amp;3\\end{bmatrix}\\) 列向量:a=\\(\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\) 几何定义 向量是有大小和方向的有向线段。向量和点不同点表示了一个固定的位置然而向量是没有位置的，你画在任何地方都行，因为向量表示它在各个方向上的位移关系。向量的几何表示如下图： 向量的运算 负向量 负向量就是在一个向量的前面加上一个负号。例如：a=\\(\\begin{bmatrix}1&amp;2&amp;3\\end{bmatrix}\\),它的负向量就是\\(\\begin{bmatrix}-1&amp;-2&amp;-3\\end{bmatrix}\\) 运算法则 -\\(\\begin{bmatrix}a_1&amp;a_2&amp;\\cdots&amp;a_n\\end{bmatrix}\\)=\\(\\begin{bmatrix}-a_1&amp;-a_2&amp;\\cdots&amp;-a_n\\end{bmatrix}\\) 几何解释 向量变负，将得到一个和原向量大小相等，方向相反的向量。如下图： 标量与向量的乘法 向量与标量相乘，结果将得到一个与原向量平行，但长度不同或方向相反的向量。 运算法则 将向量中的每个元素和标量相乘。公式如下： \\[k\\begin{bmatrix}a_1\\\\ a_2\\\\ \\vdots\\\\a_n\\end{bmatrix}=\\begin{bmatrix}ka_1\\\\ ka_2\\\\ \\vdots\\\\ka_n\\end{bmatrix}\\] 几何解释 按照k的缩放因子进行缩放，如下： 向量的加法和减法 向量相加和相减的前提是两向量维度相同。结果向量的维度和原向量相同。 运算法则 将对应的向量元素相加。公式如下： \\[\\begin{bmatrix}a_1\\\\ a_2\\\\ \\vdots\\\\a_n\\end{bmatrix}+\\begin{bmatrix}b_1\\\\ b_2\\\\ \\vdots\\\\b_n\\end{bmatrix}=\\begin{bmatrix}a_1+b_1\\\\ a_2 + b_2\\\\ \\vdots\\\\a_n+b_n\\end{bmatrix}\\] 减法的操作可以转换为加法：a-b=a+(-b) 几何解释 向量点乘(内积) 运算法则 向量点乘就是对应分量乘积的和，其结果是一个标量： \\[\\begin{bmatrix}a_1\\\\ a_2\\\\ \\vdots\\\\a_n\\end{bmatrix}\\cdot\\begin{bmatrix}b_1\\\\ b_2\\\\ \\vdots\\\\b_n\\end{bmatrix}=a_1b_1+a_2b_2+\\cdots+a_nb_n\\] 用连加符号简写为： \\(a\\cdot b=\\sum_{i=1}^{n}a_ib_i\\) 几何解释 一般来说，点乘结果描述了两个向量的“相似”程度，点乘结果越大，两向量越相近。如下图： 点乘等于向量大小与向量夹角的\\(cos\\)值的积： \\(a\\cdot b=||a|| ||b||cos{\\theta}\\) 向量叉乘(叉积) 运算法则 向量叉乘得到一个向量且不满足交换律，点乘满足交换律。公式如下： \\[\\begin{bmatrix}x_1\\\\y_1\\\\z_1\\end{bmatrix}\\times\\begin{bmatrix}x_2\\\\y_2\\\\z_2\\end{bmatrix} =\\begin{bmatrix}y_1z_2 -z_1y_2\\\\z_1x_2-x_1z_2\\\\x_1y_2-y_1x_2\\end{bmatrix}\\] 几何解释 （1）.叉乘得到的向量垂直于原来的两个向量，如下图： 这里写图片描述 \\(a\\times b\\)的长度等于向量的大小与向量的夹角\\(sin\\)值的积，如下： \\[||a\\times b||=||a||||b||sin{\\theta}\\] 已经证明了\\(a\\times b\\)垂直于a,b。但是垂直于a,b有两个方向。\\(a\\times b\\)指向哪个方向呢？通过将a的头与b的尾相连，并检测从a到b是顺时针还是逆时针。如果在左手坐标系中，左手四个（除大拇指）指母重贴与a,b向量的方向大拇指指向的方向就是\\(a\\times b\\)垂直的方向。右手坐标的也相同只是使用右手来判断。 （2）.\\(||a\\times b||\\)也等于两向量组成的平行四边形的面积。 向量大小(长度或模) 运算法则 公式如下： \\[||v||=\\sqrt{v_1^2+v_2^2+\\cdots+v_n^2}\\] 或 \\[||v||=\\sqrt{\\sum_{i=1}^{n} v_i^2}\\] 距离公式 同一坐标系中的两个点的距离，实际上可以看作是两个点构成的一个向量，那么两个点的距离就等于此向量的模长。 运算法则 距离\\(（a,b）=||b-a||=\\sqrt{(b_x-a_x)^2+(b_y-a_y)^2+(b_z-a_z)^2}\\) 标准化向量 对于许多向量，我们只关心它的方向而不是大小。在这种情况下使用单位向量将会非常方便。单位向量就是大小为1的向量，单位向量也被称之为标准向量或更简洁地称为法线。 运算法则 对于任意非零向量v，都能计算出一个和v方向相同的单位向量\\(v_{norm}\\)。这个过程被称作向量的标准化，要标准化向量，将向量除以他的大小（模）即可。公式如下： \\[v_{norm}=\\frac{v}{||v||},v\\ne 0\\] 注意：零向量是不能被标准化的，因为除零是没有定义的。 几何解释 如下图： 向量投影 给定两个向量v和n，能将v分解成两个分量：\\(v_{||}和v_{\\perp}\\)。他们分别平行和垂直于n，并满足\\(v=v_{||}+v_{\\perp}\\)。一般称平行分量\\(v_{||}\\)为v在n上的投影。 运算法则 公式如下： \\[v_{||}=n\\frac{v\\cdot n}{||n||^2}\\] \\[v_{\\perp}=v-v_{||}\\] 几何解释 如下图： 总结 向量是用来表示方位和距离的，也就是映射到每个轴的偏移。向量的常规运算有：向量与标量乘法，向量加减法，向量点乘，向量叉乘。 参考文献 [1] Fletcher Dunnlan Parberry. 3D数学基础：图形与游戏开发.北京:清华大学出版社.2005.1.","categories":[{"name":"数学物理","slug":"数学物理","permalink":"http://yoursite.com/categories/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86/"}],"tags":[]},{"title":"矩阵","slug":"数学物理/矩阵","date":"2020-04-01T03:02:05.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2020/04/01/数学物理/矩阵/","link":"","permalink":"http://yoursite.com/2020/04/01/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86/%E7%9F%A9%E9%98%B5/","excerpt":"矩阵的基础定义 矩阵是3D数学的重要基础，它主要用来描述两个坐标系之间的关系，通过定义一种运算将一个坐标系中的向量转换到另一个坐标系中。 矩阵的数学定义 在线性代数中，矩阵就是以行和列形式组织的矩形数字块。例如下列的一个3x4的矩阵： \\[\\begin{bmatrix} 0&amp;-1&amp;1&amp;1 \\\\ 1&amp;0&amp;1&amp;4 \\\\ 2&amp;1&amp;3&amp;-4 \\end{bmatrix}\\]","text":"矩阵的基础定义 矩阵是3D数学的重要基础，它主要用来描述两个坐标系之间的关系，通过定义一种运算将一个坐标系中的向量转换到另一个坐标系中。 矩阵的数学定义 在线性代数中，矩阵就是以行和列形式组织的矩形数字块。例如下列的一个3x4的矩阵： \\[\\begin{bmatrix} 0&amp;-1&amp;1&amp;1 \\\\ 1&amp;0&amp;1&amp;4 \\\\ 2&amp;1&amp;3&amp;-4 \\end{bmatrix}\\] 方阵 行和列数相同的矩阵被定义为方阵，例如：2x2, 3x3, 4x4的矩阵都是方阵。方阵的对角线元素就是行号和列号都相同的元素。例如：3x3的矩阵M的对角线元素为\\(m_{11}\\), \\(m_{22}\\), \\(m_{33}\\)。其他元素为非对角线元素。 对角矩阵 非对角线元素都为0的元素为对角矩阵。 单位矩阵 单位矩阵是一种特殊的对角矩阵。它的对角元素都为1，非对角元素都为0. 单位矩阵非常特殊，因为它是矩阵的乘法单位元，其性质是用任意一个矩阵乘以单位矩阵，都将得到原矩阵。 矩阵运算 矩阵转置 一个r x c的矩阵M。M的转置记作\\(M^T\\), 是一个c x r的矩阵，它的列由M的行组成。可以从另一方面理解，\\(M_{ij}^T = M_{ji}\\) ,及沿着对角线翻折。例如： \\(\\begin{bmatrix} 0&amp;-1&amp;1&amp;1 \\\\ 1&amp;0&amp;1&amp;4 \\\\ 2&amp;1&amp;3&amp;-4 \\end{bmatrix}^T = \\begin{bmatrix} 0&amp;1&amp;2 \\\\ -1&amp;0&amp;1 \\\\ 1&amp;1&amp;3 \\\\ 1&amp;4&amp;-4 \\end{bmatrix}\\) 标量与矩阵乘法 \\(k\\textbf{M}=k\\begin{bmatrix} m_{11}&amp;m_{12}&amp;m_{13} \\\\ m_{21}&amp;m_{22}&amp;m_{23} \\\\ m_{31}&amp;m_{32}&amp;m_{33} \\end{bmatrix}= \\begin{bmatrix} km_{11}&amp;km_{12}&amp;km_{13} \\\\ km_{21}&amp;km_{22}&amp;km_{23} \\\\ km_{31}&amp;km_{32}&amp;km_{33} \\end{bmatrix}\\) 矩阵乘法 在某些情况下，两个矩阵可以相乘。决定矩阵能否相乘以及怎么计算结果的法则初看起来有些奇怪。一个r x n 矩阵A能够乘以一个n x c的矩阵B,结果是一个r x c的矩阵，记作 AB。矩阵乘法计算如下：记r x n矩阵A与n x c矩阵B的积r x c 矩阵AB为C。 C的任意元素\\(\\textbf{C}_{ij}\\)等于A的第i行向量与B的第j列向量的点乘结果。 正式定义为： \\(c_{ij} = \\sum_{k=1}^{n}a_{ik}b_{kj}\\) 矩阵乘法满足的定律（前提是矩阵乘法有意义）： 任意矩阵M乘以单位矩阵I的到原矩阵 MI=IM=M 矩阵乘法不满足交换律，即 AB \\(\\neq\\) BA 矩阵乘法满足结合律，即 (AB)C=A(BC) 矩阵的转置\\((AB)^T = B^TA^T\\) 向量与矩阵的乘法 因为向量能被当作是一个行或一列的矩阵，所以能够用上一节的矩阵乘法来计算。但是左乘矩阵和右乘矩阵这个两个的区别特别重要。例如，用矩阵A， B和C转换向量v,用行向量记法记作vABC。注意矩阵按顺序从左往右列出。如果使用列向量，矩阵放在左边，转换从右往左发生，这种情况下应该记作CBAv 矩阵的几何定义 一般来说，方阵能够描述任意线性变换，比如： 旋转 缩放 投影 镜像 仿射","categories":[{"name":"数学物理","slug":"数学物理","permalink":"http://yoursite.com/categories/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86/"}],"tags":[]},{"title":"CentOS安装Docker","slug":"服务器/Docker/CentOS安装Docker","date":"2020-03-10T03:02:33.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2020/03/10/服务器/Docker/CentOS安装Docker/","link":"","permalink":"http://yoursite.com/2020/03/10/%E6%9C%8D%E5%8A%A1%E5%99%A8/Docker/CentOS%E5%AE%89%E8%A3%85Docker/","excerpt":"OS需求 CentOS 7 centos-extra必须启用，默认是启用的 卸载老的版本（如果有安装过的） 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 在/var/lib/docker目录中的镜像，容器和网络被保留了，如果不需要也可以将其移除掉","text":"OS需求 CentOS 7 centos-extra必须启用，默认是启用的 卸载老的版本（如果有安装过的） 12345678$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 在/var/lib/docker目录中的镜像，容器和网络被保留了，如果不需要也可以将其移除掉 安装Docker Engine社区版 Docker官方提供了3中安装方式，如下： - 大多数用户采用设置Docker仓库的方式进行安装，这也是简单的安装和更新的方式，本文也只将介绍此安装方法，其他的安装方式，参考官方文档 - 下载RPM包进行安装并且手动的安装和管理更新，这种方式适合在没有没有网的环境下安装 - 在装测试和开发环境，有的用户选择使用自动化脚本安装，生成环境不推荐 设置Docker仓库 在一个新系统安装时，首先需要设置一个Docker仓库，如果已经添加过仓库，可以跳过这步。 1. 安装需要的包：yum-utils ，此包提供了yum-config-manager工具；以及device-mapper-persistent-data 和 lvm2 （devicemapper存储驱动需要它们） 123sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 使用下面的命令设置一个稳定版的Docker仓库 123sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 安装 安装最新版 使用下面的命令安装最新版Docker引擎，如下： 1sudo yum install -y docker-ce docker-ce-cli containerd.io docker-compose 安装指定版本 安装指定版本的的Docker引擎 1. 先列出有哪些版本可以安装，命令如下： 123456yum list docker-ce --showduplicates | sort -r--命令列出的结果docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable 安装指定版本，命令如下： 1sudo yum install -y docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io 启动Docker-CE引擎 命令如下： 1sudo systemctl start docker 验证Docker-CE是否安装成功 命令如下： 1sudo docker run hello-world 登录DockerHub 1docker login 卸载Docker-CE 卸载包 1sudo yum remove docker-ce 删除镜像，容器，卷以及自定义的配置 1sudo rm -rf /var/lib/docker","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Docker","slug":"服务器/Docker","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/Docker/"}],"tags":[{"name":"Dokcer","slug":"Dokcer","permalink":"http://yoursite.com/tags/Dokcer/"}]},{"title":"Redis安装与配置","slug":"服务器/服务软件/Redis安装与配置","date":"2020-01-15T09:15:18.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2020/01/15/服务器/服务软件/Redis安装与配置/","link":"","permalink":"http://yoursite.com/2020/01/15/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/Redis%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"安装 yum 安装 12yum info redis // 查看当前源的redis最新版本信息yum install redis","text":"安装 yum 安装 12yum info redis // 查看当前源的redis最新版本信息yum install redis 源码安装 1234567# 下载地址从[官网](https://redis.io/download)获取最新的wget http://download.redis.io/releases/redis-3.2.10.tar.gztar -zxvf redis-3.2.10.tar.gzcd redis-3.2.10makemake install 注意：如果没有c编译环境记得安装，如下： 1yum -y install gcc gcc-c++ kernel-devel //安装gcc、c++编译器以及内核文件 注意：如果没有jemalloc库，在make的时候添加如下参数： 1make MALLOC=libc 安装成功后，查看redis-server的位置。 1whereis redis-server 配置 配置文件需要启动后才会生成，如何启动参见“启动”章节， 不同的安装方式配置文件放的位置不太一样， yun安装：/etc/redis.conf 源码安装,需要收到将配置文件拷贝到/etc目录： 12cd /xxx/redis-3.2.10cp redis.conf /etc/ 基础配置 123456# 端口配置port 6379# 让redis后台运行，默认是nodaemonize yes# 日志文件配置logfile \"/var/log/redis.log\" 持久化配置 快照RDB持久化配置 123456789101112131415161718# Save the DB on disk:# 设置sedis进行数据库镜像的频率。# 900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化）。# 300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化）。# 60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）。save 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yes# 在进行镜像备份时,是否进行压缩。yes：压缩，但是需要一些cpu的消耗。no：不压缩，需要更多的磁盘空间。rdbcompression yes# 一个CRC64的校验就被放在了文件末尾，当存储或者加载rbd文件的时候会有一个10%左右的性能下降，为了达到性能的最大化，你可以关掉这个配置项。rdbchecksum yes# 快照的文件名dbfilename dump.rdb# 存放快照的目录dir /var/lib/redis AOF持久化配置 12345678910111213141516# 是否开启AOF，默认关闭（no）appendonly yes# 指定 AOF 文件名appendfilename appendonly.aof# Redis支持三种不同的刷写模式：# appendfsync always #每次收到写命令就立即强制写入磁盘，是最有保证的完全的持久化，但速度也是最慢的，一般不推荐使用。appendfsync everysec #每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折中，是受推荐的方式。# appendfsync no #完全依赖OS的写入，一般为30秒左右一次，性能最好但是持久化最没有保证，不被推荐。#在日志重写时，不进行命令追加操作，而只是将其放在缓冲区里，避免与命令的追加造成DISK IO上的冲突。#设置为yes表示rewrite期间对新写操作不fsync,暂时存在内存中,等rewrite完成后再写入，默认为no，建议yesno-appendfsync-on-rewrite yes #当前AOF文件大小是上次日志重写得到AOF文件大小的二倍时，自动启动新的日志重写过程。auto-aof-rewrite-percentage 100#当前AOF文件启动新的日志重写过程的最小值，避免刚刚启动Reids时由于文件尺寸较小导致频繁的重写。auto-aof-rewrite-min-size 64mb 两种持久化的区别 1）如果能接受几分钟的数据丢失的话，建议选择快照RDB 2）要是不允许数据丢失，则需要用AOF来持久化 关于数据恢复： RDB的启动时间会更短，原因有两个： 一是RDB文件中每一条数据只有一条记录，不会像AOF日志那样可能有一条数据的多次操作记录。所以每条数据只需要写一次就行了。 另一个原因是RDB文件的存储格式和Redis数据在内存中的编码格式是一致的，不需要再进行数据编码工作，所以在CPU消耗上要远小于AOF日志的加载。 注意： AOF(Append Only File)比RDB方式有更好的持久化性。由于在使用AOF持久化方式时，Redis会将每一个收到的写命令都通过Write函数追加到文件最后，类似于MySQL的binlog。 AOF的完全持久化方式同时也带来了另一个问题，持久化文件会变得越来越大。(比如我们调用INCR test命令100次，文件中就必须保存全部的100条命令，但其实99条都是多余的。因为要恢复数据库的状态其实文件中保存一条SET test 100就够了)。为了合并重写AOF的持久化文件，Redis提供了bgrewriteaof命令。 Redis重启如何载入数据的 通过日志可以很清楚的知道redis通过那个文件来取数据的： RDB: * DB loaded from disk: 0.000 seconds AOF: * DB loaded from append only file: 0.000 seconds 保存数据则是： RDB: * DB saved on disk AOF: * Calling fsync() on the AOF file. 重启时将按照以下优先级恢复数据到内存 如果只配置AOF,重启时加载AOF文件恢复数据。 如果同时 配置了RBD和AOF,启动是只加载AOF文件恢复数据。 如果只配置RBD,启动是讲加载dump文件恢复数据。 如果你先开启了RDB模式，想再开启AOF模式，先执行bgrewriteaof命令，不然会因为恢复数据的优先级问题，数据都没有了。 为了防止悲剧发生，注意多备份，AOF模式，记得使用脚本定期执行bgrewriteaof命令。 脚本如下，可以结合Linux Crontab来使用，定期执行。 12echo 'bgrewriteaof redis 6379'redis-cli -p 6379 bgrewriteaof 启动 普通启动 首次启动，命令如下： 1redis-server 首次启动会生成默认配置文件redis.conf 配置文件生成后，后面每次启动就可以直接跟上配置文件了，如下： 1redis-server /etc/redis.conf ## 开机自启动 在/etc/rc.d/rc.local文件尾部添加一行，如下： 1redis-server /etc/redis.conf 检查一下rc.local有没有执行权限，如果没有加上执行权限，如下： 1chmod +x /etc/rc.d/rc.local 如果未执行成功,查看logfile.log日志,寻找原因。","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"服务软件","slug":"服务器/服务软件","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/"}],"tags":[]},{"title":"Git服务器搭建","slug":"服务器/服务软件/Git服务器搭建","date":"2019-12-27T07:08:26.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2019/12/27/服务器/服务软件/Git服务器搭建/","link":"","permalink":"http://yoursite.com/2019/12/27/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/Git%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/","excerpt":"CentOS安装与配置Git","text":"CentOS安装与配置Git 使用yum命令安装： 1yum install -y git 为Git单独创建一个用户 创建用户 1adduser git 修改密码 1passwd git 创建Git仓库 创建一个目录存放Git仓库，目录/data/git 12mkdir /data/gitgit init --bare Puzzle.git 将Puzzle.git的拥有者修改为上面创建的git用户 1chown git:git Puzzle.git 开启ssh key登录 编辑/etc/ssh/sshd_config文件,把下下面三个的注释打开，如果没有自行新建 123RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys 禁用git用户的shell登陆 编辑/etc/passwd文件，找到git用户对应的配置，将最后一个分号的内容修改成/usr/bin/git-shell,如下： 12git:x:1000:1001::/home/git:/bin/bash #改成如下，git:x:1000:1001::/home/git:/usr/bin/git-shell 输入git用户的密码进行验证，至此已完成在服务器配置Git仓库的任务，但是这样每次更新都会要求输入密码，特麻烦，下面章节将介绍更安全便捷的访问方式。 客户端配置 Windows配置 Git的客户端选择使用TortoiseGit。 下载TortoiseGit，下载地址如下： https://tortoisegit.org/download/ 安装TortoiseGit，就跟安装普通应用程序一样，下一步-&gt;下一步 生成Putty Key, 成功安装TortoiseGit后在其安装目录下的bin目录下会有一个PuttyGen的应用程序用于生成Putty key 启动PuttyGen生成并保存key,如下图： 生成KEY 保存KEY 拷贝公匙到服务器的/home/git/.ssh/authorized_keys文件末尾，注意如果没有相应文件夹或文件请自行创建,注意权限参见“遇到的问题”章节 使用TortoiseGit下载Git仓库 在一个空目录邮件单机，选择Git Clone...,如下图： Mac/Linux配置 将~/.ssh目录下的id_rsa.pub内容追加到到服务器的/home/git/.ssh/authorized_keys文件末尾。如果没有id_rsa.pub, 可以用ssh-keygen来创建 在其他电脑上测试能否克隆远程仓库 1git clone ssh://git@192.168.0.172:22/data/git/Puzzle.git 遇到的问题 在对应用户（git）目录下.ssh的目录权限必须是700， authorized_keys文件必须是600的权限","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"服务软件","slug":"服务器/服务软件","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/"}],"tags":[]},{"title":"Nginx安装与配置","slug":"服务器/服务软件/nginx安装与配置","date":"2019-12-08T16:00:00.000Z","updated":"2025-04-26T11:06:24.133Z","comments":true,"path":"2019/12/09/服务器/服务软件/nginx安装与配置/","link":"","permalink":"http://yoursite.com/2019/12/09/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/nginx%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/","excerpt":"简介 Nginx是个web，反向代理, 负载均衡, 邮件代理和HTTP缓存服务器.","text":"简介 Nginx是个web，反向代理, 负载均衡, 邮件代理和HTTP缓存服务器. 安装 centos安装 1.安装准备 sudo yum install yum-utils 2.设置yum仓库，使用下面的内容创建一个/etc/yum.repos.d/nginx.repo文件 [nginx-stable] name=nginx stable repo baseurl=http://nginx.org/packages/centos/\\(releasever/\\)basearch/ gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true [nginx-mainline] name=nginx mainline repo baseurl=http://nginx.org/packages/mainline/centos/\\(releasever/\\)basearch/ gpgcheck=1 enabled=0 gpgkey=https://nginx.org/keys/nginx_signing.key module_hotfixes=true 3.安装Nginx sudo yum install nginx 配置 通常Nginx的配置放在下面几目录中： - /etc/nginx/nginx.conf , - /usr/local/etc/nginx/nginx.conf 或 - /usr/local/nginx/conf/nginx.conf 先看看默认的配置文件，如下： 1234567891011121314151617181920212223242526272829user nginx; #用户worker_processes 1; #工作进程数量error_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log warn; #错误日志pid &#x2F;var&#x2F;run&#x2F;nginx.pid; #定义一个存储主进程ID文件events &#123; worker_connections 1024; #最大同时连接数&#125;http &#123; include &#x2F;etc&#x2F;nginx&#x2F;mime.types; #扩展名对应的MIME类型 default_type application&#x2F;octet-stream; log_format main &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;; access_log &#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log main; #访问日志文件 sendfile on; #非阻塞磁盘IO #tcp_nopush on; keepalive_timeout 65; #连接超时值 #gzip on; include &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;*.conf; #包含其他配置文件,在这目录下有一个default.conf文件，里面配置了服务器上下文&#125; 配置文件由上下文（模块）与指令两部组成的树形结构指令集，子上下文的指令会覆盖父上下的指令。Nginx的核心上下文有： - 主上下文或全局上下文 - events上下文 - http上下文 - server上下文 - location上下文 其他上下文参见官网文档 ## 配置 ### 核心上下文 #### 主上下文 12345# 最外层的上下文，者是主上下文. . .context &#123; . . .&#125; Events上下文 123456# main contextevents &#123; # events context . . .&#125; HTTP上下文 1234567891011# main context. . .events &#123; # events context . . .&#125;http &#123; # http context . . .&#125; Server上下文 1234567891011121314# main contexthttp &#123; # http context server &#123; # first server context &#125; server &#123; # second server context &#125;&#125; Location上下文 1234567891011121314151617181920212223242526# main contextserver &#123; # server context #location [modifier] path # = - 精准匹配 # ^~ - 优先匹配 # ~ &amp;&amp; ~* - 正则匹配 # no modifier - 前缀匹配 location /match/criteria &#123;s # first location context &#125; location /other/criteria &#123; # second location context location nested_match &#123; # first nested location &#125; location other_nested &#123; # second nested location &#125; &#125;&#125; 核心配置指令 1234567891011121314151617181920212223242526272829303132# 指定用户user www www;# 指定工作进程数量或者自动（根据cpu等相关信息自动选择）worker_processes 2|auto;# 指定日志文件error_log /var/log/nginx-error.log info;# 访问日志文件（http, server, location, if in location, limit_except）access_log /var/log/nginx-access.log;# 监听指令并为默认服务器listen *:80 default_server;# 服务器名字(域名)server_name qilezaitu.top# 指定根目录指令root /var/www/qilezaitu;# 指定首页文件index index.html index.htm index.php;# try_files指令，在try_files列表你查找文件 ，其中$uri就是用户请求的uritry_files $uri index.html =404;# 包含指令,包含其他配置或文件include /etc/nginx/conf.d/*.conf;# 返回指令return 200 \"Hello from netguru.co\"; 优化配置指令 1234567#开启各类优化配置tcp_nodelay ontcp_nopush onsendfile onkeepalive_timeout 65;worker_connections 4096;worker_rlimit_nofile 8192; 常用服务器配置 此节将简单的配置一下几种服务器，Web服务器,负载均衡服务器和代理服务器。 在/etc/nginx/nginx.conf文件中进行配置，如下： 12345678910111213141516171819202122232425262728293031323334user nginx; worker_processes auto;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;worker_rlimit_nofile 8192;events &#123; worker_connections 4096;&#125;http &#123; include /etc/nginx/mime.types; index index.html index.htm index.php; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; keepalive_timeout 65; server_names_hash_bucket_size 128; # 包含conf.d目录下的所有配置，后续的服务器配置都将在这个目录进行 include /etc/nginx/conf.d/*.conf;&#125; 最后将/etc/nginx/conf.d/的默认配置文件备份，下面将在此目录下配置不同的服务器。 Web服务器配置 在/etc/nginx/conf.d/目录下新建一个配置文件命名为home.qilezaitu.top.conf 内容如下： 12345678910server &#123; listen *:80; server_name home.qilezaitu.top www.home.qilezaitu.top; access_log /var/log/nginx/access.log main; root /usr/share/nginx/html/blog; location / &#123; &#125;&#125; 负载均衡服务器配置 在/etc/nginx/conf.d/目录下新建一个配置文件命名为balance.conf 内容如下： 12345678910111213141516upstream big_server_com &#123; server 127.0.0.3:8000 weight=5; server 127.0.0.3:8001 weight=5; server 192.168.0.1:8000; server 192.168.0.1:8001; &#125; server &#123; # simple load balancing listen 80; server_name big.server.com; access_log logs/big.server.access.log main; location / &#123; proxy_pass http://big_server_com; &#125; &#125; 代理服务器配置 在/etc/nginx/conf.d/目录下新建一个配置文件命名为proxy.conf 内容如下： 123456789101112131415161718192021222324252627proxy_redirect off;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;client_max_body_size 10m;client_body_buffer_size 128k;proxy_connect_timeout 90;proxy_send_timeout 90;proxy_read_timeout 90;proxy_buffers 32 4k;server &#123; # simple reverse-proxy listen 80; server_name domain2.com www.domain2.com; access_log logs/domain2.access.log main; # serve static files location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; root /var/www/virtual/big.server.com/htdocs; expires 30d; &#125; # pass requests for dynamic content to rails/turbogears/zope, et al location / &#123; proxy_pass http://127.0.0.1:8080; &#125; &#125; SSL/STL配置 123456789101112131415server &#123; listen 443 ssl; server_name qilezaitu.top www.qilezaitu.top; root /var/www/html; index index.html index.htm; ssl_certificate /etc/nginx/ssl/qilezaitu.top.pem; ssl_certificate_key /etc/nginx/ssl/qilezaitu.top.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers HIGH:!aNULL:!MD5; location / &#123; &#125;&#125; 强制http到https 12345server &#123; listen 80; server_name qilezaitu.top; rewrite ^(.*)$ https://qilezaitu.top permanent;&#125; 启动 启动Nginx只需要简单的输入 1nginx 当启动了nginx启动后，可以通过发送信号的方式管理你的nginx,如下： 1nginx -s signal 可用的signal： - stop:快速关闭 - quit:等待工作进程完成了当前的请求后关闭 - reload:重载配置 - reopen:刷新日志文件 将Nginx加入到自启动，在/etc/rc.d/rc.local文件中追加一行，如下： 1nginx 记录各种问题 总结 参考 Apache Vs NGINX – Which Is The Best Web Server for You? Nginx安装教程 Nginx Tutorial #1: Basic Concepts Understanding the Nginx Configuration File Structure and Configuration Contexts Alphabetical index of directives nginx.conf","categories":[{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"服务软件","slug":"服务器/服务软件","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/"}],"tags":[]}],"categories":[{"name":"Unity","slug":"Unity","permalink":"http://yoursite.com/categories/Unity/"},{"name":"UE5","slug":"UE5","permalink":"http://yoursite.com/categories/UE5/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Hexo","slug":"工具/Hexo","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/Hexo/"},{"name":"理财","slug":"理财","permalink":"http://yoursite.com/categories/%E7%90%86%E8%B4%A2/"},{"name":"Map","slug":"Map","permalink":"http://yoursite.com/categories/Map/"},{"name":"编程语言","slug":"编程语言","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"C#","slug":"编程语言/C","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/C/"},{"name":"数学物理","slug":"数学物理","permalink":"http://yoursite.com/categories/%E6%95%B0%E5%AD%A6%E7%89%A9%E7%90%86/"},{"name":"服务器","slug":"服务器","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Docker","slug":"服务器/Docker","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/Docker/"},{"name":"服务软件","slug":"服务器/服务软件","permalink":"http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/%E6%9C%8D%E5%8A%A1%E8%BD%AF%E4%BB%B6/"}],"tags":[{"name":"优化","slug":"优化","permalink":"http://yoursite.com/tags/%E4%BC%98%E5%8C%96/"},{"name":"UE5","slug":"UE5","permalink":"http://yoursite.com/tags/UE5/"},{"name":"Hexo","slug":"Hexo","permalink":"http://yoursite.com/tags/Hexo/"},{"name":"Mathjax","slug":"Mathjax","permalink":"http://yoursite.com/tags/Mathjax/"},{"name":"理财","slug":"理财","permalink":"http://yoursite.com/tags/%E7%90%86%E8%B4%A2/"},{"name":"后处理","slug":"后处理","permalink":"http://yoursite.com/tags/%E5%90%8E%E5%A4%84%E7%90%86/"},{"name":"Rendering","slug":"Rendering","permalink":"http://yoursite.com/tags/Rendering/"},{"name":"Batching","slug":"Batching","permalink":"http://yoursite.com/tags/Batching/"},{"name":"Shader","slug":"Shader","permalink":"http://yoursite.com/tags/Shader/"},{"name":"Map","slug":"Map","permalink":"http://yoursite.com/tags/Map/"},{"name":"翻译","slug":"翻译","permalink":"http://yoursite.com/tags/%E7%BF%BB%E8%AF%91/"},{"name":"Lua","slug":"Lua","permalink":"http://yoursite.com/tags/Lua/"},{"name":"源码","slug":"源码","permalink":"http://yoursite.com/tags/%E6%BA%90%E7%A0%81/"},{"name":"UI","slug":"UI","permalink":"http://yoursite.com/tags/UI/"},{"name":"动画","slug":"动画","permalink":"http://yoursite.com/tags/%E5%8A%A8%E7%94%BB/"},{"name":"协程","slug":"协程","permalink":"http://yoursite.com/tags/%E5%8D%8F%E7%A8%8B/"},{"name":"C#","slug":"C","permalink":"http://yoursite.com/tags/C/"},{"name":".Net","slug":"Net","permalink":"http://yoursite.com/tags/Net/"},{"name":"Dokcer","slug":"Dokcer","permalink":"http://yoursite.com/tags/Dokcer/"}]}